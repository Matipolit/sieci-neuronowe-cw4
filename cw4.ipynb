{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_test.pkl\", 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(\"X_train.pkl\", 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(\"y_test.pkl\", 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "with open(\"y_train.pkl\", 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 4 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class HeartDiseaseClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(HeartDiseaseClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.output = nn.Linear(64, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        #x = torch.sigmoid(self.output(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = HeartDiseaseClassifier(input_dim = X_train.shape[1] , output_dim = 1).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    # Apply sigmoid to logits and threshold at 0.5 for binary classification\n",
    "    preds = torch.sigmoid(outputs) >= 0.5  # Converts logits to binary predictions\n",
    "    return (preds == labels).float().mean()  # Calculate mean accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    last_loss = 0.\n",
    "    last_acc = 0.\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()  # Squeeze to match the label shape\n",
    "        loss = loss_fn(outputs, labels.squeeze())  # Match dimensions\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        batch_accuracy = calculate_accuracy(outputs, labels)\n",
    "        running_acc += batch_accuracy.item()\n",
    "        if i % 16 == 15:\n",
    "            last_loss = running_loss / 16 # loss per batch\n",
    "            last_acc = running_acc / 16  # Average accuracy over the last 16 batches\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            tb_writer.add_scalar('Accuracy/train', last_acc, tb_x)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 16 loss: 0.7076363936066628\n",
      "  batch 32 loss: 0.693256001919508\n",
      "  batch 48 loss: 0.6946496963500977\n",
      "LOSS train 0.6946496963500977 valid 0.7017516493797302\n",
      "EPOCH 2:\n",
      "  batch 16 loss: 0.7009462937712669\n",
      "  batch 32 loss: 0.6964547075331211\n",
      "  batch 48 loss: 0.6957279145717621\n",
      "LOSS train 0.6957279145717621 valid 0.698940634727478\n",
      "EPOCH 3:\n",
      "  batch 16 loss: 0.6866101436316967\n",
      "  batch 32 loss: 0.6905080303549767\n",
      "  batch 48 loss: 0.6943470761179924\n",
      "LOSS train 0.6943470761179924 valid 0.696040689945221\n",
      "EPOCH 4:\n",
      "  batch 16 loss: 0.6865958534181118\n",
      "  batch 32 loss: 0.6890987306833267\n",
      "  batch 48 loss: 0.6987191289663315\n",
      "LOSS train 0.6987191289663315 valid 0.693253755569458\n",
      "EPOCH 5:\n",
      "  batch 16 loss: 0.6868382133543491\n",
      "  batch 32 loss: 0.6888525262475014\n",
      "  batch 48 loss: 0.68861248716712\n",
      "LOSS train 0.68861248716712 valid 0.6902115941047668\n",
      "EPOCH 6:\n",
      "  batch 16 loss: 0.6910265646874905\n",
      "  batch 32 loss: 0.6877324432134628\n",
      "  batch 48 loss: 0.6826207004487514\n",
      "LOSS train 0.6826207004487514 valid 0.6875985860824585\n",
      "EPOCH 7:\n",
      "  batch 16 loss: 0.6857299171388149\n",
      "  batch 32 loss: 0.6793488338589668\n",
      "  batch 48 loss: 0.6822483316063881\n",
      "LOSS train 0.6822483316063881 valid 0.6850971579551697\n",
      "EPOCH 8:\n",
      "  batch 16 loss: 0.6762378476560116\n",
      "  batch 32 loss: 0.6763110794126987\n",
      "  batch 48 loss: 0.6862943172454834\n",
      "LOSS train 0.6862943172454834 valid 0.6820988059043884\n",
      "EPOCH 9:\n",
      "  batch 16 loss: 0.6805877014994621\n",
      "  batch 32 loss: 0.6738620810210705\n",
      "  batch 48 loss: 0.676010001450777\n",
      "LOSS train 0.676010001450777 valid 0.6799984574317932\n",
      "EPOCH 10:\n",
      "  batch 16 loss: 0.6776588968932629\n",
      "  batch 32 loss: 0.6796038560569286\n",
      "  batch 48 loss: 0.6645617336034775\n",
      "LOSS train 0.6645617336034775 valid 0.6774348616600037\n",
      "EPOCH 11:\n",
      "  batch 16 loss: 0.673333827406168\n",
      "  batch 32 loss: 0.6699955612421036\n",
      "  batch 48 loss: 0.678148303180933\n",
      "LOSS train 0.678148303180933 valid 0.6749868988990784\n",
      "EPOCH 12:\n",
      "  batch 16 loss: 0.664571437984705\n",
      "  batch 32 loss: 0.6758071407675743\n",
      "  batch 48 loss: 0.6692862287163734\n",
      "LOSS train 0.6692862287163734 valid 0.6724581718444824\n",
      "EPOCH 13:\n",
      "  batch 16 loss: 0.6714419685304165\n",
      "  batch 32 loss: 0.6680723428726196\n",
      "  batch 48 loss: 0.6698869802057743\n",
      "LOSS train 0.6698869802057743 valid 0.6701006293296814\n",
      "EPOCH 14:\n",
      "  batch 16 loss: 0.6601472422480583\n",
      "  batch 32 loss: 0.6680284701287746\n",
      "  batch 48 loss: 0.6702350936830044\n",
      "LOSS train 0.6702350936830044 valid 0.6674227714538574\n",
      "EPOCH 15:\n",
      "  batch 16 loss: 0.6646695919334888\n",
      "  batch 32 loss: 0.6717303767800331\n",
      "  batch 48 loss: 0.6554302237927914\n",
      "LOSS train 0.6554302237927914 valid 0.664946436882019\n",
      "EPOCH 16:\n",
      "  batch 16 loss: 0.6538282036781311\n",
      "  batch 32 loss: 0.6649443060159683\n",
      "  batch 48 loss: 0.6628957837820053\n",
      "LOSS train 0.6628957837820053 valid 0.6624953150749207\n",
      "EPOCH 17:\n",
      "  batch 16 loss: 0.646810457110405\n",
      "  batch 32 loss: 0.6564185470342636\n",
      "  batch 48 loss: 0.6634578891098499\n",
      "LOSS train 0.6634578891098499 valid 0.6599436402320862\n",
      "EPOCH 18:\n",
      "  batch 16 loss: 0.643496036529541\n",
      "  batch 32 loss: 0.6622188240289688\n",
      "  batch 48 loss: 0.6618020236492157\n",
      "LOSS train 0.6618020236492157 valid 0.6575227379798889\n",
      "EPOCH 19:\n",
      "  batch 16 loss: 0.6491407044231892\n",
      "  batch 32 loss: 0.6515932865440845\n",
      "  batch 48 loss: 0.6546728573739529\n",
      "LOSS train 0.6546728573739529 valid 0.6549708843231201\n",
      "EPOCH 20:\n",
      "  batch 16 loss: 0.6451040841639042\n",
      "  batch 32 loss: 0.6462773978710175\n",
      "  batch 48 loss: 0.6615009605884552\n",
      "LOSS train 0.6615009605884552 valid 0.6524826884269714\n",
      "EPOCH 21:\n",
      "  batch 16 loss: 0.6492925025522709\n",
      "  batch 32 loss: 0.6557341292500496\n",
      "  batch 48 loss: 0.6366741582751274\n",
      "LOSS train 0.6366741582751274 valid 0.6499126553535461\n",
      "EPOCH 22:\n",
      "  batch 16 loss: 0.6369229629635811\n",
      "  batch 32 loss: 0.6517984569072723\n",
      "  batch 48 loss: 0.6479418650269508\n",
      "LOSS train 0.6479418650269508 valid 0.6473367214202881\n",
      "EPOCH 23:\n",
      "  batch 16 loss: 0.6344694346189499\n",
      "  batch 32 loss: 0.6396074369549751\n",
      "  batch 48 loss: 0.6374167054891586\n",
      "LOSS train 0.6374167054891586 valid 0.6444802284240723\n",
      "EPOCH 24:\n",
      "  batch 16 loss: 0.6413660012185574\n",
      "  batch 32 loss: 0.6418765895068645\n",
      "  batch 48 loss: 0.6517548523843288\n",
      "LOSS train 0.6517548523843288 valid 0.6420981884002686\n",
      "EPOCH 25:\n",
      "  batch 16 loss: 0.6494121700525284\n",
      "  batch 32 loss: 0.6386760696768761\n",
      "  batch 48 loss: 0.623915147036314\n",
      "LOSS train 0.623915147036314 valid 0.6392200589179993\n",
      "EPOCH 26:\n",
      "  batch 16 loss: 0.6270650811493397\n",
      "  batch 32 loss: 0.6322275623679161\n",
      "  batch 48 loss: 0.6461255475878716\n",
      "LOSS train 0.6461255475878716 valid 0.6365292072296143\n",
      "EPOCH 27:\n",
      "  batch 16 loss: 0.6162215359508991\n",
      "  batch 32 loss: 0.6348770409822464\n",
      "  batch 48 loss: 0.6435568742454052\n",
      "LOSS train 0.6435568742454052 valid 0.6335582733154297\n",
      "EPOCH 28:\n",
      "  batch 16 loss: 0.639555137604475\n",
      "  batch 32 loss: 0.6215025410056114\n",
      "  batch 48 loss: 0.625804852694273\n",
      "LOSS train 0.625804852694273 valid 0.6306646466255188\n",
      "EPOCH 29:\n",
      "  batch 16 loss: 0.6377975642681122\n",
      "  batch 32 loss: 0.6193374916911125\n",
      "  batch 48 loss: 0.6132014282047749\n",
      "LOSS train 0.6132014282047749 valid 0.627710223197937\n",
      "EPOCH 30:\n",
      "  batch 16 loss: 0.6277176924049854\n",
      "  batch 32 loss: 0.6204671002924442\n",
      "  batch 48 loss: 0.6221786960959435\n",
      "LOSS train 0.6221786960959435 valid 0.6249443292617798\n",
      "EPOCH 31:\n",
      "  batch 16 loss: 0.6158960983157158\n",
      "  batch 32 loss: 0.6283829100430012\n",
      "  batch 48 loss: 0.6137367188930511\n",
      "LOSS train 0.6137367188930511 valid 0.6221750974655151\n",
      "EPOCH 32:\n",
      "  batch 16 loss: 0.6090524457395077\n",
      "  batch 32 loss: 0.6273763999342918\n",
      "  batch 48 loss: 0.6085200868546963\n",
      "LOSS train 0.6085200868546963 valid 0.6191989183425903\n",
      "EPOCH 33:\n",
      "  batch 16 loss: 0.6390658095479012\n",
      "  batch 32 loss: 0.6128464788198471\n",
      "  batch 48 loss: 0.5872983522713184\n",
      "LOSS train 0.5872983522713184 valid 0.6162657141685486\n",
      "EPOCH 34:\n",
      "  batch 16 loss: 0.605158869177103\n",
      "  batch 32 loss: 0.5957624129951\n",
      "  batch 48 loss: 0.6188344173133373\n",
      "LOSS train 0.6188344173133373 valid 0.6130340695381165\n",
      "EPOCH 35:\n",
      "  batch 16 loss: 0.6034981533885002\n",
      "  batch 32 loss: 0.6032147333025932\n",
      "  batch 48 loss: 0.6023353673517704\n",
      "LOSS train 0.6023353673517704 valid 0.6099209189414978\n",
      "EPOCH 36:\n",
      "  batch 16 loss: 0.6120184492319822\n",
      "  batch 32 loss: 0.5850829891860485\n",
      "  batch 48 loss: 0.6088316701352596\n",
      "LOSS train 0.6088316701352596 valid 0.6066951155662537\n",
      "EPOCH 37:\n",
      "  batch 16 loss: 0.5915684849023819\n",
      "  batch 32 loss: 0.6300816424190998\n",
      "  batch 48 loss: 0.5869917217642069\n",
      "LOSS train 0.5869917217642069 valid 0.6036391854286194\n",
      "EPOCH 38:\n",
      "  batch 16 loss: 0.5977868884801865\n",
      "  batch 32 loss: 0.590831283479929\n",
      "  batch 48 loss: 0.6115183010697365\n",
      "LOSS train 0.6115183010697365 valid 0.6003424525260925\n",
      "EPOCH 39:\n",
      "  batch 16 loss: 0.5708592720329762\n",
      "  batch 32 loss: 0.6126806400716305\n",
      "  batch 48 loss: 0.590733353048563\n",
      "LOSS train 0.590733353048563 valid 0.5969038009643555\n",
      "EPOCH 40:\n",
      "  batch 16 loss: 0.6054491773247719\n",
      "  batch 32 loss: 0.572885200381279\n",
      "  batch 48 loss: 0.5761043168604374\n",
      "LOSS train 0.5761043168604374 valid 0.5933716893196106\n",
      "EPOCH 41:\n",
      "  batch 16 loss: 0.6085600294172764\n",
      "  batch 32 loss: 0.5672110375016928\n",
      "  batch 48 loss: 0.5938269831240177\n",
      "LOSS train 0.5938269831240177 valid 0.5903013348579407\n",
      "EPOCH 42:\n",
      "  batch 16 loss: 0.5951932538300753\n",
      "  batch 32 loss: 0.5937273241579533\n",
      "  batch 48 loss: 0.5508090797811747\n",
      "LOSS train 0.5508090797811747 valid 0.586878776550293\n",
      "EPOCH 43:\n",
      "  batch 16 loss: 0.5911486968398094\n",
      "  batch 32 loss: 0.5577303450554609\n",
      "  batch 48 loss: 0.5675009153783321\n",
      "LOSS train 0.5675009153783321 valid 0.5833364129066467\n",
      "EPOCH 44:\n",
      "  batch 16 loss: 0.5439374465495348\n",
      "  batch 32 loss: 0.5917634665966034\n",
      "  batch 48 loss: 0.5670881364494562\n",
      "LOSS train 0.5670881364494562 valid 0.579886794090271\n",
      "EPOCH 45:\n",
      "  batch 16 loss: 0.5606303624808788\n",
      "  batch 32 loss: 0.5520002543926239\n",
      "  batch 48 loss: 0.5835945028811693\n",
      "LOSS train 0.5835945028811693 valid 0.5764378309249878\n",
      "EPOCH 46:\n",
      "  batch 16 loss: 0.5587038025259972\n",
      "  batch 32 loss: 0.5852537900209427\n",
      "  batch 48 loss: 0.5569049846380949\n",
      "LOSS train 0.5569049846380949 valid 0.5730737447738647\n",
      "EPOCH 47:\n",
      "  batch 16 loss: 0.5441709961742163\n",
      "  batch 32 loss: 0.5630349218845367\n",
      "  batch 48 loss: 0.5761989522725344\n",
      "LOSS train 0.5761989522725344 valid 0.5694534778594971\n",
      "EPOCH 48:\n",
      "  batch 16 loss: 0.553098738193512\n",
      "  batch 32 loss: 0.5407094769179821\n",
      "  batch 48 loss: 0.5739167761057615\n",
      "LOSS train 0.5739167761057615 valid 0.5659695863723755\n",
      "EPOCH 49:\n",
      "  batch 16 loss: 0.5882603693753481\n",
      "  batch 32 loss: 0.5340762604027987\n",
      "  batch 48 loss: 0.5239612348377705\n",
      "LOSS train 0.5239612348377705 valid 0.5624083876609802\n",
      "EPOCH 50:\n",
      "  batch 16 loss: 0.5567675027996302\n",
      "  batch 32 loss: 0.5729187652468681\n",
      "  batch 48 loss: 0.5392441395670176\n",
      "LOSS train 0.5392441395670176 valid 0.5590433478355408\n",
      "EPOCH 51:\n",
      "  batch 16 loss: 0.512721985578537\n",
      "  batch 32 loss: 0.5479175094515085\n",
      "  batch 48 loss: 0.5473714973777533\n",
      "LOSS train 0.5473714973777533 valid 0.5552259683609009\n",
      "EPOCH 52:\n",
      "  batch 16 loss: 0.5550512950867414\n",
      "  batch 32 loss: 0.5358168371021748\n",
      "  batch 48 loss: 0.5237129051238298\n",
      "LOSS train 0.5237129051238298 valid 0.5518413782119751\n",
      "EPOCH 53:\n",
      "  batch 16 loss: 0.5392672661691904\n",
      "  batch 32 loss: 0.5421650782227516\n",
      "  batch 48 loss: 0.5440901834517717\n",
      "LOSS train 0.5440901834517717 valid 0.5484259724617004\n",
      "EPOCH 54:\n",
      "  batch 16 loss: 0.5510233212262392\n",
      "  batch 32 loss: 0.5030992720276117\n",
      "  batch 48 loss: 0.504288911819458\n",
      "LOSS train 0.504288911819458 valid 0.5446287393569946\n",
      "EPOCH 55:\n",
      "  batch 16 loss: 0.5485293045639992\n",
      "  batch 32 loss: 0.5038733556866646\n",
      "  batch 48 loss: 0.5250693429261446\n",
      "LOSS train 0.5250693429261446 valid 0.5413988828659058\n",
      "EPOCH 56:\n",
      "  batch 16 loss: 0.5332290660589933\n",
      "  batch 32 loss: 0.5058874245733023\n",
      "  batch 48 loss: 0.5062325950711966\n",
      "LOSS train 0.5062325950711966 valid 0.5377875566482544\n",
      "EPOCH 57:\n",
      "  batch 16 loss: 0.5071276463568211\n",
      "  batch 32 loss: 0.511914974078536\n",
      "  batch 48 loss: 0.5182014405727386\n",
      "LOSS train 0.5182014405727386 valid 0.5344074368476868\n",
      "EPOCH 58:\n",
      "  batch 16 loss: 0.5015039220452309\n",
      "  batch 32 loss: 0.5223617572337389\n",
      "  batch 48 loss: 0.49788391031324863\n",
      "LOSS train 0.49788391031324863 valid 0.531096875667572\n",
      "EPOCH 59:\n",
      "  batch 16 loss: 0.4938013758510351\n",
      "  batch 32 loss: 0.5113170724362135\n",
      "  batch 48 loss: 0.5054754503071308\n",
      "LOSS train 0.5054754503071308 valid 0.527580201625824\n",
      "EPOCH 60:\n",
      "  batch 16 loss: 0.4837505053728819\n",
      "  batch 32 loss: 0.5111841894686222\n",
      "  batch 48 loss: 0.4920482598245144\n",
      "LOSS train 0.4920482598245144 valid 0.5242971181869507\n",
      "EPOCH 61:\n",
      "  batch 16 loss: 0.5125917438417673\n",
      "  batch 32 loss: 0.5241584423929453\n",
      "  batch 48 loss: 0.4665502719581127\n",
      "LOSS train 0.4665502719581127 valid 0.5210480093955994\n",
      "EPOCH 62:\n",
      "  batch 16 loss: 0.47123141027987003\n",
      "  batch 32 loss: 0.4799308702349663\n",
      "  batch 48 loss: 0.5039134975522757\n",
      "LOSS train 0.5039134975522757 valid 0.5176577568054199\n",
      "EPOCH 63:\n",
      "  batch 16 loss: 0.4647125843912363\n",
      "  batch 32 loss: 0.4892239850014448\n",
      "  batch 48 loss: 0.4673700127750635\n",
      "LOSS train 0.4673700127750635 valid 0.5145900845527649\n",
      "EPOCH 64:\n",
      "  batch 16 loss: 0.49890153110027313\n",
      "  batch 32 loss: 0.45983428321778774\n",
      "  batch 48 loss: 0.47241054475307465\n",
      "LOSS train 0.47241054475307465 valid 0.5116825103759766\n",
      "EPOCH 65:\n",
      "  batch 16 loss: 0.5210739318281412\n",
      "  batch 32 loss: 0.45601360499858856\n",
      "  batch 48 loss: 0.4572449456900358\n",
      "LOSS train 0.4572449456900358 valid 0.5086918473243713\n",
      "EPOCH 66:\n",
      "  batch 16 loss: 0.4769101981073618\n",
      "  batch 32 loss: 0.4418089687824249\n",
      "  batch 48 loss: 0.49535269290208817\n",
      "LOSS train 0.49535269290208817 valid 0.5057144165039062\n",
      "EPOCH 67:\n",
      "  batch 16 loss: 0.4749836251139641\n",
      "  batch 32 loss: 0.47378128208220005\n",
      "  batch 48 loss: 0.48064642399549484\n",
      "LOSS train 0.48064642399549484 valid 0.5028108358383179\n",
      "EPOCH 68:\n",
      "  batch 16 loss: 0.4534669667482376\n",
      "  batch 32 loss: 0.4693055860698223\n",
      "  batch 48 loss: 0.4691751357167959\n",
      "LOSS train 0.4691751357167959 valid 0.4997917413711548\n",
      "EPOCH 69:\n",
      "  batch 16 loss: 0.4784634616225958\n",
      "  batch 32 loss: 0.44744438491761684\n",
      "  batch 48 loss: 0.4489957056939602\n",
      "LOSS train 0.4489957056939602 valid 0.49706459045410156\n",
      "EPOCH 70:\n",
      "  batch 16 loss: 0.4166142102330923\n",
      "  batch 32 loss: 0.4789715874940157\n",
      "  batch 48 loss: 0.4246463840827346\n",
      "LOSS train 0.4246463840827346 valid 0.49425145983695984\n",
      "EPOCH 71:\n",
      "  batch 16 loss: 0.46075357403606176\n",
      "  batch 32 loss: 0.46158455684781075\n",
      "  batch 48 loss: 0.42529850266873837\n",
      "LOSS train 0.42529850266873837 valid 0.49173927307128906\n",
      "EPOCH 72:\n",
      "  batch 16 loss: 0.49316152930259705\n",
      "  batch 32 loss: 0.4335205424576998\n",
      "  batch 48 loss: 0.4198058107867837\n",
      "LOSS train 0.4198058107867837 valid 0.48924845457077026\n",
      "EPOCH 73:\n",
      "  batch 16 loss: 0.44454940035939217\n",
      "  batch 32 loss: 0.4740535840392113\n",
      "  batch 48 loss: 0.4374452717602253\n",
      "LOSS train 0.4374452717602253 valid 0.486929714679718\n",
      "EPOCH 74:\n",
      "  batch 16 loss: 0.44476517476141453\n",
      "  batch 32 loss: 0.44168286956846714\n",
      "  batch 48 loss: 0.39795430935919285\n",
      "LOSS train 0.39795430935919285 valid 0.48447445034980774\n",
      "EPOCH 75:\n",
      "  batch 16 loss: 0.3945271326228976\n",
      "  batch 32 loss: 0.4459737315773964\n",
      "  batch 48 loss: 0.45297076739370823\n",
      "LOSS train 0.45297076739370823 valid 0.4821709990501404\n",
      "EPOCH 76:\n",
      "  batch 16 loss: 0.47194284945726395\n",
      "  batch 32 loss: 0.39505061227828264\n",
      "  batch 48 loss: 0.4198501743376255\n",
      "LOSS train 0.4198501743376255 valid 0.47992271184921265\n",
      "EPOCH 77:\n",
      "  batch 16 loss: 0.42504068836569786\n",
      "  batch 32 loss: 0.42730971798300743\n",
      "  batch 48 loss: 0.4146707532927394\n",
      "LOSS train 0.4146707532927394 valid 0.477843701839447\n",
      "EPOCH 78:\n",
      "  batch 16 loss: 0.4396261954680085\n",
      "  batch 32 loss: 0.36561327893286943\n",
      "  batch 48 loss: 0.48093110136687756\n",
      "LOSS train 0.48093110136687756 valid 0.4757661819458008\n",
      "EPOCH 79:\n",
      "  batch 16 loss: 0.4263250855728984\n",
      "  batch 32 loss: 0.42618350964039564\n",
      "  batch 48 loss: 0.42084953282028437\n",
      "LOSS train 0.42084953282028437 valid 0.47385019063949585\n",
      "EPOCH 80:\n",
      "  batch 16 loss: 0.413498287089169\n",
      "  batch 32 loss: 0.4154784493148327\n",
      "  batch 48 loss: 0.4357560994103551\n",
      "LOSS train 0.4357560994103551 valid 0.47196632623672485\n",
      "EPOCH 81:\n",
      "  batch 16 loss: 0.4307591998949647\n",
      "  batch 32 loss: 0.4455584194511175\n",
      "  batch 48 loss: 0.3665433544665575\n",
      "LOSS train 0.3665433544665575 valid 0.47015246748924255\n",
      "EPOCH 82:\n",
      "  batch 16 loss: 0.4005879135802388\n",
      "  batch 32 loss: 0.42339788004755974\n",
      "  batch 48 loss: 0.40184801910072565\n",
      "LOSS train 0.40184801910072565 valid 0.46839696168899536\n",
      "EPOCH 83:\n",
      "  batch 16 loss: 0.4083360843360424\n",
      "  batch 32 loss: 0.39250122755765915\n",
      "  batch 48 loss: 0.4256617361679673\n",
      "LOSS train 0.4256617361679673 valid 0.46681031584739685\n",
      "EPOCH 84:\n",
      "  batch 16 loss: 0.3746312325820327\n",
      "  batch 32 loss: 0.4669387470930815\n",
      "  batch 48 loss: 0.3696766411885619\n",
      "LOSS train 0.3696766411885619 valid 0.4652293920516968\n",
      "EPOCH 85:\n",
      "  batch 16 loss: 0.4291137298569083\n",
      "  batch 32 loss: 0.3915405813604593\n",
      "  batch 48 loss: 0.40220886189490557\n",
      "LOSS train 0.40220886189490557 valid 0.4638368487358093\n",
      "EPOCH 86:\n",
      "  batch 16 loss: 0.40084763057529926\n",
      "  batch 32 loss: 0.38154728151857853\n",
      "  batch 48 loss: 0.3998879762366414\n",
      "LOSS train 0.3998879762366414 valid 0.46242839097976685\n",
      "EPOCH 87:\n",
      "  batch 16 loss: 0.36193891055881977\n",
      "  batch 32 loss: 0.3651702832430601\n",
      "  batch 48 loss: 0.44213548582047224\n",
      "LOSS train 0.44213548582047224 valid 0.46101635694503784\n",
      "EPOCH 88:\n",
      "  batch 16 loss: 0.3783790245652199\n",
      "  batch 32 loss: 0.4366710167378187\n",
      "  batch 48 loss: 0.33505476266145706\n",
      "LOSS train 0.33505476266145706 valid 0.4597148001194\n",
      "EPOCH 89:\n",
      "  batch 16 loss: 0.4325393969193101\n",
      "  batch 32 loss: 0.31539057940244675\n",
      "  batch 48 loss: 0.3478857963345945\n",
      "LOSS train 0.3478857963345945 valid 0.45840033888816833\n",
      "EPOCH 90:\n",
      "  batch 16 loss: 0.40890189073979855\n",
      "  batch 32 loss: 0.4181206179782748\n",
      "  batch 48 loss: 0.35850359592586756\n",
      "LOSS train 0.35850359592586756 valid 0.4573517441749573\n",
      "EPOCH 91:\n",
      "  batch 16 loss: 0.28295019268989563\n",
      "  batch 32 loss: 0.4264249987900257\n",
      "  batch 48 loss: 0.40728223882615566\n",
      "LOSS train 0.40728223882615566 valid 0.4561672806739807\n",
      "EPOCH 92:\n",
      "  batch 16 loss: 0.35619862750172615\n",
      "  batch 32 loss: 0.3988698599860072\n",
      "  batch 48 loss: 0.3349401652812958\n",
      "LOSS train 0.3349401652812958 valid 0.45496898889541626\n",
      "EPOCH 93:\n",
      "  batch 16 loss: 0.4141734968870878\n",
      "  batch 32 loss: 0.3493440207093954\n",
      "  batch 48 loss: 0.41170963272452354\n",
      "LOSS train 0.41170963272452354 valid 0.45415163040161133\n",
      "EPOCH 94:\n",
      "  batch 16 loss: 0.31802663020789623\n",
      "  batch 32 loss: 0.41806624829769135\n",
      "  batch 48 loss: 0.37595270620658994\n",
      "LOSS train 0.37595270620658994 valid 0.45317167043685913\n",
      "EPOCH 95:\n",
      "  batch 16 loss: 0.35564161743968725\n",
      "  batch 32 loss: 0.3639831133186817\n",
      "  batch 48 loss: 0.3955046087503433\n",
      "LOSS train 0.3955046087503433 valid 0.4523234963417053\n",
      "EPOCH 96:\n",
      "  batch 16 loss: 0.365018755197525\n",
      "  batch 32 loss: 0.3876374764367938\n",
      "  batch 48 loss: 0.3580993264913559\n",
      "LOSS train 0.3580993264913559 valid 0.45170941948890686\n",
      "EPOCH 97:\n",
      "  batch 16 loss: 0.34603365138173103\n",
      "  batch 32 loss: 0.42793937027454376\n",
      "  batch 48 loss: 0.3078318005427718\n",
      "LOSS train 0.3078318005427718 valid 0.45086348056793213\n",
      "EPOCH 98:\n",
      "  batch 16 loss: 0.421313707716763\n",
      "  batch 32 loss: 0.33403392881155014\n",
      "  batch 48 loss: 0.38934349082410336\n",
      "LOSS train 0.38934349082410336 valid 0.4500994086265564\n",
      "EPOCH 99:\n",
      "  batch 16 loss: 0.41070073936134577\n",
      "  batch 32 loss: 0.25898755341768265\n",
      "  batch 48 loss: 0.40071025863289833\n",
      "LOSS train 0.40071025863289833 valid 0.4493173658847809\n",
      "EPOCH 100:\n",
      "  batch 16 loss: 0.3959011686965823\n",
      "  batch 32 loss: 0.28013003431260586\n",
      "  batch 48 loss: 0.41505824448540807\n",
      "LOSS train 0.41505824448540807 valid 0.4488615095615387\n",
      "EPOCH 101:\n",
      "  batch 16 loss: 0.34312394820153713\n",
      "  batch 32 loss: 0.4033583980053663\n",
      "  batch 48 loss: 0.3325352524407208\n",
      "LOSS train 0.3325352524407208 valid 0.44824978709220886\n",
      "EPOCH 102:\n",
      "  batch 16 loss: 0.3926555756479502\n",
      "  batch 32 loss: 0.3255615746602416\n",
      "  batch 48 loss: 0.35333973728120327\n",
      "LOSS train 0.35333973728120327 valid 0.4477825164794922\n",
      "EPOCH 103:\n",
      "  batch 16 loss: 0.4024658231064677\n",
      "  batch 32 loss: 0.30315471440553665\n",
      "  batch 48 loss: 0.26251973770558834\n",
      "LOSS train 0.26251973770558834 valid 0.44730043411254883\n",
      "EPOCH 104:\n",
      "  batch 16 loss: 0.3933263262733817\n",
      "  batch 32 loss: 0.3677862845361233\n",
      "  batch 48 loss: 0.3116412959061563\n",
      "LOSS train 0.3116412959061563 valid 0.44693925976753235\n",
      "EPOCH 105:\n",
      "  batch 16 loss: 0.3498707739636302\n",
      "  batch 32 loss: 0.34057231480255723\n",
      "  batch 48 loss: 0.32040976639837027\n",
      "LOSS train 0.32040976639837027 valid 0.4465998709201813\n",
      "EPOCH 106:\n",
      "  batch 16 loss: 0.3647301448509097\n",
      "  batch 32 loss: 0.32850687857717276\n",
      "  batch 48 loss: 0.35112330224364996\n",
      "LOSS train 0.35112330224364996 valid 0.44625818729400635\n",
      "EPOCH 107:\n",
      "  batch 16 loss: 0.3252262957394123\n",
      "  batch 32 loss: 0.46443362440913916\n",
      "  batch 48 loss: 0.2767854845151305\n",
      "LOSS train 0.2767854845151305 valid 0.44582411646842957\n",
      "EPOCH 108:\n",
      "  batch 16 loss: 0.2933959513902664\n",
      "  batch 32 loss: 0.3656748691573739\n",
      "  batch 48 loss: 0.43323240894824266\n",
      "LOSS train 0.43323240894824266 valid 0.44558948278427124\n",
      "EPOCH 109:\n",
      "  batch 16 loss: 0.3467153273522854\n",
      "  batch 32 loss: 0.27497558761388063\n",
      "  batch 48 loss: 0.4006780171766877\n",
      "LOSS train 0.4006780171766877 valid 0.44541135430336\n",
      "EPOCH 110:\n",
      "  batch 16 loss: 0.4309802344068885\n",
      "  batch 32 loss: 0.3235632907599211\n",
      "  batch 48 loss: 0.3053858713246882\n",
      "LOSS train 0.3053858713246882 valid 0.4447318911552429\n",
      "EPOCH 111:\n",
      "  batch 16 loss: 0.3195104426704347\n",
      "  batch 32 loss: 0.2826607432216406\n",
      "  batch 48 loss: 0.39590360689908266\n",
      "LOSS train 0.39590360689908266 valid 0.44466739892959595\n",
      "EPOCH 112:\n",
      "  batch 16 loss: 0.3306689905002713\n",
      "  batch 32 loss: 0.3271839995868504\n",
      "  batch 48 loss: 0.34414613619446754\n",
      "LOSS train 0.34414613619446754 valid 0.44447892904281616\n",
      "EPOCH 113:\n",
      "  batch 16 loss: 0.4573836922645569\n",
      "  batch 32 loss: 0.2874556831084192\n",
      "  batch 48 loss: 0.2933705821633339\n",
      "LOSS train 0.2933705821633339 valid 0.4444180130958557\n",
      "EPOCH 114:\n",
      "  batch 16 loss: 0.34356375969946384\n",
      "  batch 32 loss: 0.3409571214579046\n",
      "  batch 48 loss: 0.3152327495627105\n",
      "LOSS train 0.3152327495627105 valid 0.44412773847579956\n",
      "EPOCH 115:\n",
      "  batch 16 loss: 0.33398363646119833\n",
      "  batch 32 loss: 0.33414755668491125\n",
      "  batch 48 loss: 0.24461419181898236\n",
      "LOSS train 0.24461419181898236 valid 0.4439593255519867\n",
      "EPOCH 116:\n",
      "  batch 16 loss: 0.2921521542593837\n",
      "  batch 32 loss: 0.33471811283379793\n",
      "  batch 48 loss: 0.37613652739673853\n",
      "LOSS train 0.37613652739673853 valid 0.4438474774360657\n",
      "EPOCH 117:\n",
      "  batch 16 loss: 0.35816815122962\n",
      "  batch 32 loss: 0.28656933829188347\n",
      "  batch 48 loss: 0.37285235803574324\n",
      "LOSS train 0.37285235803574324 valid 0.4437890648841858\n",
      "EPOCH 118:\n",
      "  batch 16 loss: 0.31544444616883993\n",
      "  batch 32 loss: 0.33384715882129967\n",
      "  batch 48 loss: 0.3201468475162983\n",
      "LOSS train 0.3201468475162983 valid 0.44351041316986084\n",
      "EPOCH 119:\n",
      "  batch 16 loss: 0.30397517839446664\n",
      "  batch 32 loss: 0.31701293401420116\n",
      "  batch 48 loss: 0.3521008472889662\n",
      "LOSS train 0.3521008472889662 valid 0.4438251852989197\n",
      "EPOCH 120:\n",
      "  batch 16 loss: 0.286558844614774\n",
      "  batch 32 loss: 0.3600460598245263\n",
      "  batch 48 loss: 0.4257854549214244\n",
      "LOSS train 0.4257854549214244 valid 0.44354718923568726\n",
      "EPOCH 121:\n",
      "  batch 16 loss: 0.26251634722575545\n",
      "  batch 32 loss: 0.3853853074833751\n",
      "  batch 48 loss: 0.3533066092059016\n",
      "LOSS train 0.3533066092059016 valid 0.4436672329902649\n",
      "EPOCH 122:\n",
      "  batch 16 loss: 0.3563661817461252\n",
      "  batch 32 loss: 0.3196941870264709\n",
      "  batch 48 loss: 0.28691406617872417\n",
      "LOSS train 0.28691406617872417 valid 0.44358065724372864\n",
      "EPOCH 123:\n",
      "  batch 16 loss: 0.3063274025917053\n",
      "  batch 32 loss: 0.37125484459102154\n",
      "  batch 48 loss: 0.33238151390105486\n",
      "LOSS train 0.33238151390105486 valid 0.4436580538749695\n",
      "EPOCH 124:\n",
      "  batch 16 loss: 0.37650798354297876\n",
      "  batch 32 loss: 0.31289789779111743\n",
      "  batch 48 loss: 0.30034064990468323\n",
      "LOSS train 0.30034064990468323 valid 0.4438406825065613\n",
      "EPOCH 125:\n",
      "  batch 16 loss: 0.3644063766114414\n",
      "  batch 32 loss: 0.25166718289256096\n",
      "  batch 48 loss: 0.3487976808100939\n",
      "LOSS train 0.3487976808100939 valid 0.44381511211395264\n",
      "EPOCH 126:\n",
      "  batch 16 loss: 0.3099144382867962\n",
      "  batch 32 loss: 0.2713599381968379\n",
      "  batch 48 loss: 0.4233710686676204\n",
      "LOSS train 0.4233710686676204 valid 0.4438092112541199\n",
      "EPOCH 127:\n",
      "  batch 16 loss: 0.29363551875576377\n",
      "  batch 32 loss: 0.3729689624160528\n",
      "  batch 48 loss: 0.28028568997979164\n",
      "LOSS train 0.28028568997979164 valid 0.44375309348106384\n",
      "EPOCH 128:\n",
      "  batch 16 loss: 0.44427646044641733\n",
      "  batch 32 loss: 0.30877765524201095\n",
      "  batch 48 loss: 0.25854960759170353\n",
      "LOSS train 0.25854960759170353 valid 0.44379478693008423\n",
      "EPOCH 129:\n",
      "  batch 16 loss: 0.3951201131567359\n",
      "  batch 32 loss: 0.3006205605342984\n",
      "  batch 48 loss: 0.26064050383865833\n",
      "LOSS train 0.26064050383865833 valid 0.4439828395843506\n",
      "EPOCH 130:\n",
      "  batch 16 loss: 0.3533898456953466\n",
      "  batch 32 loss: 0.27531007351353765\n",
      "  batch 48 loss: 0.2799780007917434\n",
      "LOSS train 0.2799780007917434 valid 0.44402754306793213\n",
      "EPOCH 131:\n",
      "  batch 16 loss: 0.30127718951553106\n",
      "  batch 32 loss: 0.3752824291586876\n",
      "  batch 48 loss: 0.3302544788457453\n",
      "LOSS train 0.3302544788457453 valid 0.444284051656723\n",
      "EPOCH 132:\n",
      "  batch 16 loss: 0.28899264615029097\n",
      "  batch 32 loss: 0.36057233647443354\n",
      "  batch 48 loss: 0.33846792252734303\n",
      "LOSS train 0.33846792252734303 valid 0.44417452812194824\n",
      "EPOCH 133:\n",
      "  batch 16 loss: 0.3386973771266639\n",
      "  batch 32 loss: 0.23593974066898227\n",
      "  batch 48 loss: 0.34401768166571856\n",
      "LOSS train 0.34401768166571856 valid 0.44423723220825195\n",
      "EPOCH 134:\n",
      "  batch 16 loss: 0.2587420598138124\n",
      "  batch 32 loss: 0.34065928822383285\n",
      "  batch 48 loss: 0.35522968228906393\n",
      "LOSS train 0.35522968228906393 valid 0.4443650543689728\n",
      "EPOCH 135:\n",
      "  batch 16 loss: 0.31164639070630074\n",
      "  batch 32 loss: 0.4063427373766899\n",
      "  batch 48 loss: 0.2568731615319848\n",
      "LOSS train 0.2568731615319848 valid 0.44461286067962646\n",
      "EPOCH 136:\n",
      "  batch 16 loss: 0.3026655144058168\n",
      "  batch 32 loss: 0.23282172670587897\n",
      "  batch 48 loss: 0.32494456274434924\n",
      "LOSS train 0.32494456274434924 valid 0.4449605941772461\n",
      "EPOCH 137:\n",
      "  batch 16 loss: 0.3274782747030258\n",
      "  batch 32 loss: 0.3605547556653619\n",
      "  batch 48 loss: 0.2432338297367096\n",
      "LOSS train 0.2432338297367096 valid 0.44519203901290894\n",
      "EPOCH 138:\n",
      "  batch 16 loss: 0.2935235998593271\n",
      "  batch 32 loss: 0.29897059639915824\n",
      "  batch 48 loss: 0.3023709184490144\n",
      "LOSS train 0.3023709184490144 valid 0.445505291223526\n",
      "EPOCH 139:\n",
      "  batch 16 loss: 0.29038549354299903\n",
      "  batch 32 loss: 0.2887272243387997\n",
      "  batch 48 loss: 0.29130075918510556\n",
      "LOSS train 0.29130075918510556 valid 0.44567954540252686\n",
      "EPOCH 140:\n",
      "  batch 16 loss: 0.3307299572043121\n",
      "  batch 32 loss: 0.33549448987469077\n",
      "  batch 48 loss: 0.2590123370755464\n",
      "LOSS train 0.2590123370755464 valid 0.4458829164505005\n",
      "EPOCH 141:\n",
      "  batch 16 loss: 0.27920194575563073\n",
      "  batch 32 loss: 0.32369696255773306\n",
      "  batch 48 loss: 0.2791495369747281\n",
      "LOSS train 0.2791495369747281 valid 0.4462079703807831\n",
      "EPOCH 142:\n",
      "  batch 16 loss: 0.32542496966198087\n",
      "  batch 32 loss: 0.22796432324685156\n",
      "  batch 48 loss: 0.39302854891866446\n",
      "LOSS train 0.39302854891866446 valid 0.4463198184967041\n",
      "EPOCH 143:\n",
      "  batch 16 loss: 0.3599505336023867\n",
      "  batch 32 loss: 0.27319871773943305\n",
      "  batch 48 loss: 0.2829253631643951\n",
      "LOSS train 0.2829253631643951 valid 0.4466364085674286\n",
      "EPOCH 144:\n",
      "  batch 16 loss: 0.380336525849998\n",
      "  batch 32 loss: 0.292701160768047\n",
      "  batch 48 loss: 0.27751108235679567\n",
      "LOSS train 0.27751108235679567 valid 0.44694846868515015\n",
      "EPOCH 145:\n",
      "  batch 16 loss: 0.2648337490390986\n",
      "  batch 32 loss: 0.33318591210991144\n",
      "  batch 48 loss: 0.2837799231056124\n",
      "LOSS train 0.2837799231056124 valid 0.4468422830104828\n",
      "EPOCH 146:\n",
      "  batch 16 loss: 0.274998243432492\n",
      "  batch 32 loss: 0.20558650884777308\n",
      "  batch 48 loss: 0.4046266404911876\n",
      "LOSS train 0.4046266404911876 valid 0.4473775029182434\n",
      "EPOCH 147:\n",
      "  batch 16 loss: 0.2729634896386415\n",
      "  batch 32 loss: 0.3482429694849998\n",
      "  batch 48 loss: 0.32107844948768616\n",
      "LOSS train 0.32107844948768616 valid 0.44762200117111206\n",
      "EPOCH 148:\n",
      "  batch 16 loss: 0.3558194828219712\n",
      "  batch 32 loss: 0.2245428920723498\n",
      "  batch 48 loss: 0.42892218520864844\n",
      "LOSS train 0.42892218520864844 valid 0.447591632604599\n",
      "EPOCH 149:\n",
      "  batch 16 loss: 0.23084196355193853\n",
      "  batch 32 loss: 0.3665788611397147\n",
      "  batch 48 loss: 0.3125711928587407\n",
      "LOSS train 0.3125711928587407 valid 0.4478262662887573\n",
      "EPOCH 150:\n",
      "  batch 16 loss: 0.2336166608147323\n",
      "  batch 32 loss: 0.3886983641423285\n",
      "  batch 48 loss: 0.25629062694497406\n",
      "LOSS train 0.25629062694497406 valid 0.44818973541259766\n",
      "EPOCH 151:\n",
      "  batch 16 loss: 0.19474136969074607\n",
      "  batch 32 loss: 0.1927122944034636\n",
      "  batch 48 loss: 0.39186387322843075\n",
      "LOSS train 0.39186387322843075 valid 0.448258638381958\n",
      "EPOCH 152:\n",
      "  batch 16 loss: 0.33967221714556217\n",
      "  batch 32 loss: 0.31794074177742004\n",
      "  batch 48 loss: 0.20055722258985043\n",
      "LOSS train 0.20055722258985043 valid 0.4489513039588928\n",
      "EPOCH 153:\n",
      "  batch 16 loss: 0.32117028441280127\n",
      "  batch 32 loss: 0.25573223177343607\n",
      "  batch 48 loss: 0.338036549044773\n",
      "LOSS train 0.338036549044773 valid 0.4489031434059143\n",
      "EPOCH 154:\n",
      "  batch 16 loss: 0.30846920097246766\n",
      "  batch 32 loss: 0.2342690103687346\n",
      "  batch 48 loss: 0.3639273070730269\n",
      "LOSS train 0.3639273070730269 valid 0.44936829805374146\n",
      "EPOCH 155:\n",
      "  batch 16 loss: 0.2578776048030704\n",
      "  batch 32 loss: 0.3759472188539803\n",
      "  batch 48 loss: 0.21710644382983446\n",
      "LOSS train 0.21710644382983446 valid 0.44916170835494995\n",
      "EPOCH 156:\n",
      "  batch 16 loss: 0.2824080013670027\n",
      "  batch 32 loss: 0.2705225725658238\n",
      "  batch 48 loss: 0.4402413950301707\n",
      "LOSS train 0.4402413950301707 valid 0.4494984745979309\n",
      "EPOCH 157:\n",
      "  batch 16 loss: 0.36930188769474626\n",
      "  batch 32 loss: 0.2288290960714221\n",
      "  batch 48 loss: 0.24142103386111557\n",
      "LOSS train 0.24142103386111557 valid 0.44992631673812866\n",
      "EPOCH 158:\n",
      "  batch 16 loss: 0.387402665335685\n",
      "  batch 32 loss: 0.2502875770442188\n",
      "  batch 48 loss: 0.3168735704384744\n",
      "LOSS train 0.3168735704384744 valid 0.44991299510002136\n",
      "EPOCH 159:\n",
      "  batch 16 loss: 0.33113239193335176\n",
      "  batch 32 loss: 0.2585395423229784\n",
      "  batch 48 loss: 0.2873271720018238\n",
      "LOSS train 0.2873271720018238 valid 0.45046672224998474\n",
      "EPOCH 160:\n",
      "  batch 16 loss: 0.2842108856420964\n",
      "  batch 32 loss: 0.2130673793144524\n",
      "  batch 48 loss: 0.33471238263882697\n",
      "LOSS train 0.33471238263882697 valid 0.4504293203353882\n",
      "EPOCH 161:\n",
      "  batch 16 loss: 0.30155379441566765\n",
      "  batch 32 loss: 0.3209885354153812\n",
      "  batch 48 loss: 0.2528660846874118\n",
      "LOSS train 0.2528660846874118 valid 0.450626015663147\n",
      "EPOCH 162:\n",
      "  batch 16 loss: 0.3506028694100678\n",
      "  batch 32 loss: 0.30047415709123015\n",
      "  batch 48 loss: 0.26961923157796264\n",
      "LOSS train 0.26961923157796264 valid 0.451279878616333\n",
      "EPOCH 163:\n",
      "  batch 16 loss: 0.3353057811036706\n",
      "  batch 32 loss: 0.22921206941828132\n",
      "  batch 48 loss: 0.25478887069039047\n",
      "LOSS train 0.25478887069039047 valid 0.4514991044998169\n",
      "EPOCH 164:\n",
      "  batch 16 loss: 0.2447053489740938\n",
      "  batch 32 loss: 0.2837579767219722\n",
      "  batch 48 loss: 0.3021546150557697\n",
      "LOSS train 0.3021546150557697 valid 0.4518301486968994\n",
      "EPOCH 165:\n",
      "  batch 16 loss: 0.29283040133304894\n",
      "  batch 32 loss: 0.25671146623790264\n",
      "  batch 48 loss: 0.32369511714205146\n",
      "LOSS train 0.32369511714205146 valid 0.4519074857234955\n",
      "EPOCH 166:\n",
      "  batch 16 loss: 0.3098121457733214\n",
      "  batch 32 loss: 0.28979613399133086\n",
      "  batch 48 loss: 0.3097588177770376\n",
      "LOSS train 0.3097588177770376 valid 0.4520567059516907\n",
      "EPOCH 167:\n",
      "  batch 16 loss: 0.3129253974184394\n",
      "  batch 32 loss: 0.17946930648759007\n",
      "  batch 48 loss: 0.3835697849281132\n",
      "LOSS train 0.3835697849281132 valid 0.45260781049728394\n",
      "EPOCH 168:\n",
      "  batch 16 loss: 0.3378851627930999\n",
      "  batch 32 loss: 0.27734728320501745\n",
      "  batch 48 loss: 0.3158862334676087\n",
      "LOSS train 0.3158862334676087 valid 0.4528358578681946\n",
      "EPOCH 169:\n",
      "  batch 16 loss: 0.3524409565143287\n",
      "  batch 32 loss: 0.2408853517845273\n",
      "  batch 48 loss: 0.24233005195856094\n",
      "LOSS train 0.24233005195856094 valid 0.45320925116539\n",
      "EPOCH 170:\n",
      "  batch 16 loss: 0.3077656764071435\n",
      "  batch 32 loss: 0.34861749364063144\n",
      "  batch 48 loss: 0.2557257395237684\n",
      "LOSS train 0.2557257395237684 valid 0.452961266040802\n",
      "EPOCH 171:\n",
      "  batch 16 loss: 0.24921323941089213\n",
      "  batch 32 loss: 0.23452602594625205\n",
      "  batch 48 loss: 0.45267906691879034\n",
      "LOSS train 0.45267906691879034 valid 0.4533010721206665\n",
      "EPOCH 172:\n",
      "  batch 16 loss: 0.31955794896930456\n",
      "  batch 32 loss: 0.3125722664408386\n",
      "  batch 48 loss: 0.2814890476875007\n",
      "LOSS train 0.2814890476875007 valid 0.4533345103263855\n",
      "EPOCH 173:\n",
      "  batch 16 loss: 0.26742258528247476\n",
      "  batch 32 loss: 0.33943252824246883\n",
      "  batch 48 loss: 0.2375609022565186\n",
      "LOSS train 0.2375609022565186 valid 0.4539187252521515\n",
      "EPOCH 174:\n",
      "  batch 16 loss: 0.335342203034088\n",
      "  batch 32 loss: 0.2929620111826807\n",
      "  batch 48 loss: 0.19201134727336466\n",
      "LOSS train 0.19201134727336466 valid 0.4539453387260437\n",
      "EPOCH 175:\n",
      "  batch 16 loss: 0.3357786010019481\n",
      "  batch 32 loss: 0.3253682947251946\n",
      "  batch 48 loss: 0.23372576804831624\n",
      "LOSS train 0.23372576804831624 valid 0.45456182956695557\n",
      "EPOCH 176:\n",
      "  batch 16 loss: 0.27485115150921047\n",
      "  batch 32 loss: 0.35075514134950936\n",
      "  batch 48 loss: 0.2804683833383024\n",
      "LOSS train 0.2804683833383024 valid 0.4548298716545105\n",
      "EPOCH 177:\n",
      "  batch 16 loss: 0.2733531785197556\n",
      "  batch 32 loss: 0.29615727881900966\n",
      "  batch 48 loss: 0.23688561865128577\n",
      "LOSS train 0.23688561865128577 valid 0.45530229806900024\n",
      "EPOCH 178:\n",
      "  batch 16 loss: 0.25837655225768685\n",
      "  batch 32 loss: 0.3485246393829584\n",
      "  batch 48 loss: 0.2639090104494244\n",
      "LOSS train 0.2639090104494244 valid 0.45497989654541016\n",
      "EPOCH 179:\n",
      "  batch 16 loss: 0.30052921874448657\n",
      "  batch 32 loss: 0.2234117859043181\n",
      "  batch 48 loss: 0.309707892825827\n",
      "LOSS train 0.309707892825827 valid 0.4558125436306\n",
      "EPOCH 180:\n",
      "  batch 16 loss: 0.2755929557606578\n",
      "  batch 32 loss: 0.35623679077252746\n",
      "  batch 48 loss: 0.25743922335095704\n",
      "LOSS train 0.25743922335095704 valid 0.45581504702568054\n",
      "EPOCH 181:\n",
      "  batch 16 loss: 0.29983061784878373\n",
      "  batch 32 loss: 0.28848879178985953\n",
      "  batch 48 loss: 0.2437308405060321\n",
      "LOSS train 0.2437308405060321 valid 0.456317663192749\n",
      "EPOCH 182:\n",
      "  batch 16 loss: 0.2936340121086687\n",
      "  batch 32 loss: 0.23318616941105574\n",
      "  batch 48 loss: 0.2803072405513376\n",
      "LOSS train 0.2803072405513376 valid 0.45654118061065674\n",
      "EPOCH 183:\n",
      "  batch 16 loss: 0.23988226801156998\n",
      "  batch 32 loss: 0.2602591186296195\n",
      "  batch 48 loss: 0.277932163560763\n",
      "LOSS train 0.277932163560763 valid 0.45687323808670044\n",
      "EPOCH 184:\n",
      "  batch 16 loss: 0.2593126269057393\n",
      "  batch 32 loss: 0.29268025513738394\n",
      "  batch 48 loss: 0.3074898908380419\n",
      "LOSS train 0.3074898908380419 valid 0.457393079996109\n",
      "EPOCH 185:\n",
      "  batch 16 loss: 0.308023767080158\n",
      "  batch 32 loss: 0.2562745192553848\n",
      "  batch 48 loss: 0.3167077614925802\n",
      "LOSS train 0.3167077614925802 valid 0.4574905037879944\n",
      "EPOCH 186:\n",
      "  batch 16 loss: 0.2522682885173708\n",
      "  batch 32 loss: 0.2928239197935909\n",
      "  batch 48 loss: 0.2857494994532317\n",
      "LOSS train 0.2857494994532317 valid 0.4577298164367676\n",
      "EPOCH 187:\n",
      "  batch 16 loss: 0.25961017911322415\n",
      "  batch 32 loss: 0.28500692755915225\n",
      "  batch 48 loss: 0.30227816477417946\n",
      "LOSS train 0.30227816477417946 valid 0.45832115411758423\n",
      "EPOCH 188:\n",
      "  batch 16 loss: 0.2233599980827421\n",
      "  batch 32 loss: 0.29262702562846243\n",
      "  batch 48 loss: 0.29747737175785005\n",
      "LOSS train 0.29747737175785005 valid 0.4584552049636841\n",
      "EPOCH 189:\n",
      "  batch 16 loss: 0.31525551015511155\n",
      "  batch 32 loss: 0.27600995812099427\n",
      "  batch 48 loss: 0.24451831122860312\n",
      "LOSS train 0.24451831122860312 valid 0.4587617516517639\n",
      "EPOCH 190:\n",
      "  batch 16 loss: 0.28399142785929143\n",
      "  batch 32 loss: 0.2927273197565228\n",
      "  batch 48 loss: 0.32069375133141875\n",
      "LOSS train 0.32069375133141875 valid 0.45909327268600464\n",
      "EPOCH 191:\n",
      "  batch 16 loss: 0.2878034505993128\n",
      "  batch 32 loss: 0.28592818742617965\n",
      "  batch 48 loss: 0.24161212076433003\n",
      "LOSS train 0.24161212076433003 valid 0.45923858880996704\n",
      "EPOCH 192:\n",
      "  batch 16 loss: 0.25087863742373884\n",
      "  batch 32 loss: 0.30840271431952715\n",
      "  batch 48 loss: 0.2959740635706112\n",
      "LOSS train 0.2959740635706112 valid 0.4595903158187866\n",
      "EPOCH 193:\n",
      "  batch 16 loss: 0.2772728344425559\n",
      "  batch 32 loss: 0.243372238939628\n",
      "  batch 48 loss: 0.3020087508484721\n",
      "LOSS train 0.3020087508484721 valid 0.4595200717449188\n",
      "EPOCH 194:\n",
      "  batch 16 loss: 0.3559693982824683\n",
      "  batch 32 loss: 0.2677973099052906\n",
      "  batch 48 loss: 0.20588786620646715\n",
      "LOSS train 0.20588786620646715 valid 0.45995616912841797\n",
      "EPOCH 195:\n",
      "  batch 16 loss: 0.2933099092915654\n",
      "  batch 32 loss: 0.27631287928670645\n",
      "  batch 48 loss: 0.311437017749995\n",
      "LOSS train 0.311437017749995 valid 0.4602047801017761\n",
      "EPOCH 196:\n",
      "  batch 16 loss: 0.23551583127118647\n",
      "  batch 32 loss: 0.2601541073527187\n",
      "  batch 48 loss: 0.28262749197892845\n",
      "LOSS train 0.28262749197892845 valid 0.461169570684433\n",
      "EPOCH 197:\n",
      "  batch 16 loss: 0.19209245359525084\n",
      "  batch 32 loss: 0.309642969397828\n",
      "  batch 48 loss: 0.30440161330625415\n",
      "LOSS train 0.30440161330625415 valid 0.461217999458313\n",
      "EPOCH 198:\n",
      "  batch 16 loss: 0.40083226142451167\n",
      "  batch 32 loss: 0.2518209599656984\n",
      "  batch 48 loss: 0.22837971383705735\n",
      "LOSS train 0.22837971383705735 valid 0.46082204580307007\n",
      "EPOCH 199:\n",
      "  batch 16 loss: 0.2650172944413498\n",
      "  batch 32 loss: 0.3256265022791922\n",
      "  batch 48 loss: 0.2496079751290381\n",
      "LOSS train 0.2496079751290381 valid 0.4611005187034607\n",
      "EPOCH 200:\n",
      "  batch 16 loss: 0.3782718686852604\n",
      "  batch 32 loss: 0.19257564470171928\n",
      "  batch 48 loss: 0.26023930916562676\n",
      "LOSS train 0.26023930916562676 valid 0.4616127014160156\n",
      "EPOCH 201:\n",
      "  batch 16 loss: 0.22191048227250576\n",
      "  batch 32 loss: 0.2920857479330152\n",
      "  batch 48 loss: 0.25293580803554505\n",
      "LOSS train 0.25293580803554505 valid 0.46232426166534424\n",
      "EPOCH 202:\n",
      "  batch 16 loss: 0.2783294061664492\n",
      "  batch 32 loss: 0.37399386696051806\n",
      "  batch 48 loss: 0.16924380511045456\n",
      "LOSS train 0.16924380511045456 valid 0.46237456798553467\n",
      "EPOCH 203:\n",
      "  batch 16 loss: 0.30552349239587784\n",
      "  batch 32 loss: 0.28020586096681654\n",
      "  batch 48 loss: 0.30119596142321825\n",
      "LOSS train 0.30119596142321825 valid 0.46229827404022217\n",
      "EPOCH 204:\n",
      "  batch 16 loss: 0.2268769950605929\n",
      "  batch 32 loss: 0.2537695753853768\n",
      "  batch 48 loss: 0.30289203580468893\n",
      "LOSS train 0.30289203580468893 valid 0.4631098508834839\n",
      "EPOCH 205:\n",
      "  batch 16 loss: 0.16756237391382456\n",
      "  batch 32 loss: 0.2822129959240556\n",
      "  batch 48 loss: 0.2652125465683639\n",
      "LOSS train 0.2652125465683639 valid 0.46372437477111816\n",
      "EPOCH 206:\n",
      "  batch 16 loss: 0.20591775234788656\n",
      "  batch 32 loss: 0.1847495671827346\n",
      "  batch 48 loss: 0.3436874817125499\n",
      "LOSS train 0.3436874817125499 valid 0.46372759342193604\n",
      "EPOCH 207:\n",
      "  batch 16 loss: 0.29276717314496636\n",
      "  batch 32 loss: 0.2815961509477347\n",
      "  batch 48 loss: 0.2719753982964903\n",
      "LOSS train 0.2719753982964903 valid 0.4638065993785858\n",
      "EPOCH 208:\n",
      "  batch 16 loss: 0.2876436609076336\n",
      "  batch 32 loss: 0.24232848919928074\n",
      "  batch 48 loss: 0.1825953903608024\n",
      "LOSS train 0.1825953903608024 valid 0.4642490744590759\n",
      "EPOCH 209:\n",
      "  batch 16 loss: 0.24188133073039353\n",
      "  batch 32 loss: 0.2931206616340205\n",
      "  batch 48 loss: 0.3317368165589869\n",
      "LOSS train 0.3317368165589869 valid 0.46431225538253784\n",
      "EPOCH 210:\n",
      "  batch 16 loss: 0.1927497813012451\n",
      "  batch 32 loss: 0.29578662221319973\n",
      "  batch 48 loss: 0.2363825524225831\n",
      "LOSS train 0.2363825524225831 valid 0.46466976404190063\n",
      "EPOCH 211:\n",
      "  batch 16 loss: 0.28196811070665717\n",
      "  batch 32 loss: 0.23491113539785147\n",
      "  batch 48 loss: 0.2828013973776251\n",
      "LOSS train 0.2828013973776251 valid 0.4645763635635376\n",
      "EPOCH 212:\n",
      "  batch 16 loss: 0.3542514485307038\n",
      "  batch 32 loss: 0.1668588825268671\n",
      "  batch 48 loss: 0.2888225542847067\n",
      "LOSS train 0.2888225542847067 valid 0.4650900959968567\n",
      "EPOCH 213:\n",
      "  batch 16 loss: 0.29282213281840086\n",
      "  batch 32 loss: 0.3405365066137165\n",
      "  batch 48 loss: 0.1922856163000688\n",
      "LOSS train 0.1922856163000688 valid 0.46540284156799316\n",
      "EPOCH 214:\n",
      "  batch 16 loss: 0.21992689790204167\n",
      "  batch 32 loss: 0.31314994930289686\n",
      "  batch 48 loss: 0.3089194279164076\n",
      "LOSS train 0.3089194279164076 valid 0.46573084592819214\n",
      "EPOCH 215:\n",
      "  batch 16 loss: 0.20613348903134465\n",
      "  batch 32 loss: 0.3387376149185002\n",
      "  batch 48 loss: 0.23859249893575907\n",
      "LOSS train 0.23859249893575907 valid 0.46598273515701294\n",
      "EPOCH 216:\n",
      "  batch 16 loss: 0.20034731063060462\n",
      "  batch 32 loss: 0.4364277278073132\n",
      "  batch 48 loss: 0.19476410560309887\n",
      "LOSS train 0.19476410560309887 valid 0.4666266441345215\n",
      "EPOCH 217:\n",
      "  batch 16 loss: 0.20249367761425674\n",
      "  batch 32 loss: 0.2963052033446729\n",
      "  batch 48 loss: 0.2310780545230955\n",
      "LOSS train 0.2310780545230955 valid 0.4669247269630432\n",
      "EPOCH 218:\n",
      "  batch 16 loss: 0.17774588277097791\n",
      "  batch 32 loss: 0.3922944238875061\n",
      "  batch 48 loss: 0.27695195411797613\n",
      "LOSS train 0.27695195411797613 valid 0.46705591678619385\n",
      "EPOCH 219:\n",
      "  batch 16 loss: 0.27666454622521996\n",
      "  batch 32 loss: 0.28908791905269027\n",
      "  batch 48 loss: 0.22723083663731813\n",
      "LOSS train 0.22723083663731813 valid 0.4676925241947174\n",
      "EPOCH 220:\n",
      "  batch 16 loss: 0.28699461137875915\n",
      "  batch 32 loss: 0.24358547758311033\n",
      "  batch 48 loss: 0.2518064472824335\n",
      "LOSS train 0.2518064472824335 valid 0.46775493025779724\n",
      "EPOCH 221:\n",
      "  batch 16 loss: 0.31035722815431654\n",
      "  batch 32 loss: 0.29456037108320743\n",
      "  batch 48 loss: 0.22509066201746464\n",
      "LOSS train 0.22509066201746464 valid 0.46807822585105896\n",
      "EPOCH 222:\n",
      "  batch 16 loss: 0.2862957287579775\n",
      "  batch 32 loss: 0.269727170933038\n",
      "  batch 48 loss: 0.28246493521146476\n",
      "LOSS train 0.28246493521146476 valid 0.4683840274810791\n",
      "EPOCH 223:\n",
      "  batch 16 loss: 0.21802342543378472\n",
      "  batch 32 loss: 0.2717106200288981\n",
      "  batch 48 loss: 0.2809064050670713\n",
      "LOSS train 0.2809064050670713 valid 0.4685818552970886\n",
      "EPOCH 224:\n",
      "  batch 16 loss: 0.2885715840384364\n",
      "  batch 32 loss: 0.3457726549822837\n",
      "  batch 48 loss: 0.1461583198979497\n",
      "LOSS train 0.1461583198979497 valid 0.46958062052726746\n",
      "EPOCH 225:\n",
      "  batch 16 loss: 0.3097410034388304\n",
      "  batch 32 loss: 0.23693793336860836\n",
      "  batch 48 loss: 0.27562719373963773\n",
      "LOSS train 0.27562719373963773 valid 0.4694487452507019\n",
      "EPOCH 226:\n",
      "  batch 16 loss: 0.18843242269940674\n",
      "  batch 32 loss: 0.32763143349438906\n",
      "  batch 48 loss: 0.21275095164310187\n",
      "LOSS train 0.21275095164310187 valid 0.4703470468521118\n",
      "EPOCH 227:\n",
      "  batch 16 loss: 0.1874178785365075\n",
      "  batch 32 loss: 0.25651051942259073\n",
      "  batch 48 loss: 0.2826734750997275\n",
      "LOSS train 0.2826734750997275 valid 0.4706159830093384\n",
      "EPOCH 228:\n",
      "  batch 16 loss: 0.2426281285006553\n",
      "  batch 32 loss: 0.21853796765208244\n",
      "  batch 48 loss: 0.26460354635491967\n",
      "LOSS train 0.26460354635491967 valid 0.4704190492630005\n",
      "EPOCH 229:\n",
      "  batch 16 loss: 0.1874707480892539\n",
      "  batch 32 loss: 0.3007351974956691\n",
      "  batch 48 loss: 0.30115530535113066\n",
      "LOSS train 0.30115530535113066 valid 0.4707441031932831\n",
      "EPOCH 230:\n",
      "  batch 16 loss: 0.19383448432199657\n",
      "  batch 32 loss: 0.31006209168117493\n",
      "  batch 48 loss: 0.2990534014534205\n",
      "LOSS train 0.2990534014534205 valid 0.47126057744026184\n",
      "EPOCH 231:\n",
      "  batch 16 loss: 0.22302654455415905\n",
      "  batch 32 loss: 0.32522082794457674\n",
      "  batch 48 loss: 0.19165691593661904\n",
      "LOSS train 0.19165691593661904 valid 0.4710270166397095\n",
      "EPOCH 232:\n",
      "  batch 16 loss: 0.32961881812661886\n",
      "  batch 32 loss: 0.17880207730922848\n",
      "  batch 48 loss: 0.25754412671085447\n",
      "LOSS train 0.25754412671085447 valid 0.4716402590274811\n",
      "EPOCH 233:\n",
      "  batch 16 loss: 0.22504027653485537\n",
      "  batch 32 loss: 0.32156017841771245\n",
      "  batch 48 loss: 0.2124325530603528\n",
      "LOSS train 0.2124325530603528 valid 0.4718446135520935\n",
      "EPOCH 234:\n",
      "  batch 16 loss: 0.303834383841604\n",
      "  batch 32 loss: 0.2501256153918803\n",
      "  batch 48 loss: 0.2785822927253321\n",
      "LOSS train 0.2785822927253321 valid 0.4722456634044647\n",
      "EPOCH 235:\n",
      "  batch 16 loss: 0.20529185992199928\n",
      "  batch 32 loss: 0.34051047498360276\n",
      "  batch 48 loss: 0.1854865336790681\n",
      "LOSS train 0.1854865336790681 valid 0.47215545177459717\n",
      "EPOCH 236:\n",
      "  batch 16 loss: 0.35617989860475063\n",
      "  batch 32 loss: 0.20723710698075593\n",
      "  batch 48 loss: 0.25318817468360066\n",
      "LOSS train 0.25318817468360066 valid 0.4727461040019989\n",
      "EPOCH 237:\n",
      "  batch 16 loss: 0.323539660545066\n",
      "  batch 32 loss: 0.26406683633103967\n",
      "  batch 48 loss: 0.2214731969870627\n",
      "LOSS train 0.2214731969870627 valid 0.4729485511779785\n",
      "EPOCH 238:\n",
      "  batch 16 loss: 0.2889262018725276\n",
      "  batch 32 loss: 0.2873916872777045\n",
      "  batch 48 loss: 0.16880153375677764\n",
      "LOSS train 0.16880153375677764 valid 0.4738430082798004\n",
      "EPOCH 239:\n",
      "  batch 16 loss: 0.30704832286573946\n",
      "  batch 32 loss: 0.20656975382007658\n",
      "  batch 48 loss: 0.2322463379241526\n",
      "LOSS train 0.2322463379241526 valid 0.47417309880256653\n",
      "EPOCH 240:\n",
      "  batch 16 loss: 0.3209671159856953\n",
      "  batch 32 loss: 0.21464181109331548\n",
      "  batch 48 loss: 0.2405451729428023\n",
      "LOSS train 0.2405451729428023 valid 0.4740971028804779\n",
      "EPOCH 241:\n",
      "  batch 16 loss: 0.20728559861890972\n",
      "  batch 32 loss: 0.3257547907996923\n",
      "  batch 48 loss: 0.24278649152256548\n",
      "LOSS train 0.24278649152256548 valid 0.47451654076576233\n",
      "EPOCH 242:\n",
      "  batch 16 loss: 0.26823819195851684\n",
      "  batch 32 loss: 0.2628531123045832\n",
      "  batch 48 loss: 0.23062647529877722\n",
      "LOSS train 0.23062647529877722 valid 0.47479328513145447\n",
      "EPOCH 243:\n",
      "  batch 16 loss: 0.3014653940917924\n",
      "  batch 32 loss: 0.33312150021083653\n",
      "  batch 48 loss: 0.2235924955457449\n",
      "LOSS train 0.2235924955457449 valid 0.4749961197376251\n",
      "EPOCH 244:\n",
      "  batch 16 loss: 0.16329248435795307\n",
      "  batch 32 loss: 0.27794341079425067\n",
      "  batch 48 loss: 0.3233722010627389\n",
      "LOSS train 0.3233722010627389 valid 0.47525298595428467\n",
      "EPOCH 245:\n",
      "  batch 16 loss: 0.21334228687919676\n",
      "  batch 32 loss: 0.29214640939608216\n",
      "  batch 48 loss: 0.21332988305948675\n",
      "LOSS train 0.21332988305948675 valid 0.47554588317871094\n",
      "EPOCH 246:\n",
      "  batch 16 loss: 0.2274522699881345\n",
      "  batch 32 loss: 0.34142660826910287\n",
      "  batch 48 loss: 0.23242428665980697\n",
      "LOSS train 0.23242428665980697 valid 0.47612324357032776\n",
      "EPOCH 247:\n",
      "  batch 16 loss: 0.23924829554744065\n",
      "  batch 32 loss: 0.17976859374903142\n",
      "  batch 48 loss: 0.39958808897063136\n",
      "LOSS train 0.39958808897063136 valid 0.47660988569259644\n",
      "EPOCH 248:\n",
      "  batch 16 loss: 0.2600694363936782\n",
      "  batch 32 loss: 0.22943246690556407\n",
      "  batch 48 loss: 0.24029996199533343\n",
      "LOSS train 0.24029996199533343 valid 0.47656700015068054\n",
      "EPOCH 249:\n",
      "  batch 16 loss: 0.24833495495840907\n",
      "  batch 32 loss: 0.22079754481092095\n",
      "  batch 48 loss: 0.19645435316488147\n",
      "LOSS train 0.19645435316488147 valid 0.47767776250839233\n",
      "EPOCH 250:\n",
      "  batch 16 loss: 0.30649065878242254\n",
      "  batch 32 loss: 0.19957644469104707\n",
      "  batch 48 loss: 0.23807433969341218\n",
      "LOSS train 0.23807433969341218 valid 0.4778817594051361\n",
      "EPOCH 251:\n",
      "  batch 16 loss: 0.2550134644843638\n",
      "  batch 32 loss: 0.20911754434928298\n",
      "  batch 48 loss: 0.3384525440633297\n",
      "LOSS train 0.3384525440633297 valid 0.4779697358608246\n",
      "EPOCH 252:\n",
      "  batch 16 loss: 0.2027045376598835\n",
      "  batch 32 loss: 0.18508534622378647\n",
      "  batch 48 loss: 0.26349319494329393\n",
      "LOSS train 0.26349319494329393 valid 0.4783230423927307\n",
      "EPOCH 253:\n",
      "  batch 16 loss: 0.30747615662403405\n",
      "  batch 32 loss: 0.18410041276365519\n",
      "  batch 48 loss: 0.21716319140978158\n",
      "LOSS train 0.21716319140978158 valid 0.4786597490310669\n",
      "EPOCH 254:\n",
      "  batch 16 loss: 0.22010071482509375\n",
      "  batch 32 loss: 0.282358015072532\n",
      "  batch 48 loss: 0.29754388285800815\n",
      "LOSS train 0.29754388285800815 valid 0.47935912013053894\n",
      "EPOCH 255:\n",
      "  batch 16 loss: 0.2988028528634459\n",
      "  batch 32 loss: 0.2801522604422644\n",
      "  batch 48 loss: 0.18468599731568247\n",
      "LOSS train 0.18468599731568247 valid 0.4797461926937103\n",
      "EPOCH 256:\n",
      "  batch 16 loss: 0.23341053095646203\n",
      "  batch 32 loss: 0.25253439298830926\n",
      "  batch 48 loss: 0.16737140901386738\n",
      "LOSS train 0.16737140901386738 valid 0.4800518751144409\n",
      "EPOCH 257:\n",
      "  batch 16 loss: 0.16955383610911667\n",
      "  batch 32 loss: 0.2544565468560904\n",
      "  batch 48 loss: 0.3825488027650863\n",
      "LOSS train 0.3825488027650863 valid 0.4803214371204376\n",
      "EPOCH 258:\n",
      "  batch 16 loss: 0.21231852262280881\n",
      "  batch 32 loss: 0.37574567599222064\n",
      "  batch 48 loss: 0.227228878531605\n",
      "LOSS train 0.227228878531605 valid 0.48055920004844666\n",
      "EPOCH 259:\n",
      "  batch 16 loss: 0.29592334502376616\n",
      "  batch 32 loss: 0.21454367553815246\n",
      "  batch 48 loss: 0.24480221408884972\n",
      "LOSS train 0.24480221408884972 valid 0.4807518422603607\n",
      "EPOCH 260:\n",
      "  batch 16 loss: 0.2554149384377524\n",
      "  batch 32 loss: 0.2566094440408051\n",
      "  batch 48 loss: 0.2439319472759962\n",
      "LOSS train 0.2439319472759962 valid 0.48109176754951477\n",
      "EPOCH 261:\n",
      "  batch 16 loss: 0.295375497546047\n",
      "  batch 32 loss: 0.19453507871367037\n",
      "  batch 48 loss: 0.3475773090030998\n",
      "LOSS train 0.3475773090030998 valid 0.48129093647003174\n",
      "EPOCH 262:\n",
      "  batch 16 loss: 0.2920339945703745\n",
      "  batch 32 loss: 0.23208584869280457\n",
      "  batch 48 loss: 0.20539074612315744\n",
      "LOSS train 0.20539074612315744 valid 0.48168349266052246\n",
      "EPOCH 263:\n",
      "  batch 16 loss: 0.15397546626627445\n",
      "  batch 32 loss: 0.26087316824123263\n",
      "  batch 48 loss: 0.30175337055698037\n",
      "LOSS train 0.30175337055698037 valid 0.48207101225852966\n",
      "EPOCH 264:\n",
      "  batch 16 loss: 0.2816204138798639\n",
      "  batch 32 loss: 0.2510279269190505\n",
      "  batch 48 loss: 0.20688420196529478\n",
      "LOSS train 0.20688420196529478 valid 0.4821833074092865\n",
      "EPOCH 265:\n",
      "  batch 16 loss: 0.23287496692501009\n",
      "  batch 32 loss: 0.2622729560825974\n",
      "  batch 48 loss: 0.24875274393707514\n",
      "LOSS train 0.24875274393707514 valid 0.48305046558380127\n",
      "EPOCH 266:\n",
      "  batch 16 loss: 0.22135038254782557\n",
      "  batch 32 loss: 0.292009747819975\n",
      "  batch 48 loss: 0.26159966737031937\n",
      "LOSS train 0.26159966737031937 valid 0.4830302596092224\n",
      "EPOCH 267:\n",
      "  batch 16 loss: 0.24162519932724535\n",
      "  batch 32 loss: 0.1992277402896434\n",
      "  batch 48 loss: 0.20970279956236482\n",
      "LOSS train 0.20970279956236482 valid 0.48363304138183594\n",
      "EPOCH 268:\n",
      "  batch 16 loss: 0.19532291102223098\n",
      "  batch 32 loss: 0.30009052669629455\n",
      "  batch 48 loss: 0.22315461793914437\n",
      "LOSS train 0.22315461793914437 valid 0.48340097069740295\n",
      "EPOCH 269:\n",
      "  batch 16 loss: 0.2800248828716576\n",
      "  batch 32 loss: 0.246375777060166\n",
      "  batch 48 loss: 0.19459162233397365\n",
      "LOSS train 0.19459162233397365 valid 0.4832748770713806\n",
      "EPOCH 270:\n",
      "  batch 16 loss: 0.28816799935884774\n",
      "  batch 32 loss: 0.19343201932497323\n",
      "  batch 48 loss: 0.2692057486856356\n",
      "LOSS train 0.2692057486856356 valid 0.48391589522361755\n",
      "EPOCH 271:\n",
      "  batch 16 loss: 0.40039875032380223\n",
      "  batch 32 loss: 0.13168894639238715\n",
      "  batch 48 loss: 0.18556255172006786\n",
      "LOSS train 0.18556255172006786 valid 0.48433512449264526\n",
      "EPOCH 272:\n",
      "  batch 16 loss: 0.2551334503805265\n",
      "  batch 32 loss: 0.20085600554011762\n",
      "  batch 48 loss: 0.24922640656586736\n",
      "LOSS train 0.24922640656586736 valid 0.48448845744132996\n",
      "EPOCH 273:\n",
      "  batch 16 loss: 0.3340339895221405\n",
      "  batch 32 loss: 0.223612145986408\n",
      "  batch 48 loss: 0.16563255025539547\n",
      "LOSS train 0.16563255025539547 valid 0.48438161611557007\n",
      "EPOCH 274:\n",
      "  batch 16 loss: 0.31474361987784505\n",
      "  batch 32 loss: 0.16140476684086025\n",
      "  batch 48 loss: 0.26740663847886026\n",
      "LOSS train 0.26740663847886026 valid 0.48457324504852295\n",
      "EPOCH 275:\n",
      "  batch 16 loss: 0.2443480568472296\n",
      "  batch 32 loss: 0.23361070803366601\n",
      "  batch 48 loss: 0.24664548132568598\n",
      "LOSS train 0.24664548132568598 valid 0.4858330190181732\n",
      "EPOCH 276:\n",
      "  batch 16 loss: 0.21812238404527307\n",
      "  batch 32 loss: 0.3171205037506297\n",
      "  batch 48 loss: 0.20587941980920732\n",
      "LOSS train 0.20587941980920732 valid 0.4858095347881317\n",
      "EPOCH 277:\n",
      "  batch 16 loss: 0.25863227155059576\n",
      "  batch 32 loss: 0.2139432402618695\n",
      "  batch 48 loss: 0.22432886017486453\n",
      "LOSS train 0.22432886017486453 valid 0.4860421121120453\n",
      "EPOCH 278:\n",
      "  batch 16 loss: 0.14144685422070324\n",
      "  batch 32 loss: 0.2249745950102806\n",
      "  batch 48 loss: 0.29434115579351783\n",
      "LOSS train 0.29434115579351783 valid 0.48618921637535095\n",
      "EPOCH 279:\n",
      "  batch 16 loss: 0.18271130649372935\n",
      "  batch 32 loss: 0.19005365041084588\n",
      "  batch 48 loss: 0.2860866994597018\n",
      "LOSS train 0.2860866994597018 valid 0.4862764775753021\n",
      "EPOCH 280:\n",
      "  batch 16 loss: 0.2887660226551816\n",
      "  batch 32 loss: 0.2386913918890059\n",
      "  batch 48 loss: 0.19072112068533897\n",
      "LOSS train 0.19072112068533897 valid 0.4868275225162506\n",
      "EPOCH 281:\n",
      "  batch 16 loss: 0.2309053202625364\n",
      "  batch 32 loss: 0.2920319497352466\n",
      "  batch 48 loss: 0.19952810963150114\n",
      "LOSS train 0.19952810963150114 valid 0.48717400431632996\n",
      "EPOCH 282:\n",
      "  batch 16 loss: 0.2864956692792475\n",
      "  batch 32 loss: 0.19109541189391166\n",
      "  batch 48 loss: 0.2511153742671013\n",
      "LOSS train 0.2511153742671013 valid 0.48743757605552673\n",
      "EPOCH 283:\n",
      "  batch 16 loss: 0.2160543529316783\n",
      "  batch 32 loss: 0.23678742826450616\n",
      "  batch 48 loss: 0.2379725705832243\n",
      "LOSS train 0.2379725705832243 valid 0.4877507984638214\n",
      "EPOCH 284:\n",
      "  batch 16 loss: 0.20756753045134246\n",
      "  batch 32 loss: 0.22894392511807382\n",
      "  batch 48 loss: 0.30689544207416475\n",
      "LOSS train 0.30689544207416475 valid 0.4876644015312195\n",
      "EPOCH 285:\n",
      "  batch 16 loss: 0.2318117010872811\n",
      "  batch 32 loss: 0.19476166821550578\n",
      "  batch 48 loss: 0.16398942633531988\n",
      "LOSS train 0.16398942633531988 valid 0.4881192445755005\n",
      "EPOCH 286:\n",
      "  batch 16 loss: 0.2445381940342486\n",
      "  batch 32 loss: 0.22411974892020226\n",
      "  batch 48 loss: 0.3181036664173007\n",
      "LOSS train 0.3181036664173007 valid 0.48838165402412415\n",
      "EPOCH 287:\n",
      "  batch 16 loss: 0.2512809335021302\n",
      "  batch 32 loss: 0.15772238781210035\n",
      "  batch 48 loss: 0.2598261737730354\n",
      "LOSS train 0.2598261737730354 valid 0.4888688027858734\n",
      "EPOCH 288:\n",
      "  batch 16 loss: 0.14832327980548143\n",
      "  batch 32 loss: 0.28882589004933834\n",
      "  batch 48 loss: 0.2513630222529173\n",
      "LOSS train 0.2513630222529173 valid 0.48876506090164185\n",
      "EPOCH 289:\n",
      "  batch 16 loss: 0.34122000518254936\n",
      "  batch 32 loss: 0.15922982653137296\n",
      "  batch 48 loss: 0.21912596543552354\n",
      "LOSS train 0.21912596543552354 valid 0.48910224437713623\n",
      "EPOCH 290:\n",
      "  batch 16 loss: 0.21423610294004902\n",
      "  batch 32 loss: 0.2370745773660019\n",
      "  batch 48 loss: 0.2298834198154509\n",
      "LOSS train 0.2298834198154509 valid 0.48988762497901917\n",
      "EPOCH 291:\n",
      "  batch 16 loss: 0.23623473336920142\n",
      "  batch 32 loss: 0.2116670268587768\n",
      "  batch 48 loss: 0.29987920611165464\n",
      "LOSS train 0.29987920611165464 valid 0.49018627405166626\n",
      "EPOCH 292:\n",
      "  batch 16 loss: 0.2691702002193779\n",
      "  batch 32 loss: 0.2326284688897431\n",
      "  batch 48 loss: 0.2549096951261163\n",
      "LOSS train 0.2549096951261163 valid 0.4903079867362976\n",
      "EPOCH 293:\n",
      "  batch 16 loss: 0.240250094095245\n",
      "  batch 32 loss: 0.2854529983596876\n",
      "  batch 48 loss: 0.20437666680663824\n",
      "LOSS train 0.20437666680663824 valid 0.490607351064682\n",
      "EPOCH 294:\n",
      "  batch 16 loss: 0.24285763094667345\n",
      "  batch 32 loss: 0.17756833834573627\n",
      "  batch 48 loss: 0.16923163505271077\n",
      "LOSS train 0.16923163505271077 valid 0.4913240969181061\n",
      "EPOCH 295:\n",
      "  batch 16 loss: 0.2310904114274308\n",
      "  batch 32 loss: 0.21421738527715206\n",
      "  batch 48 loss: 0.3002817144151777\n",
      "LOSS train 0.3002817144151777 valid 0.49148809909820557\n",
      "EPOCH 296:\n",
      "  batch 16 loss: 0.17535416036844254\n",
      "  batch 32 loss: 0.2707804134115577\n",
      "  batch 48 loss: 0.2616256242617965\n",
      "LOSS train 0.2616256242617965 valid 0.49147123098373413\n",
      "EPOCH 297:\n",
      "  batch 16 loss: 0.24240345088765025\n",
      "  batch 32 loss: 0.21021674841176718\n",
      "  batch 48 loss: 0.28900572983548045\n",
      "LOSS train 0.28900572983548045 valid 0.492070734500885\n",
      "EPOCH 298:\n",
      "  batch 16 loss: 0.2586541804485023\n",
      "  batch 32 loss: 0.2406230354681611\n",
      "  batch 48 loss: 0.21696632320526987\n",
      "LOSS train 0.21696632320526987 valid 0.49258992075920105\n",
      "EPOCH 299:\n",
      "  batch 16 loss: 0.20164439256768674\n",
      "  batch 32 loss: 0.22882838326040655\n",
      "  batch 48 loss: 0.2592848089989275\n",
      "LOSS train 0.2592848089989275 valid 0.49252593517303467\n",
      "EPOCH 300:\n",
      "  batch 16 loss: 0.19047912897076458\n",
      "  batch 32 loss: 0.3132824948988855\n",
      "  batch 48 loss: 0.23550776997581124\n",
      "LOSS train 0.23550776997581124 valid 0.49275141954421997\n",
      "EPOCH 301:\n",
      "  batch 16 loss: 0.24707390973344445\n",
      "  batch 32 loss: 0.24894991680048406\n",
      "  batch 48 loss: 0.2831225384725258\n",
      "LOSS train 0.2831225384725258 valid 0.4928872585296631\n",
      "EPOCH 302:\n",
      "  batch 16 loss: 0.23921864770818502\n",
      "  batch 32 loss: 0.2723223235225305\n",
      "  batch 48 loss: 0.24553000275045633\n",
      "LOSS train 0.24553000275045633 valid 0.4932510554790497\n",
      "EPOCH 303:\n",
      "  batch 16 loss: 0.22634081169962883\n",
      "  batch 32 loss: 0.1591928046545945\n",
      "  batch 48 loss: 0.2270083036273718\n",
      "LOSS train 0.2270083036273718 valid 0.49393293261528015\n",
      "EPOCH 304:\n",
      "  batch 16 loss: 0.2729042292921804\n",
      "  batch 32 loss: 0.2138859847909771\n",
      "  batch 48 loss: 0.19389521854463965\n",
      "LOSS train 0.19389521854463965 valid 0.4938187897205353\n",
      "EPOCH 305:\n",
      "  batch 16 loss: 0.23230195161886513\n",
      "  batch 32 loss: 0.16269357944838703\n",
      "  batch 48 loss: 0.23943009576760232\n",
      "LOSS train 0.23943009576760232 valid 0.49476826190948486\n",
      "EPOCH 306:\n",
      "  batch 16 loss: 0.2895645257085562\n",
      "  batch 32 loss: 0.2280768952332437\n",
      "  batch 48 loss: 0.21654483675956726\n",
      "LOSS train 0.21654483675956726 valid 0.4946404993534088\n",
      "EPOCH 307:\n",
      "  batch 16 loss: 0.19802677095867693\n",
      "  batch 32 loss: 0.30585132632404566\n",
      "  batch 48 loss: 0.22321380116045475\n",
      "LOSS train 0.22321380116045475 valid 0.4952946603298187\n",
      "EPOCH 308:\n",
      "  batch 16 loss: 0.2832214457448572\n",
      "  batch 32 loss: 0.18534902343526483\n",
      "  batch 48 loss: 0.250266166171059\n",
      "LOSS train 0.250266166171059 valid 0.49482545256614685\n",
      "EPOCH 309:\n",
      "  batch 16 loss: 0.2236274431925267\n",
      "  batch 32 loss: 0.1819823100231588\n",
      "  batch 48 loss: 0.28224119008518755\n",
      "LOSS train 0.28224119008518755 valid 0.49525129795074463\n",
      "EPOCH 310:\n",
      "  batch 16 loss: 0.26564896013587713\n",
      "  batch 32 loss: 0.2315986609319225\n",
      "  batch 48 loss: 0.18606959807220846\n",
      "LOSS train 0.18606959807220846 valid 0.4956265687942505\n",
      "EPOCH 311:\n",
      "  batch 16 loss: 0.1817547600949183\n",
      "  batch 32 loss: 0.21310083894059062\n",
      "  batch 48 loss: 0.28804582240991294\n",
      "LOSS train 0.28804582240991294 valid 0.4962358772754669\n",
      "EPOCH 312:\n",
      "  batch 16 loss: 0.29707751027308404\n",
      "  batch 32 loss: 0.13753772294148803\n",
      "  batch 48 loss: 0.2430387152126059\n",
      "LOSS train 0.2430387152126059 valid 0.4964010417461395\n",
      "EPOCH 313:\n",
      "  batch 16 loss: 0.22327249683439732\n",
      "  batch 32 loss: 0.23745354067068547\n",
      "  batch 48 loss: 0.27682618983089924\n",
      "LOSS train 0.27682618983089924 valid 0.49690407514572144\n",
      "EPOCH 314:\n",
      "  batch 16 loss: 0.21544560766778886\n",
      "  batch 32 loss: 0.2527083868626505\n",
      "  batch 48 loss: 0.2805546913295984\n",
      "LOSS train 0.2805546913295984 valid 0.49717921018600464\n",
      "EPOCH 315:\n",
      "  batch 16 loss: 0.2537380922585726\n",
      "  batch 32 loss: 0.207312410348095\n",
      "  batch 48 loss: 0.2755526357796043\n",
      "LOSS train 0.2755526357796043 valid 0.49756667017936707\n",
      "EPOCH 316:\n",
      "  batch 16 loss: 0.30527450051158667\n",
      "  batch 32 loss: 0.16318573814351112\n",
      "  batch 48 loss: 0.22214927477762103\n",
      "LOSS train 0.22214927477762103 valid 0.49782705307006836\n",
      "EPOCH 317:\n",
      "  batch 16 loss: 0.21887709596194327\n",
      "  batch 32 loss: 0.21743641211651266\n",
      "  batch 48 loss: 0.20747628249228\n",
      "LOSS train 0.20747628249228 valid 0.497758150100708\n",
      "EPOCH 318:\n",
      "  batch 16 loss: 0.13474212470464408\n",
      "  batch 32 loss: 0.282820463180542\n",
      "  batch 48 loss: 0.17576395510695875\n",
      "LOSS train 0.17576395510695875 valid 0.49869266152381897\n",
      "EPOCH 319:\n",
      "  batch 16 loss: 0.2742820035200566\n",
      "  batch 32 loss: 0.29136466048657894\n",
      "  batch 48 loss: 0.14312450570287183\n",
      "LOSS train 0.14312450570287183 valid 0.4985544681549072\n",
      "EPOCH 320:\n",
      "  batch 16 loss: 0.31103514414280653\n",
      "  batch 32 loss: 0.21105637506116182\n",
      "  batch 48 loss: 0.19259753939695656\n",
      "LOSS train 0.19259753939695656 valid 0.4987320601940155\n",
      "EPOCH 321:\n",
      "  batch 16 loss: 0.26520002842880785\n",
      "  batch 32 loss: 0.17416617221897468\n",
      "  batch 48 loss: 0.29642784176394343\n",
      "LOSS train 0.29642784176394343 valid 0.49946364760398865\n",
      "EPOCH 322:\n",
      "  batch 16 loss: 0.3652405581669882\n",
      "  batch 32 loss: 0.14737376174889505\n",
      "  batch 48 loss: 0.1974612507619895\n",
      "LOSS train 0.1974612507619895 valid 0.49992039799690247\n",
      "EPOCH 323:\n",
      "  batch 16 loss: 0.21761515561956912\n",
      "  batch 32 loss: 0.1968914062017575\n",
      "  batch 48 loss: 0.3274962776340544\n",
      "LOSS train 0.3274962776340544 valid 0.5001002550125122\n",
      "EPOCH 324:\n",
      "  batch 16 loss: 0.23741329926997423\n",
      "  batch 32 loss: 0.23051761998794973\n",
      "  batch 48 loss: 0.2137692510150373\n",
      "LOSS train 0.2137692510150373 valid 0.5000187158584595\n",
      "EPOCH 325:\n",
      "  batch 16 loss: 0.186871865298599\n",
      "  batch 32 loss: 0.2743885850068182\n",
      "  batch 48 loss: 0.2779803776938934\n",
      "LOSS train 0.2779803776938934 valid 0.5007942914962769\n",
      "EPOCH 326:\n",
      "  batch 16 loss: 0.14484069490572438\n",
      "  batch 32 loss: 0.2815724026877433\n",
      "  batch 48 loss: 0.26333330082707107\n",
      "LOSS train 0.26333330082707107 valid 0.5009738206863403\n",
      "EPOCH 327:\n",
      "  batch 16 loss: 0.3067106625530869\n",
      "  batch 32 loss: 0.24626341683324426\n",
      "  batch 48 loss: 0.13149306806735694\n",
      "LOSS train 0.13149306806735694 valid 0.501366138458252\n",
      "EPOCH 328:\n",
      "  batch 16 loss: 0.24186470825225115\n",
      "  batch 32 loss: 0.21083472622558475\n",
      "  batch 48 loss: 0.17125617200508714\n",
      "LOSS train 0.17125617200508714 valid 0.5013930797576904\n",
      "EPOCH 329:\n",
      "  batch 16 loss: 0.13050349336117506\n",
      "  batch 32 loss: 0.23740379512310028\n",
      "  batch 48 loss: 0.3411601141560823\n",
      "LOSS train 0.3411601141560823 valid 0.5015817880630493\n",
      "EPOCH 330:\n",
      "  batch 16 loss: 0.2242254054872319\n",
      "  batch 32 loss: 0.19128906162222847\n",
      "  batch 48 loss: 0.2637195533607155\n",
      "LOSS train 0.2637195533607155 valid 0.5018454790115356\n",
      "EPOCH 331:\n",
      "  batch 16 loss: 0.19788284529931843\n",
      "  batch 32 loss: 0.18964629876427352\n",
      "  batch 48 loss: 0.24939639400690794\n",
      "LOSS train 0.24939639400690794 valid 0.5019100904464722\n",
      "EPOCH 332:\n",
      "  batch 16 loss: 0.1728278216905892\n",
      "  batch 32 loss: 0.2563628063071519\n",
      "  batch 48 loss: 0.2585230264812708\n",
      "LOSS train 0.2585230264812708 valid 0.5028457641601562\n",
      "EPOCH 333:\n",
      "  batch 16 loss: 0.226753048482351\n",
      "  batch 32 loss: 0.15476288250647485\n",
      "  batch 48 loss: 0.31680188211612403\n",
      "LOSS train 0.31680188211612403 valid 0.5031629800796509\n",
      "EPOCH 334:\n",
      "  batch 16 loss: 0.31397476489655674\n",
      "  batch 32 loss: 0.13785057701170444\n",
      "  batch 48 loss: 0.19709618017077446\n",
      "LOSS train 0.19709618017077446 valid 0.5034335851669312\n",
      "EPOCH 335:\n",
      "  batch 16 loss: 0.2836464065767359\n",
      "  batch 32 loss: 0.2293669090140611\n",
      "  batch 48 loss: 0.19790285313501954\n",
      "LOSS train 0.19790285313501954 valid 0.5036324262619019\n",
      "EPOCH 336:\n",
      "  batch 16 loss: 0.2225645844009705\n",
      "  batch 32 loss: 0.17931507783941925\n",
      "  batch 48 loss: 0.29715105588547885\n",
      "LOSS train 0.29715105588547885 valid 0.5039840936660767\n",
      "EPOCH 337:\n",
      "  batch 16 loss: 0.25362860644236207\n",
      "  batch 32 loss: 0.23129769566003233\n",
      "  batch 48 loss: 0.1825351684819907\n",
      "LOSS train 0.1825351684819907 valid 0.5041499137878418\n",
      "EPOCH 338:\n",
      "  batch 16 loss: 0.25706710218219087\n",
      "  batch 32 loss: 0.19586603622883558\n",
      "  batch 48 loss: 0.2592013927642256\n",
      "LOSS train 0.2592013927642256 valid 0.5042858123779297\n",
      "EPOCH 339:\n",
      "  batch 16 loss: 0.23725009360350668\n",
      "  batch 32 loss: 0.19586404075380415\n",
      "  batch 48 loss: 0.2687530694529414\n",
      "LOSS train 0.2687530694529414 valid 0.5051814913749695\n",
      "EPOCH 340:\n",
      "  batch 16 loss: 0.2550674753729254\n",
      "  batch 32 loss: 0.2557521478738636\n",
      "  batch 48 loss: 0.1609546587569639\n",
      "LOSS train 0.1609546587569639 valid 0.5046650171279907\n",
      "EPOCH 341:\n",
      "  batch 16 loss: 0.20539408456534147\n",
      "  batch 32 loss: 0.20183870260370895\n",
      "  batch 48 loss: 0.23713659518398345\n",
      "LOSS train 0.23713659518398345 valid 0.5054169297218323\n",
      "EPOCH 342:\n",
      "  batch 16 loss: 0.1649901680648327\n",
      "  batch 32 loss: 0.2398490373743698\n",
      "  batch 48 loss: 0.23284207982942462\n",
      "LOSS train 0.23284207982942462 valid 0.5053344964981079\n",
      "EPOCH 343:\n",
      "  batch 16 loss: 0.19509422400733456\n",
      "  batch 32 loss: 0.3106737992493436\n",
      "  batch 48 loss: 0.2053436879068613\n",
      "LOSS train 0.2053436879068613 valid 0.5058905482292175\n",
      "EPOCH 344:\n",
      "  batch 16 loss: 0.28717341320589185\n",
      "  batch 32 loss: 0.20795818278566003\n",
      "  batch 48 loss: 0.16826133523136377\n",
      "LOSS train 0.16826133523136377 valid 0.5066307783126831\n",
      "EPOCH 345:\n",
      "  batch 16 loss: 0.14661759394221008\n",
      "  batch 32 loss: 0.3490118128247559\n",
      "  batch 48 loss: 0.14851030189311132\n",
      "LOSS train 0.14851030189311132 valid 0.5062062740325928\n",
      "EPOCH 346:\n",
      "  batch 16 loss: 0.280188434291631\n",
      "  batch 32 loss: 0.17223580239806324\n",
      "  batch 48 loss: 0.20598643156699836\n",
      "LOSS train 0.20598643156699836 valid 0.5066877007484436\n",
      "EPOCH 347:\n",
      "  batch 16 loss: 0.1850520590087399\n",
      "  batch 32 loss: 0.2764697385719046\n",
      "  batch 48 loss: 0.15161069063469768\n",
      "LOSS train 0.15161069063469768 valid 0.5069584846496582\n",
      "EPOCH 348:\n",
      "  batch 16 loss: 0.1680255038663745\n",
      "  batch 32 loss: 0.16679024905897677\n",
      "  batch 48 loss: 0.24303609982598573\n",
      "LOSS train 0.24303609982598573 valid 0.507663905620575\n",
      "EPOCH 349:\n",
      "  batch 16 loss: 0.1749568844679743\n",
      "  batch 32 loss: 0.17365889553911984\n",
      "  batch 48 loss: 0.22930458292830735\n",
      "LOSS train 0.22930458292830735 valid 0.5086666345596313\n",
      "EPOCH 350:\n",
      "  batch 16 loss: 0.2463554834248498\n",
      "  batch 32 loss: 0.12504501186776906\n",
      "  batch 48 loss: 0.24929943890310824\n",
      "LOSS train 0.24929943890310824 valid 0.5083789825439453\n",
      "EPOCH 351:\n",
      "  batch 16 loss: 0.34552152873948216\n",
      "  batch 32 loss: 0.1842098725028336\n",
      "  batch 48 loss: 0.2082915900973603\n",
      "LOSS train 0.2082915900973603 valid 0.5078766942024231\n",
      "EPOCH 352:\n",
      "  batch 16 loss: 0.20197737694252282\n",
      "  batch 32 loss: 0.19352444098331034\n",
      "  batch 48 loss: 0.2496714978478849\n",
      "LOSS train 0.2496714978478849 valid 0.5080901980400085\n",
      "EPOCH 353:\n",
      "  batch 16 loss: 0.25184671278111637\n",
      "  batch 32 loss: 0.19399434933438897\n",
      "  batch 48 loss: 0.14854538286454044\n",
      "LOSS train 0.14854538286454044 valid 0.5086559653282166\n",
      "EPOCH 354:\n",
      "  batch 16 loss: 0.2541287431376986\n",
      "  batch 32 loss: 0.23435118794441223\n",
      "  batch 48 loss: 0.16110726550687104\n",
      "LOSS train 0.16110726550687104 valid 0.5089360475540161\n",
      "EPOCH 355:\n",
      "  batch 16 loss: 0.20722244714852422\n",
      "  batch 32 loss: 0.21504595992155373\n",
      "  batch 48 loss: 0.19280912599060684\n",
      "LOSS train 0.19280912599060684 valid 0.5093393921852112\n",
      "EPOCH 356:\n",
      "  batch 16 loss: 0.17502801842056215\n",
      "  batch 32 loss: 0.1854952655849047\n",
      "  batch 48 loss: 0.3027005430194549\n",
      "LOSS train 0.3027005430194549 valid 0.50961834192276\n",
      "EPOCH 357:\n",
      "  batch 16 loss: 0.17141205968800932\n",
      "  batch 32 loss: 0.24198900256305933\n",
      "  batch 48 loss: 0.22472458431730047\n",
      "LOSS train 0.22472458431730047 valid 0.5104765295982361\n",
      "EPOCH 358:\n",
      "  batch 16 loss: 0.20831408654339612\n",
      "  batch 32 loss: 0.1927846847102046\n",
      "  batch 48 loss: 0.2823349211830646\n",
      "LOSS train 0.2823349211830646 valid 0.5108134746551514\n",
      "EPOCH 359:\n",
      "  batch 16 loss: 0.2017853418365121\n",
      "  batch 32 loss: 0.23100303567480296\n",
      "  batch 48 loss: 0.25107175775337964\n",
      "LOSS train 0.25107175775337964 valid 0.5105259418487549\n",
      "EPOCH 360:\n",
      "  batch 16 loss: 0.20434833131730556\n",
      "  batch 32 loss: 0.17938847803452518\n",
      "  batch 48 loss: 0.3101109324488789\n",
      "LOSS train 0.3101109324488789 valid 0.5106838345527649\n",
      "EPOCH 361:\n",
      "  batch 16 loss: 0.26178657985292375\n",
      "  batch 32 loss: 0.18181849992834032\n",
      "  batch 48 loss: 0.1672075904207304\n",
      "LOSS train 0.1672075904207304 valid 0.5112912654876709\n",
      "EPOCH 362:\n",
      "  batch 16 loss: 0.22508681344334036\n",
      "  batch 32 loss: 0.18570187268778682\n",
      "  batch 48 loss: 0.24487857648637146\n",
      "LOSS train 0.24487857648637146 valid 0.5111984014511108\n",
      "EPOCH 363:\n",
      "  batch 16 loss: 0.31184330000542104\n",
      "  batch 32 loss: 0.1954462688881904\n",
      "  batch 48 loss: 0.17155075154732913\n",
      "LOSS train 0.17155075154732913 valid 0.5118643045425415\n",
      "EPOCH 364:\n",
      "  batch 16 loss: 0.1776971893850714\n",
      "  batch 32 loss: 0.23999072145670652\n",
      "  batch 48 loss: 0.1821110833552666\n",
      "LOSS train 0.1821110833552666 valid 0.5118176937103271\n",
      "EPOCH 365:\n",
      "  batch 16 loss: 0.25307406578212976\n",
      "  batch 32 loss: 0.15283095091581345\n",
      "  batch 48 loss: 0.16194785467814654\n",
      "LOSS train 0.16194785467814654 valid 0.5124026536941528\n",
      "EPOCH 366:\n",
      "  batch 16 loss: 0.18403186090290546\n",
      "  batch 32 loss: 0.21334739739540964\n",
      "  batch 48 loss: 0.24343195656547323\n",
      "LOSS train 0.24343195656547323 valid 0.5128308534622192\n",
      "EPOCH 367:\n",
      "  batch 16 loss: 0.16703044273890555\n",
      "  batch 32 loss: 0.1813031944911927\n",
      "  batch 48 loss: 0.2615568642504513\n",
      "LOSS train 0.2615568642504513 valid 0.5127675533294678\n",
      "EPOCH 368:\n",
      "  batch 16 loss: 0.2512873172527179\n",
      "  batch 32 loss: 0.1963959961431101\n",
      "  batch 48 loss: 0.1611491802323144\n",
      "LOSS train 0.1611491802323144 valid 0.5130264759063721\n",
      "EPOCH 369:\n",
      "  batch 16 loss: 0.11539092205930501\n",
      "  batch 32 loss: 0.208972793770954\n",
      "  batch 48 loss: 0.322725233156234\n",
      "LOSS train 0.322725233156234 valid 0.5134268999099731\n",
      "EPOCH 370:\n",
      "  batch 16 loss: 0.2949347343528643\n",
      "  batch 32 loss: 0.19365513534285128\n",
      "  batch 48 loss: 0.16313484776765108\n",
      "LOSS train 0.16313484776765108 valid 0.513860821723938\n",
      "EPOCH 371:\n",
      "  batch 16 loss: 0.2743676104582846\n",
      "  batch 32 loss: 0.14068992738611996\n",
      "  batch 48 loss: 0.23071991105098277\n",
      "LOSS train 0.23071991105098277 valid 0.5142577886581421\n",
      "EPOCH 372:\n",
      "  batch 16 loss: 0.16920606582425535\n",
      "  batch 32 loss: 0.19939423631876707\n",
      "  batch 48 loss: 0.21987128595355898\n",
      "LOSS train 0.21987128595355898 valid 0.5148352384567261\n",
      "EPOCH 373:\n",
      "  batch 16 loss: 0.19316272111609578\n",
      "  batch 32 loss: 0.16416754154488444\n",
      "  batch 48 loss: 0.22577987308613956\n",
      "LOSS train 0.22577987308613956 valid 0.5152550935745239\n",
      "EPOCH 374:\n",
      "  batch 16 loss: 0.277688599890098\n",
      "  batch 32 loss: 0.15893715151469223\n",
      "  batch 48 loss: 0.2526990052429028\n",
      "LOSS train 0.2526990052429028 valid 0.5147908329963684\n",
      "EPOCH 375:\n",
      "  batch 16 loss: 0.13241003942675889\n",
      "  batch 32 loss: 0.31027147092390805\n",
      "  batch 48 loss: 0.20872428896836936\n",
      "LOSS train 0.20872428896836936 valid 0.5152296423912048\n",
      "EPOCH 376:\n",
      "  batch 16 loss: 0.15277523500844836\n",
      "  batch 32 loss: 0.19045705441385508\n",
      "  batch 48 loss: 0.27173993084579706\n",
      "LOSS train 0.27173993084579706 valid 0.5154853463172913\n",
      "EPOCH 377:\n",
      "  batch 16 loss: 0.18574512470513582\n",
      "  batch 32 loss: 0.20264350855723023\n",
      "  batch 48 loss: 0.2204379930626601\n",
      "LOSS train 0.2204379930626601 valid 0.5160558223724365\n",
      "EPOCH 378:\n",
      "  batch 16 loss: 0.23730175977107137\n",
      "  batch 32 loss: 0.13022979034576565\n",
      "  batch 48 loss: 0.23171173979062587\n",
      "LOSS train 0.23171173979062587 valid 0.5164681077003479\n",
      "EPOCH 379:\n",
      "  batch 16 loss: 0.1695522938389331\n",
      "  batch 32 loss: 0.21917595696868375\n",
      "  batch 48 loss: 0.224792149849236\n",
      "LOSS train 0.224792149849236 valid 0.5162896513938904\n",
      "EPOCH 380:\n",
      "  batch 16 loss: 0.137053930782713\n",
      "  batch 32 loss: 0.19793543173000216\n",
      "  batch 48 loss: 0.26578322157729417\n",
      "LOSS train 0.26578322157729417 valid 0.5168442130088806\n",
      "EPOCH 381:\n",
      "  batch 16 loss: 0.16916002362268046\n",
      "  batch 32 loss: 0.24125625379383564\n",
      "  batch 48 loss: 0.22735867567826062\n",
      "LOSS train 0.22735867567826062 valid 0.5170566439628601\n",
      "EPOCH 382:\n",
      "  batch 16 loss: 0.24698635510867462\n",
      "  batch 32 loss: 0.20048272097483277\n",
      "  batch 48 loss: 0.18738715490326285\n",
      "LOSS train 0.18738715490326285 valid 0.5172171592712402\n",
      "EPOCH 383:\n",
      "  batch 16 loss: 0.28129506844561547\n",
      "  batch 32 loss: 0.15178187005221844\n",
      "  batch 48 loss: 0.2426619284087792\n",
      "LOSS train 0.2426619284087792 valid 0.5173349380493164\n",
      "EPOCH 384:\n",
      "  batch 16 loss: 0.205098744307179\n",
      "  batch 32 loss: 0.20492665166966617\n",
      "  batch 48 loss: 0.19853876845445484\n",
      "LOSS train 0.19853876845445484 valid 0.5177467465400696\n",
      "EPOCH 385:\n",
      "  batch 16 loss: 0.18774494662648067\n",
      "  batch 32 loss: 0.24471988703589886\n",
      "  batch 48 loss: 0.1036454745917581\n",
      "LOSS train 0.1036454745917581 valid 0.5184187293052673\n",
      "EPOCH 386:\n",
      "  batch 16 loss: 0.29153749207034707\n",
      "  batch 32 loss: 0.19215440703555942\n",
      "  batch 48 loss: 0.18458284810185432\n",
      "LOSS train 0.18458284810185432 valid 0.5185813903808594\n",
      "EPOCH 387:\n",
      "  batch 16 loss: 0.14934700960293412\n",
      "  batch 32 loss: 0.20942514907801524\n",
      "  batch 48 loss: 0.20949932979419827\n",
      "LOSS train 0.20949932979419827 valid 0.5192760229110718\n",
      "EPOCH 388:\n",
      "  batch 16 loss: 0.17332060274202377\n",
      "  batch 32 loss: 0.19438055373029783\n",
      "  batch 48 loss: 0.2392338253557682\n",
      "LOSS train 0.2392338253557682 valid 0.5193739533424377\n",
      "EPOCH 389:\n",
      "  batch 16 loss: 0.21539102133829147\n",
      "  batch 32 loss: 0.21318274177610874\n",
      "  batch 48 loss: 0.19067814713343978\n",
      "LOSS train 0.19067814713343978 valid 0.519868016242981\n",
      "EPOCH 390:\n",
      "  batch 16 loss: 0.18261037953197956\n",
      "  batch 32 loss: 0.26175788219552487\n",
      "  batch 48 loss: 0.20055536250583827\n",
      "LOSS train 0.20055536250583827 valid 0.5204272270202637\n",
      "EPOCH 391:\n",
      "  batch 16 loss: 0.20159103727201\n",
      "  batch 32 loss: 0.17158272810047492\n",
      "  batch 48 loss: 0.19615631888154894\n",
      "LOSS train 0.19615631888154894 valid 0.5207177400588989\n",
      "EPOCH 392:\n",
      "  batch 16 loss: 0.15871424501528963\n",
      "  batch 32 loss: 0.1967511432012543\n",
      "  batch 48 loss: 0.3197119543328881\n",
      "LOSS train 0.3197119543328881 valid 0.5209978818893433\n",
      "EPOCH 393:\n",
      "  batch 16 loss: 0.20299158932175487\n",
      "  batch 32 loss: 0.23572477558627725\n",
      "  batch 48 loss: 0.23841037205420434\n",
      "LOSS train 0.23841037205420434 valid 0.5216050148010254\n",
      "EPOCH 394:\n",
      "  batch 16 loss: 0.18366565043106675\n",
      "  batch 32 loss: 0.23194574797526002\n",
      "  batch 48 loss: 0.16196598997339606\n",
      "LOSS train 0.16196598997339606 valid 0.5217587947845459\n",
      "EPOCH 395:\n",
      "  batch 16 loss: 0.1433309691492468\n",
      "  batch 32 loss: 0.2497921991162002\n",
      "  batch 48 loss: 0.19729621952865273\n",
      "LOSS train 0.19729621952865273 valid 0.5223957300186157\n",
      "EPOCH 396:\n",
      "  batch 16 loss: 0.16148239583708346\n",
      "  batch 32 loss: 0.3028258893173188\n",
      "  batch 48 loss: 0.17937939753755927\n",
      "LOSS train 0.17937939753755927 valid 0.522374153137207\n",
      "EPOCH 397:\n",
      "  batch 16 loss: 0.2027672499534674\n",
      "  batch 32 loss: 0.20943216094747186\n",
      "  batch 48 loss: 0.1820642498205416\n",
      "LOSS train 0.1820642498205416 valid 0.5233219861984253\n",
      "EPOCH 398:\n",
      "  batch 16 loss: 0.22042573301587254\n",
      "  batch 32 loss: 0.2181025124154985\n",
      "  batch 48 loss: 0.20683292509056628\n",
      "LOSS train 0.20683292509056628 valid 0.5233036279678345\n",
      "EPOCH 399:\n",
      "  batch 16 loss: 0.18838887015590444\n",
      "  batch 32 loss: 0.16820266365539283\n",
      "  batch 48 loss: 0.29869297123514116\n",
      "LOSS train 0.29869297123514116 valid 0.5237458944320679\n",
      "EPOCH 400:\n",
      "  batch 16 loss: 0.13795904867583886\n",
      "  batch 32 loss: 0.2631220333278179\n",
      "  batch 48 loss: 0.1833175765350461\n",
      "LOSS train 0.1833175765350461 valid 0.524483323097229\n",
      "EPOCH 401:\n",
      "  batch 16 loss: 0.2058909210609272\n",
      "  batch 32 loss: 0.13192793331108987\n",
      "  batch 48 loss: 0.19257305981591344\n",
      "LOSS train 0.19257305981591344 valid 0.5244165062904358\n",
      "EPOCH 402:\n",
      "  batch 16 loss: 0.24669212731532753\n",
      "  batch 32 loss: 0.17111555219162256\n",
      "  batch 48 loss: 0.16202389710815623\n",
      "LOSS train 0.16202389710815623 valid 0.5247547626495361\n",
      "EPOCH 403:\n",
      "  batch 16 loss: 0.2910694789607078\n",
      "  batch 32 loss: 0.22424899064935744\n",
      "  batch 48 loss: 0.16620740480720997\n",
      "LOSS train 0.16620740480720997 valid 0.5250592231750488\n",
      "EPOCH 404:\n",
      "  batch 16 loss: 0.2564237565966323\n",
      "  batch 32 loss: 0.15946525475010276\n",
      "  batch 48 loss: 0.16088726092129946\n",
      "LOSS train 0.16088726092129946 valid 0.5262427926063538\n",
      "EPOCH 405:\n",
      "  batch 16 loss: 0.14408514801471028\n",
      "  batch 32 loss: 0.18164977862033993\n",
      "  batch 48 loss: 0.24471892794827\n",
      "LOSS train 0.24471892794827 valid 0.5259008407592773\n",
      "EPOCH 406:\n",
      "  batch 16 loss: 0.10104496381245553\n",
      "  batch 32 loss: 0.1454125416930765\n",
      "  batch 48 loss: 0.2771422720979899\n",
      "LOSS train 0.2771422720979899 valid 0.5264110565185547\n",
      "EPOCH 407:\n",
      "  batch 16 loss: 0.14797281858045608\n",
      "  batch 32 loss: 0.15202721604146063\n",
      "  batch 48 loss: 0.2411417798139155\n",
      "LOSS train 0.2411417798139155 valid 0.526591420173645\n",
      "EPOCH 408:\n",
      "  batch 16 loss: 0.26537616830319166\n",
      "  batch 32 loss: 0.2355192630784586\n",
      "  batch 48 loss: 0.1767722254153341\n",
      "LOSS train 0.1767722254153341 valid 0.5269737839698792\n",
      "EPOCH 409:\n",
      "  batch 16 loss: 0.19136700441595167\n",
      "  batch 32 loss: 0.20586373249534518\n",
      "  batch 48 loss: 0.25317597039975226\n",
      "LOSS train 0.25317597039975226 valid 0.527348518371582\n",
      "EPOCH 410:\n",
      "  batch 16 loss: 0.21991524350596592\n",
      "  batch 32 loss: 0.15899534127674997\n",
      "  batch 48 loss: 0.2221395862288773\n",
      "LOSS train 0.2221395862288773 valid 0.5275152325630188\n",
      "EPOCH 411:\n",
      "  batch 16 loss: 0.27178583637578413\n",
      "  batch 32 loss: 0.27384417271241546\n",
      "  batch 48 loss: 0.10308643302414566\n",
      "LOSS train 0.10308643302414566 valid 0.528227686882019\n",
      "EPOCH 412:\n",
      "  batch 16 loss: 0.21392910997383296\n",
      "  batch 32 loss: 0.17619094636756927\n",
      "  batch 48 loss: 0.2277369648218155\n",
      "LOSS train 0.2277369648218155 valid 0.5282758474349976\n",
      "EPOCH 413:\n",
      "  batch 16 loss: 0.1574126921768766\n",
      "  batch 32 loss: 0.2207232427317649\n",
      "  batch 48 loss: 0.17086349648889154\n",
      "LOSS train 0.17086349648889154 valid 0.5286372900009155\n",
      "EPOCH 414:\n",
      "  batch 16 loss: 0.20489012217149138\n",
      "  batch 32 loss: 0.18293622240889817\n",
      "  batch 48 loss: 0.24772336753085256\n",
      "LOSS train 0.24772336753085256 valid 0.529503583908081\n",
      "EPOCH 415:\n",
      "  batch 16 loss: 0.2117709779413417\n",
      "  batch 32 loss: 0.251797511940822\n",
      "  batch 48 loss: 0.17248417157679796\n",
      "LOSS train 0.17248417157679796 valid 0.5296506285667419\n",
      "EPOCH 416:\n",
      "  batch 16 loss: 0.1769135703216307\n",
      "  batch 32 loss: 0.2086563672637567\n",
      "  batch 48 loss: 0.14960469829384238\n",
      "LOSS train 0.14960469829384238 valid 0.5302903056144714\n",
      "EPOCH 417:\n",
      "  batch 16 loss: 0.24642711505293846\n",
      "  batch 32 loss: 0.21252395049668849\n",
      "  batch 48 loss: 0.10425198537996039\n",
      "LOSS train 0.10425198537996039 valid 0.5305007696151733\n",
      "EPOCH 418:\n",
      "  batch 16 loss: 0.18873060413170606\n",
      "  batch 32 loss: 0.1868681227788329\n",
      "  batch 48 loss: 0.24515444418648258\n",
      "LOSS train 0.24515444418648258 valid 0.5305673480033875\n",
      "EPOCH 419:\n",
      "  batch 16 loss: 0.16796786291524768\n",
      "  batch 32 loss: 0.2129168477258645\n",
      "  batch 48 loss: 0.16435431211721152\n",
      "LOSS train 0.16435431211721152 valid 0.5316036939620972\n",
      "EPOCH 420:\n",
      "  batch 16 loss: 0.18014045408926904\n",
      "  batch 32 loss: 0.20932097034528852\n",
      "  batch 48 loss: 0.21863595399190672\n",
      "LOSS train 0.21863595399190672 valid 0.5319322347640991\n",
      "EPOCH 421:\n",
      "  batch 16 loss: 0.14099562366027385\n",
      "  batch 32 loss: 0.15228820033371449\n",
      "  batch 48 loss: 0.2574369123612996\n",
      "LOSS train 0.2574369123612996 valid 0.5318835973739624\n",
      "EPOCH 422:\n",
      "  batch 16 loss: 0.11904735630378127\n",
      "  batch 32 loss: 0.3176666626241058\n",
      "  batch 48 loss: 0.18599184730555862\n",
      "LOSS train 0.18599184730555862 valid 0.5322442054748535\n",
      "EPOCH 423:\n",
      "  batch 16 loss: 0.2703661129344255\n",
      "  batch 32 loss: 0.20908872349536978\n",
      "  batch 48 loss: 0.10938394034747034\n",
      "LOSS train 0.10938394034747034 valid 0.5326230525970459\n",
      "EPOCH 424:\n",
      "  batch 16 loss: 0.1682081981562078\n",
      "  batch 32 loss: 0.18547220213804394\n",
      "  batch 48 loss: 0.22987568425014615\n",
      "LOSS train 0.22987568425014615 valid 0.533618688583374\n",
      "EPOCH 425:\n",
      "  batch 16 loss: 0.22444570349762216\n",
      "  batch 32 loss: 0.1437879306031391\n",
      "  batch 48 loss: 0.2318905961001292\n",
      "LOSS train 0.2318905961001292 valid 0.5337384939193726\n",
      "EPOCH 426:\n",
      "  batch 16 loss: 0.11386948451399803\n",
      "  batch 32 loss: 0.15229415358044207\n",
      "  batch 48 loss: 0.18846357375150546\n",
      "LOSS train 0.18846357375150546 valid 0.5344343185424805\n",
      "EPOCH 427:\n",
      "  batch 16 loss: 0.2114386090543121\n",
      "  batch 32 loss: 0.16830813698470592\n",
      "  batch 48 loss: 0.20753983198665082\n",
      "LOSS train 0.20753983198665082 valid 0.5342009663581848\n",
      "EPOCH 428:\n",
      "  batch 16 loss: 0.2177096763625741\n",
      "  batch 32 loss: 0.1565345412818715\n",
      "  batch 48 loss: 0.152029104239773\n",
      "LOSS train 0.152029104239773 valid 0.5350395441055298\n",
      "EPOCH 429:\n",
      "  batch 16 loss: 0.18418916402151808\n",
      "  batch 32 loss: 0.22188671818003058\n",
      "  batch 48 loss: 0.16702550690388307\n",
      "LOSS train 0.16702550690388307 valid 0.5355075001716614\n",
      "EPOCH 430:\n",
      "  batch 16 loss: 0.16038580029271543\n",
      "  batch 32 loss: 0.1822970871871803\n",
      "  batch 48 loss: 0.24612052086740732\n",
      "LOSS train 0.24612052086740732 valid 0.5359153747558594\n",
      "EPOCH 431:\n",
      "  batch 16 loss: 0.12684546690434217\n",
      "  batch 32 loss: 0.20896427641855553\n",
      "  batch 48 loss: 0.23173097684048116\n",
      "LOSS train 0.23173097684048116 valid 0.5363548398017883\n",
      "EPOCH 432:\n",
      "  batch 16 loss: 0.12976887437980622\n",
      "  batch 32 loss: 0.18009259109385312\n",
      "  batch 48 loss: 0.25381401053164154\n",
      "LOSS train 0.25381401053164154 valid 0.5362919569015503\n",
      "EPOCH 433:\n",
      "  batch 16 loss: 0.19760193093679845\n",
      "  batch 32 loss: 0.1590857688570395\n",
      "  batch 48 loss: 0.28772457025479525\n",
      "LOSS train 0.28772457025479525 valid 0.5371794700622559\n",
      "EPOCH 434:\n",
      "  batch 16 loss: 0.24749227508436888\n",
      "  batch 32 loss: 0.13320056220982224\n",
      "  batch 48 loss: 0.1833547952119261\n",
      "LOSS train 0.1833547952119261 valid 0.537041187286377\n",
      "EPOCH 435:\n",
      "  batch 16 loss: 0.08687325776554644\n",
      "  batch 32 loss: 0.21726786997169256\n",
      "  batch 48 loss: 0.2659758625086397\n",
      "LOSS train 0.2659758625086397 valid 0.5378021001815796\n",
      "EPOCH 436:\n",
      "  batch 16 loss: 0.2268729783827439\n",
      "  batch 32 loss: 0.13174261548556387\n",
      "  batch 48 loss: 0.21548149606678635\n",
      "LOSS train 0.21548149606678635 valid 0.5382652282714844\n",
      "EPOCH 437:\n",
      "  batch 16 loss: 0.1792433699592948\n",
      "  batch 32 loss: 0.1737313115154393\n",
      "  batch 48 loss: 0.24334161076694727\n",
      "LOSS train 0.24334161076694727 valid 0.538772463798523\n",
      "EPOCH 438:\n",
      "  batch 16 loss: 0.30676547437906265\n",
      "  batch 32 loss: 0.1774117099121213\n",
      "  batch 48 loss: 0.1284005173947662\n",
      "LOSS train 0.1284005173947662 valid 0.5393565893173218\n",
      "EPOCH 439:\n",
      "  batch 16 loss: 0.15333638188894838\n",
      "  batch 32 loss: 0.2721791311050765\n",
      "  batch 48 loss: 0.13384308863896877\n",
      "LOSS train 0.13384308863896877 valid 0.5395627021789551\n",
      "EPOCH 440:\n",
      "  batch 16 loss: 0.2323412218829617\n",
      "  batch 32 loss: 0.17969329352490604\n",
      "  batch 48 loss: 0.23687932349275798\n",
      "LOSS train 0.23687932349275798 valid 0.5397838950157166\n",
      "EPOCH 441:\n",
      "  batch 16 loss: 0.22474969318136573\n",
      "  batch 32 loss: 0.2273791175102815\n",
      "  batch 48 loss: 0.16299768455792218\n",
      "LOSS train 0.16299768455792218 valid 0.5404748320579529\n",
      "EPOCH 442:\n",
      "  batch 16 loss: 0.21393277891911566\n",
      "  batch 32 loss: 0.20221037021838129\n",
      "  batch 48 loss: 0.14669467380736023\n",
      "LOSS train 0.14669467380736023 valid 0.5407850742340088\n",
      "EPOCH 443:\n",
      "  batch 16 loss: 0.1316371961729601\n",
      "  batch 32 loss: 0.1524515764322132\n",
      "  batch 48 loss: 0.15235034690704197\n",
      "LOSS train 0.15235034690704197 valid 0.5413846969604492\n",
      "EPOCH 444:\n",
      "  batch 16 loss: 0.17266882862895727\n",
      "  batch 32 loss: 0.2217740265186876\n",
      "  batch 48 loss: 0.18922458798624575\n",
      "LOSS train 0.18922458798624575 valid 0.5415240526199341\n",
      "EPOCH 445:\n",
      "  batch 16 loss: 0.1613633232191205\n",
      "  batch 32 loss: 0.23105671326629817\n",
      "  batch 48 loss: 0.12374374538194388\n",
      "LOSS train 0.12374374538194388 valid 0.5424543023109436\n",
      "EPOCH 446:\n",
      "  batch 16 loss: 0.21531975199468434\n",
      "  batch 32 loss: 0.17021834140177816\n",
      "  batch 48 loss: 0.11626716946193483\n",
      "LOSS train 0.11626716946193483 valid 0.542454183101654\n",
      "EPOCH 447:\n",
      "  batch 16 loss: 0.1665664075408131\n",
      "  batch 32 loss: 0.2572468788130209\n",
      "  batch 48 loss: 0.1827962672105059\n",
      "LOSS train 0.1827962672105059 valid 0.5430561304092407\n",
      "EPOCH 448:\n",
      "  batch 16 loss: 0.16896740719676018\n",
      "  batch 32 loss: 0.24625838862266392\n",
      "  batch 48 loss: 0.16364331205841154\n",
      "LOSS train 0.16364331205841154 valid 0.5432643890380859\n",
      "EPOCH 449:\n",
      "  batch 16 loss: 0.22662266602856107\n",
      "  batch 32 loss: 0.172453620005399\n",
      "  batch 48 loss: 0.15518908557714894\n",
      "LOSS train 0.15518908557714894 valid 0.5434936881065369\n",
      "EPOCH 450:\n",
      "  batch 16 loss: 0.17741529550403357\n",
      "  batch 32 loss: 0.2168408704455942\n",
      "  batch 48 loss: 0.19296016145381145\n",
      "LOSS train 0.19296016145381145 valid 0.5442202091217041\n",
      "EPOCH 451:\n",
      "  batch 16 loss: 0.12164624535944313\n",
      "  batch 32 loss: 0.17572407482657582\n",
      "  batch 48 loss: 0.18932816106826067\n",
      "LOSS train 0.18932816106826067 valid 0.545072078704834\n",
      "EPOCH 452:\n",
      "  batch 16 loss: 0.13495468150358647\n",
      "  batch 32 loss: 0.16862383426632732\n",
      "  batch 48 loss: 0.2287667078198865\n",
      "LOSS train 0.2287667078198865 valid 0.5459769368171692\n",
      "EPOCH 453:\n",
      "  batch 16 loss: 0.18702912144362926\n",
      "  batch 32 loss: 0.18121562921442091\n",
      "  batch 48 loss: 0.22332861309405416\n",
      "LOSS train 0.22332861309405416 valid 0.5453335046768188\n",
      "EPOCH 454:\n",
      "  batch 16 loss: 0.13820305466651917\n",
      "  batch 32 loss: 0.23213084693998098\n",
      "  batch 48 loss: 0.1730962192232255\n",
      "LOSS train 0.1730962192232255 valid 0.5457966327667236\n",
      "EPOCH 455:\n",
      "  batch 16 loss: 0.22838357544969767\n",
      "  batch 32 loss: 0.14641784795094281\n",
      "  batch 48 loss: 0.23871421767398715\n",
      "LOSS train 0.23871421767398715 valid 0.5462242364883423\n",
      "EPOCH 456:\n",
      "  batch 16 loss: 0.1673672019969672\n",
      "  batch 32 loss: 0.2375476955785416\n",
      "  batch 48 loss: 0.17559458425967023\n",
      "LOSS train 0.17559458425967023 valid 0.5460397601127625\n",
      "EPOCH 457:\n",
      "  batch 16 loss: 0.12640802396344952\n",
      "  batch 32 loss: 0.15466491517145187\n",
      "  batch 48 loss: 0.2152868912089616\n",
      "LOSS train 0.2152868912089616 valid 0.5473530292510986\n",
      "EPOCH 458:\n",
      "  batch 16 loss: 0.17392118624411523\n",
      "  batch 32 loss: 0.2566985663725063\n",
      "  batch 48 loss: 0.13717052160063758\n",
      "LOSS train 0.13717052160063758 valid 0.546991229057312\n",
      "EPOCH 459:\n",
      "  batch 16 loss: 0.15658290381543338\n",
      "  batch 32 loss: 0.1478549407038372\n",
      "  batch 48 loss: 0.2137135558295995\n",
      "LOSS train 0.2137135558295995 valid 0.5480210781097412\n",
      "EPOCH 460:\n",
      "  batch 16 loss: 0.25400077080121264\n",
      "  batch 32 loss: 0.1563031242112629\n",
      "  batch 48 loss: 0.19325023819692433\n",
      "LOSS train 0.19325023819692433 valid 0.5479754209518433\n",
      "EPOCH 461:\n",
      "  batch 16 loss: 0.16957925318274647\n",
      "  batch 32 loss: 0.17856332915835083\n",
      "  batch 48 loss: 0.17459941701963544\n",
      "LOSS train 0.17459941701963544 valid 0.5486620664596558\n",
      "EPOCH 462:\n",
      "  batch 16 loss: 0.16805340291466564\n",
      "  batch 32 loss: 0.14917001797584817\n",
      "  batch 48 loss: 0.1959888919373043\n",
      "LOSS train 0.1959888919373043 valid 0.5491099953651428\n",
      "EPOCH 463:\n",
      "  batch 16 loss: 0.13401779625564814\n",
      "  batch 32 loss: 0.1710024840431288\n",
      "  batch 48 loss: 0.19455751206260175\n",
      "LOSS train 0.19455751206260175 valid 0.5490041971206665\n",
      "EPOCH 464:\n",
      "  batch 16 loss: 0.14440928446128964\n",
      "  batch 32 loss: 0.22569376579485834\n",
      "  batch 48 loss: 0.20409721578471363\n",
      "LOSS train 0.20409721578471363 valid 0.5493500828742981\n",
      "EPOCH 465:\n",
      "  batch 16 loss: 0.13069067150354385\n",
      "  batch 32 loss: 0.2521560618188232\n",
      "  batch 48 loss: 0.22352804779075086\n",
      "LOSS train 0.22352804779075086 valid 0.5499430298805237\n",
      "EPOCH 466:\n",
      "  batch 16 loss: 0.08982215583091602\n",
      "  batch 32 loss: 0.15828444820363075\n",
      "  batch 48 loss: 0.15439993055770174\n",
      "LOSS train 0.15439993055770174 valid 0.550742506980896\n",
      "EPOCH 467:\n",
      "  batch 16 loss: 0.21439301338978112\n",
      "  batch 32 loss: 0.11914303316734731\n",
      "  batch 48 loss: 0.1682064337655902\n",
      "LOSS train 0.1682064337655902 valid 0.5505495071411133\n",
      "EPOCH 468:\n",
      "  batch 16 loss: 0.12977306568063796\n",
      "  batch 32 loss: 0.2022933104308322\n",
      "  batch 48 loss: 0.17743069905554876\n",
      "LOSS train 0.17743069905554876 valid 0.5516147613525391\n",
      "EPOCH 469:\n",
      "  batch 16 loss: 0.20734003378311172\n",
      "  batch 32 loss: 0.1857282203854993\n",
      "  batch 48 loss: 0.1487691516522318\n",
      "LOSS train 0.1487691516522318 valid 0.5516518354415894\n",
      "EPOCH 470:\n",
      "  batch 16 loss: 0.30211009341292083\n",
      "  batch 32 loss: 0.21082613652106375\n",
      "  batch 48 loss: 0.08750496417633258\n",
      "LOSS train 0.08750496417633258 valid 0.5524022579193115\n",
      "EPOCH 471:\n",
      "  batch 16 loss: 0.12645532831083983\n",
      "  batch 32 loss: 0.1445230869576335\n",
      "  batch 48 loss: 0.2549134855507873\n",
      "LOSS train 0.2549134855507873 valid 0.5531934499740601\n",
      "EPOCH 472:\n",
      "  batch 16 loss: 0.16918757208622992\n",
      "  batch 32 loss: 0.2391418577171862\n",
      "  batch 48 loss: 0.19319672312121838\n",
      "LOSS train 0.19319672312121838 valid 0.5533211827278137\n",
      "EPOCH 473:\n",
      "  batch 16 loss: 0.12130723561858758\n",
      "  batch 32 loss: 0.22729223204078153\n",
      "  batch 48 loss: 0.18512128456495702\n",
      "LOSS train 0.18512128456495702 valid 0.5541549921035767\n",
      "EPOCH 474:\n",
      "  batch 16 loss: 0.151504386740271\n",
      "  batch 32 loss: 0.1764861333067529\n",
      "  batch 48 loss: 0.21892351339920424\n",
      "LOSS train 0.21892351339920424 valid 0.5543086528778076\n",
      "EPOCH 475:\n",
      "  batch 16 loss: 0.1529618038330227\n",
      "  batch 32 loss: 0.21655118750641122\n",
      "  batch 48 loss: 0.09956632094690576\n",
      "LOSS train 0.09956632094690576 valid 0.5549549460411072\n",
      "EPOCH 476:\n",
      "  batch 16 loss: 0.11689682386349887\n",
      "  batch 32 loss: 0.1335153627442196\n",
      "  batch 48 loss: 0.14170826086774468\n",
      "LOSS train 0.14170826086774468 valid 0.556012749671936\n",
      "EPOCH 477:\n",
      "  batch 16 loss: 0.20360502030234784\n",
      "  batch 32 loss: 0.18669303256319836\n",
      "  batch 48 loss: 0.12023205670993775\n",
      "LOSS train 0.12023205670993775 valid 0.5549483299255371\n",
      "EPOCH 478:\n",
      "  batch 16 loss: 0.1867602841812186\n",
      "  batch 32 loss: 0.19325271667912602\n",
      "  batch 48 loss: 0.11895920391543768\n",
      "LOSS train 0.11895920391543768 valid 0.5565475225448608\n",
      "EPOCH 479:\n",
      "  batch 16 loss: 0.14527505860314704\n",
      "  batch 32 loss: 0.1480672546895221\n",
      "  batch 48 loss: 0.19758573616854846\n",
      "LOSS train 0.19758573616854846 valid 0.556532621383667\n",
      "EPOCH 480:\n",
      "  batch 16 loss: 0.2043501411098987\n",
      "  batch 32 loss: 0.17849980876781046\n",
      "  batch 48 loss: 0.15656183357350528\n",
      "LOSS train 0.15656183357350528 valid 0.5561968088150024\n",
      "EPOCH 481:\n",
      "  batch 16 loss: 0.1432298436120618\n",
      "  batch 32 loss: 0.15146228222874925\n",
      "  batch 48 loss: 0.15578448120504618\n",
      "LOSS train 0.15578448120504618 valid 0.5568833351135254\n",
      "EPOCH 482:\n",
      "  batch 16 loss: 0.17559839057503268\n",
      "  batch 32 loss: 0.2085129168117419\n",
      "  batch 48 loss: 0.15512720972765237\n",
      "LOSS train 0.15512720972765237 valid 0.5574958324432373\n",
      "EPOCH 483:\n",
      "  batch 16 loss: 0.21444409131072462\n",
      "  batch 32 loss: 0.16718014347134158\n",
      "  batch 48 loss: 0.09655523824039847\n",
      "LOSS train 0.09655523824039847 valid 0.55812668800354\n",
      "EPOCH 484:\n",
      "  batch 16 loss: 0.18332631298108026\n",
      "  batch 32 loss: 0.15898352122167125\n",
      "  batch 48 loss: 0.1619004158419557\n",
      "LOSS train 0.1619004158419557 valid 0.5588048696517944\n",
      "EPOCH 485:\n",
      "  batch 16 loss: 0.1891043350333348\n",
      "  batch 32 loss: 0.20360202656593174\n",
      "  batch 48 loss: 0.1235263631970156\n",
      "LOSS train 0.1235263631970156 valid 0.5580757856369019\n",
      "EPOCH 486:\n",
      "  batch 16 loss: 0.16770060523413122\n",
      "  batch 32 loss: 0.20933295020950027\n",
      "  batch 48 loss: 0.1941727609373629\n",
      "LOSS train 0.1941727609373629 valid 0.5586878657341003\n",
      "EPOCH 487:\n",
      "  batch 16 loss: 0.126354044303298\n",
      "  batch 32 loss: 0.18155294191092253\n",
      "  batch 48 loss: 0.18789536610711366\n",
      "LOSS train 0.18789536610711366 valid 0.5590248107910156\n",
      "EPOCH 488:\n",
      "  batch 16 loss: 0.22548592317616567\n",
      "  batch 32 loss: 0.1571486290777102\n",
      "  batch 48 loss: 0.14468061047955416\n",
      "LOSS train 0.14468061047955416 valid 0.5591799020767212\n",
      "EPOCH 489:\n",
      "  batch 16 loss: 0.16791030205786228\n",
      "  batch 32 loss: 0.2321995246456936\n",
      "  batch 48 loss: 0.16707421498722397\n",
      "LOSS train 0.16707421498722397 valid 0.5594998598098755\n",
      "EPOCH 490:\n",
      "  batch 16 loss: 0.2142937984317541\n",
      "  batch 32 loss: 0.17270294821355492\n",
      "  batch 48 loss: 0.13583890267182142\n",
      "LOSS train 0.13583890267182142 valid 0.5604540109634399\n",
      "EPOCH 491:\n",
      "  batch 16 loss: 0.19280269759474322\n",
      "  batch 32 loss: 0.134259054902941\n",
      "  batch 48 loss: 0.20372567395679653\n",
      "LOSS train 0.20372567395679653 valid 0.5609791278839111\n",
      "EPOCH 492:\n",
      "  batch 16 loss: 0.11713486420921981\n",
      "  batch 32 loss: 0.09420674515422434\n",
      "  batch 48 loss: 0.3328217474045232\n",
      "LOSS train 0.3328217474045232 valid 0.5615990161895752\n",
      "EPOCH 493:\n",
      "  batch 16 loss: 0.1620927711483091\n",
      "  batch 32 loss: 0.20391492877388373\n",
      "  batch 48 loss: 0.16706565528875217\n",
      "LOSS train 0.16706565528875217 valid 0.5609044432640076\n",
      "EPOCH 494:\n",
      "  batch 16 loss: 0.1762809160281904\n",
      "  batch 32 loss: 0.09796897508203983\n",
      "  batch 48 loss: 0.21189565060194582\n",
      "LOSS train 0.21189565060194582 valid 0.5621971487998962\n",
      "EPOCH 495:\n",
      "  batch 16 loss: 0.2028088026563637\n",
      "  batch 32 loss: 0.11770740753854625\n",
      "  batch 48 loss: 0.1942057099658996\n",
      "LOSS train 0.1942057099658996 valid 0.5624330043792725\n",
      "EPOCH 496:\n",
      "  batch 16 loss: 0.16952713602222502\n",
      "  batch 32 loss: 0.16582828311948106\n",
      "  batch 48 loss: 0.15684132341993973\n",
      "LOSS train 0.15684132341993973 valid 0.562576174736023\n",
      "EPOCH 497:\n",
      "  batch 16 loss: 0.15322651783935726\n",
      "  batch 32 loss: 0.13865166855975986\n",
      "  batch 48 loss: 0.20784434076631442\n",
      "LOSS train 0.20784434076631442 valid 0.562816858291626\n",
      "EPOCH 498:\n",
      "  batch 16 loss: 0.20021108421497047\n",
      "  batch 32 loss: 0.1666417052038014\n",
      "  batch 48 loss: 0.12597860768437386\n",
      "LOSS train 0.12597860768437386 valid 0.5633302927017212\n",
      "EPOCH 499:\n",
      "  batch 16 loss: 0.16789142991183326\n",
      "  batch 32 loss: 0.1187944051634986\n",
      "  batch 48 loss: 0.1593158058822155\n",
      "LOSS train 0.1593158058822155 valid 0.5644496083259583\n",
      "EPOCH 500:\n",
      "  batch 16 loss: 0.25366930570453405\n",
      "  batch 32 loss: 0.13833521262858994\n",
      "  batch 48 loss: 0.13681076373904943\n",
      "LOSS train 0.13681076373904943 valid 0.5636869668960571\n",
      "EPOCH 501:\n",
      "  batch 16 loss: 0.15022564341779798\n",
      "  batch 32 loss: 0.17293523147236556\n",
      "  batch 48 loss: 0.13638071197783574\n",
      "LOSS train 0.13638071197783574 valid 0.5643503665924072\n",
      "EPOCH 502:\n",
      "  batch 16 loss: 0.14352018479257822\n",
      "  batch 32 loss: 0.18194341869093478\n",
      "  batch 48 loss: 0.1698165638372302\n",
      "LOSS train 0.1698165638372302 valid 0.5649963617324829\n",
      "EPOCH 503:\n",
      "  batch 16 loss: 0.1559655915480107\n",
      "  batch 32 loss: 0.18011167482472956\n",
      "  batch 48 loss: 0.24184472377237398\n",
      "LOSS train 0.24184472377237398 valid 0.5651763677597046\n",
      "EPOCH 504:\n",
      "  batch 16 loss: 0.1350699377944693\n",
      "  batch 32 loss: 0.21850331570021808\n",
      "  batch 48 loss: 0.1734811884816736\n",
      "LOSS train 0.1734811884816736 valid 0.5653136968612671\n",
      "EPOCH 505:\n",
      "  batch 16 loss: 0.10627883591223508\n",
      "  batch 32 loss: 0.1746173835126683\n",
      "  batch 48 loss: 0.22137678635772318\n",
      "LOSS train 0.22137678635772318 valid 0.5659787654876709\n",
      "EPOCH 506:\n",
      "  batch 16 loss: 0.16939248621929437\n",
      "  batch 32 loss: 0.17869534739293158\n",
      "  batch 48 loss: 0.1528373741894029\n",
      "LOSS train 0.1528373741894029 valid 0.5662999153137207\n",
      "EPOCH 507:\n",
      "  batch 16 loss: 0.20027479738928378\n",
      "  batch 32 loss: 0.15850365976803005\n",
      "  batch 48 loss: 0.11266235017683357\n",
      "LOSS train 0.11266235017683357 valid 0.566870391368866\n",
      "EPOCH 508:\n",
      "  batch 16 loss: 0.20161942357663065\n",
      "  batch 32 loss: 0.16867101099342108\n",
      "  batch 48 loss: 0.10144037459394895\n",
      "LOSS train 0.10144037459394895 valid 0.5677270889282227\n",
      "EPOCH 509:\n",
      "  batch 16 loss: 0.12318653799593449\n",
      "  batch 32 loss: 0.19709075498394668\n",
      "  batch 48 loss: 0.22724278413807042\n",
      "LOSS train 0.22724278413807042 valid 0.5674980282783508\n",
      "EPOCH 510:\n",
      "  batch 16 loss: 0.1560459474567324\n",
      "  batch 32 loss: 0.15793839748948812\n",
      "  batch 48 loss: 0.16057294356869534\n",
      "LOSS train 0.16057294356869534 valid 0.5679938793182373\n",
      "EPOCH 511:\n",
      "  batch 16 loss: 0.1627822332084179\n",
      "  batch 32 loss: 0.13980841333977878\n",
      "  batch 48 loss: 0.18072300928179175\n",
      "LOSS train 0.18072300928179175 valid 0.5682101249694824\n",
      "EPOCH 512:\n",
      "  batch 16 loss: 0.1640468715922907\n",
      "  batch 32 loss: 0.17443623137660325\n",
      "  batch 48 loss: 0.11001205822685733\n",
      "LOSS train 0.11001205822685733 valid 0.5683600902557373\n",
      "EPOCH 513:\n",
      "  batch 16 loss: 0.14919736748561263\n",
      "  batch 32 loss: 0.1325898023787886\n",
      "  batch 48 loss: 0.26309203903656453\n",
      "LOSS train 0.26309203903656453 valid 0.5684798955917358\n",
      "EPOCH 514:\n",
      "  batch 16 loss: 0.1126692317193374\n",
      "  batch 32 loss: 0.21078061690786853\n",
      "  batch 48 loss: 0.10591096560528968\n",
      "LOSS train 0.10591096560528968 valid 0.569159746170044\n",
      "EPOCH 515:\n",
      "  batch 16 loss: 0.17172348767053336\n",
      "  batch 32 loss: 0.1399079596158117\n",
      "  batch 48 loss: 0.20396941027138382\n",
      "LOSS train 0.20396941027138382 valid 0.5689941644668579\n",
      "EPOCH 516:\n",
      "  batch 16 loss: 0.24093510807142593\n",
      "  batch 32 loss: 0.12011205917224288\n",
      "  batch 48 loss: 0.1400216280017048\n",
      "LOSS train 0.1400216280017048 valid 0.5695279836654663\n",
      "EPOCH 517:\n",
      "  batch 16 loss: 0.16612558520864695\n",
      "  batch 32 loss: 0.10321761609520763\n",
      "  batch 48 loss: 0.1683021927892696\n",
      "LOSS train 0.1683021927892696 valid 0.5707326531410217\n",
      "EPOCH 518:\n",
      "  batch 16 loss: 0.21816403872799128\n",
      "  batch 32 loss: 0.13146532722748816\n",
      "  batch 48 loss: 0.15947736939415336\n",
      "LOSS train 0.15947736939415336 valid 0.570518970489502\n",
      "EPOCH 519:\n",
      "  batch 16 loss: 0.1931663790019229\n",
      "  batch 32 loss: 0.1663026042515412\n",
      "  batch 48 loss: 0.09312310098903254\n",
      "LOSS train 0.09312310098903254 valid 0.5710878372192383\n",
      "EPOCH 520:\n",
      "  batch 16 loss: 0.14091171450854745\n",
      "  batch 32 loss: 0.11678756267065182\n",
      "  batch 48 loss: 0.24084864795440808\n",
      "LOSS train 0.24084864795440808 valid 0.5717819333076477\n",
      "EPOCH 521:\n",
      "  batch 16 loss: 0.18290414684452116\n",
      "  batch 32 loss: 0.11950902892567683\n",
      "  batch 48 loss: 0.21585239778505638\n",
      "LOSS train 0.21585239778505638 valid 0.5723909139633179\n",
      "EPOCH 522:\n",
      "  batch 16 loss: 0.19925821188371629\n",
      "  batch 32 loss: 0.1469993757782504\n",
      "  batch 48 loss: 0.2066703550517559\n",
      "LOSS train 0.2066703550517559 valid 0.5721698999404907\n",
      "EPOCH 523:\n",
      "  batch 16 loss: 0.14867039500677492\n",
      "  batch 32 loss: 0.21988130453974009\n",
      "  batch 48 loss: 0.14011726243188605\n",
      "LOSS train 0.14011726243188605 valid 0.5725248456001282\n",
      "EPOCH 524:\n",
      "  batch 16 loss: 0.13022959441877902\n",
      "  batch 32 loss: 0.19597264303592965\n",
      "  batch 48 loss: 0.210496666142717\n",
      "LOSS train 0.210496666142717 valid 0.5729877948760986\n",
      "EPOCH 525:\n",
      "  batch 16 loss: 0.12047866138163954\n",
      "  batch 32 loss: 0.21772803436033428\n",
      "  batch 48 loss: 0.18583593354560435\n",
      "LOSS train 0.18583593354560435 valid 0.573428750038147\n",
      "EPOCH 526:\n",
      "  batch 16 loss: 0.10292304184986278\n",
      "  batch 32 loss: 0.24272943346295506\n",
      "  batch 48 loss: 0.16850288200657815\n",
      "LOSS train 0.16850288200657815 valid 0.573134183883667\n",
      "EPOCH 527:\n",
      "  batch 16 loss: 0.15070552472025156\n",
      "  batch 32 loss: 0.12279362147091888\n",
      "  batch 48 loss: 0.15207720000762492\n",
      "LOSS train 0.15207720000762492 valid 0.5737746953964233\n",
      "EPOCH 528:\n",
      "  batch 16 loss: 0.2562987742712721\n",
      "  batch 32 loss: 0.12601791776251048\n",
      "  batch 48 loss: 0.13862038031220436\n",
      "LOSS train 0.13862038031220436 valid 0.5745043754577637\n",
      "EPOCH 529:\n",
      "  batch 16 loss: 0.1264226908388082\n",
      "  batch 32 loss: 0.21378330106381327\n",
      "  batch 48 loss: 0.17309023474808782\n",
      "LOSS train 0.17309023474808782 valid 0.5748579502105713\n",
      "EPOCH 530:\n",
      "  batch 16 loss: 0.15801744163036346\n",
      "  batch 32 loss: 0.14979780605062842\n",
      "  batch 48 loss: 0.1431394351820927\n",
      "LOSS train 0.1431394351820927 valid 0.5753047466278076\n",
      "EPOCH 531:\n",
      "  batch 16 loss: 0.17536968644708395\n",
      "  batch 32 loss: 0.17086195526644588\n",
      "  batch 48 loss: 0.15672111010644585\n",
      "LOSS train 0.15672111010644585 valid 0.5757260322570801\n",
      "EPOCH 532:\n",
      "  batch 16 loss: 0.11749478055571672\n",
      "  batch 32 loss: 0.1947122864658013\n",
      "  batch 48 loss: 0.1481878123304341\n",
      "LOSS train 0.1481878123304341 valid 0.57586669921875\n",
      "EPOCH 533:\n",
      "  batch 16 loss: 0.13824591704178602\n",
      "  batch 32 loss: 0.13050108595052734\n",
      "  batch 48 loss: 0.2740371215622872\n",
      "LOSS train 0.2740371215622872 valid 0.5764238834381104\n",
      "EPOCH 534:\n",
      "  batch 16 loss: 0.23355368140619248\n",
      "  batch 32 loss: 0.15713433432392776\n",
      "  batch 48 loss: 0.1135834890010301\n",
      "LOSS train 0.1135834890010301 valid 0.5763782262802124\n",
      "EPOCH 535:\n",
      "  batch 16 loss: 0.14061544195283204\n",
      "  batch 32 loss: 0.13545021857134998\n",
      "  batch 48 loss: 0.16975512460339814\n",
      "LOSS train 0.16975512460339814 valid 0.5776478052139282\n",
      "EPOCH 536:\n",
      "  batch 16 loss: 0.12283977819606662\n",
      "  batch 32 loss: 0.19475925993174314\n",
      "  batch 48 loss: 0.17090983342495747\n",
      "LOSS train 0.17090983342495747 valid 0.5780439972877502\n",
      "EPOCH 537:\n",
      "  batch 16 loss: 0.24442293564788997\n",
      "  batch 32 loss: 0.18062154413200915\n",
      "  batch 48 loss: 0.11336163431406021\n",
      "LOSS train 0.11336163431406021 valid 0.577942967414856\n",
      "EPOCH 538:\n",
      "  batch 16 loss: 0.22415476199239492\n",
      "  batch 32 loss: 0.14029105741064996\n",
      "  batch 48 loss: 0.1128172010066919\n",
      "LOSS train 0.1128172010066919 valid 0.5785055756568909\n",
      "EPOCH 539:\n",
      "  batch 16 loss: 0.17564231983851641\n",
      "  batch 32 loss: 0.1297982059768401\n",
      "  batch 48 loss: 0.18002274655736983\n",
      "LOSS train 0.18002274655736983 valid 0.5789376497268677\n",
      "EPOCH 540:\n",
      "  batch 16 loss: 0.14266514999326319\n",
      "  batch 32 loss: 0.15713730512652546\n",
      "  batch 48 loss: 0.18529827310703695\n",
      "LOSS train 0.18529827310703695 valid 0.5796197652816772\n",
      "EPOCH 541:\n",
      "  batch 16 loss: 0.09315127099398524\n",
      "  batch 32 loss: 0.17064228258095682\n",
      "  batch 48 loss: 0.23757230350747705\n",
      "LOSS train 0.23757230350747705 valid 0.579293966293335\n",
      "EPOCH 542:\n",
      "  batch 16 loss: 0.20685401861555874\n",
      "  batch 32 loss: 0.10253467009169981\n",
      "  batch 48 loss: 0.1446358533576131\n",
      "LOSS train 0.1446358533576131 valid 0.5801751613616943\n",
      "EPOCH 543:\n",
      "  batch 16 loss: 0.14718405052553862\n",
      "  batch 32 loss: 0.1643481376231648\n",
      "  batch 48 loss: 0.1776869441528106\n",
      "LOSS train 0.1776869441528106 valid 0.5806946754455566\n",
      "EPOCH 544:\n",
      "  batch 16 loss: 0.17766489274799824\n",
      "  batch 32 loss: 0.11002547707175836\n",
      "  batch 48 loss: 0.1579339805757627\n",
      "LOSS train 0.1579339805757627 valid 0.5805090665817261\n",
      "EPOCH 545:\n",
      "  batch 16 loss: 0.16486426396295428\n",
      "  batch 32 loss: 0.19347695584292524\n",
      "  batch 48 loss: 0.10604047949891537\n",
      "LOSS train 0.10604047949891537 valid 0.5807456970214844\n",
      "EPOCH 546:\n",
      "  batch 16 loss: 0.1467297854833305\n",
      "  batch 32 loss: 0.23830632492899895\n",
      "  batch 48 loss: 0.12317303312011063\n",
      "LOSS train 0.12317303312011063 valid 0.581080436706543\n",
      "EPOCH 547:\n",
      "  batch 16 loss: 0.1202571390895173\n",
      "  batch 32 loss: 0.18530278897378594\n",
      "  batch 48 loss: 0.15057253040140495\n",
      "LOSS train 0.15057253040140495 valid 0.5815705060958862\n",
      "EPOCH 548:\n",
      "  batch 16 loss: 0.13661929123918526\n",
      "  batch 32 loss: 0.17481929290806875\n",
      "  batch 48 loss: 0.1301397883798927\n",
      "LOSS train 0.1301397883798927 valid 0.5821641683578491\n",
      "EPOCH 549:\n",
      "  batch 16 loss: 0.11195065977517515\n",
      "  batch 32 loss: 0.20645366003736854\n",
      "  batch 48 loss: 0.15396851557306945\n",
      "LOSS train 0.15396851557306945 valid 0.5822230577468872\n",
      "EPOCH 550:\n",
      "  batch 16 loss: 0.15833170659607276\n",
      "  batch 32 loss: 0.13622537261107937\n",
      "  batch 48 loss: 0.12811323476489633\n",
      "LOSS train 0.12811323476489633 valid 0.5829510688781738\n",
      "EPOCH 551:\n",
      "  batch 16 loss: 0.08547072747023776\n",
      "  batch 32 loss: 0.1969897688832134\n",
      "  batch 48 loss: 0.15020366478711367\n",
      "LOSS train 0.15020366478711367 valid 0.5837295055389404\n",
      "EPOCH 552:\n",
      "  batch 16 loss: 0.1447964790277183\n",
      "  batch 32 loss: 0.21220049122348428\n",
      "  batch 48 loss: 0.13622858369490132\n",
      "LOSS train 0.13622858369490132 valid 0.5832333564758301\n",
      "EPOCH 553:\n",
      "  batch 16 loss: 0.13469596975483\n",
      "  batch 32 loss: 0.15992246713722125\n",
      "  batch 48 loss: 0.1590845372120384\n",
      "LOSS train 0.1590845372120384 valid 0.5843155384063721\n",
      "EPOCH 554:\n",
      "  batch 16 loss: 0.08997793967137113\n",
      "  batch 32 loss: 0.2579634516732767\n",
      "  batch 48 loss: 0.12477124040015042\n",
      "LOSS train 0.12477124040015042 valid 0.5844761729240417\n",
      "EPOCH 555:\n",
      "  batch 16 loss: 0.12547767348587513\n",
      "  batch 32 loss: 0.11698245126171969\n",
      "  batch 48 loss: 0.15856366777734365\n",
      "LOSS train 0.15856366777734365 valid 0.5850836634635925\n",
      "EPOCH 556:\n",
      "  batch 16 loss: 0.18297991674626246\n",
      "  batch 32 loss: 0.16217823908664286\n",
      "  batch 48 loss: 0.14000241097528487\n",
      "LOSS train 0.14000241097528487 valid 0.5853005647659302\n",
      "EPOCH 557:\n",
      "  batch 16 loss: 0.13326155941467732\n",
      "  batch 32 loss: 0.19522272900212556\n",
      "  batch 48 loss: 0.1299029771471396\n",
      "LOSS train 0.1299029771471396 valid 0.586175799369812\n",
      "EPOCH 558:\n",
      "  batch 16 loss: 0.10993867146316916\n",
      "  batch 32 loss: 0.10473805165383965\n",
      "  batch 48 loss: 0.21125541790388525\n",
      "LOSS train 0.21125541790388525 valid 0.5865269303321838\n",
      "EPOCH 559:\n",
      "  batch 16 loss: 0.16963631426915526\n",
      "  batch 32 loss: 0.13160108411102556\n",
      "  batch 48 loss: 0.12136058090254664\n",
      "LOSS train 0.12136058090254664 valid 0.5864524841308594\n",
      "EPOCH 560:\n",
      "  batch 16 loss: 0.1960167359211482\n",
      "  batch 32 loss: 0.16799739783164114\n",
      "  batch 48 loss: 0.14822238316992298\n",
      "LOSS train 0.14822238316992298 valid 0.5866410732269287\n",
      "EPOCH 561:\n",
      "  batch 16 loss: 0.1510568477679044\n",
      "  batch 32 loss: 0.18289505346911028\n",
      "  batch 48 loss: 0.13128314597997814\n",
      "LOSS train 0.13128314597997814 valid 0.5874820947647095\n",
      "EPOCH 562:\n",
      "  batch 16 loss: 0.12299037927004974\n",
      "  batch 32 loss: 0.198379278939683\n",
      "  batch 48 loss: 0.15292671730276197\n",
      "LOSS train 0.15292671730276197 valid 0.5874550342559814\n",
      "EPOCH 563:\n",
      "  batch 16 loss: 0.1137848911457695\n",
      "  batch 32 loss: 0.17533910577185452\n",
      "  batch 48 loss: 0.17860054480843246\n",
      "LOSS train 0.17860054480843246 valid 0.588176429271698\n",
      "EPOCH 564:\n",
      "  batch 16 loss: 0.10805599647574127\n",
      "  batch 32 loss: 0.1280161936301738\n",
      "  batch 48 loss: 0.18981430324492976\n",
      "LOSS train 0.18981430324492976 valid 0.5887101292610168\n",
      "EPOCH 565:\n",
      "  batch 16 loss: 0.18239795288536698\n",
      "  batch 32 loss: 0.16340013229637407\n",
      "  batch 48 loss: 0.0789850102737546\n",
      "LOSS train 0.0789850102737546 valid 0.5886266827583313\n",
      "EPOCH 566:\n",
      "  batch 16 loss: 0.09606225666357204\n",
      "  batch 32 loss: 0.17316407593898475\n",
      "  batch 48 loss: 0.12753257591975853\n",
      "LOSS train 0.12753257591975853 valid 0.5896738767623901\n",
      "EPOCH 567:\n",
      "  batch 16 loss: 0.1491709352703765\n",
      "  batch 32 loss: 0.0883647313748952\n",
      "  batch 48 loss: 0.12595413264352828\n",
      "LOSS train 0.12595413264352828 valid 0.5896583795547485\n",
      "EPOCH 568:\n",
      "  batch 16 loss: 0.11987870978191495\n",
      "  batch 32 loss: 0.11193301738239825\n",
      "  batch 48 loss: 0.20536165975499898\n",
      "LOSS train 0.20536165975499898 valid 0.5899839401245117\n",
      "EPOCH 569:\n",
      "  batch 16 loss: 0.15957074332982302\n",
      "  batch 32 loss: 0.1798227948602289\n",
      "  batch 48 loss: 0.13760815927525982\n",
      "LOSS train 0.13760815927525982 valid 0.5904881954193115\n",
      "EPOCH 570:\n",
      "  batch 16 loss: 0.12967407598625869\n",
      "  batch 32 loss: 0.15473574737552553\n",
      "  batch 48 loss: 0.20635699515696615\n",
      "LOSS train 0.20635699515696615 valid 0.5908130407333374\n",
      "EPOCH 571:\n",
      "  batch 16 loss: 0.1338119748979807\n",
      "  batch 32 loss: 0.09325545688625425\n",
      "  batch 48 loss: 0.2040612320706714\n",
      "LOSS train 0.2040612320706714 valid 0.591260552406311\n",
      "EPOCH 572:\n",
      "  batch 16 loss: 0.1607638468267396\n",
      "  batch 32 loss: 0.1324838544242084\n",
      "  batch 48 loss: 0.09757843802071875\n",
      "LOSS train 0.09757843802071875 valid 0.5921717286109924\n",
      "EPOCH 573:\n",
      "  batch 16 loss: 0.15723484382033348\n",
      "  batch 32 loss: 0.17045722994953394\n",
      "  batch 48 loss: 0.13703451387118548\n",
      "LOSS train 0.13703451387118548 valid 0.5918376445770264\n",
      "EPOCH 574:\n",
      "  batch 16 loss: 0.13878714153543115\n",
      "  batch 32 loss: 0.13319645321462303\n",
      "  batch 48 loss: 0.16514014021959156\n",
      "LOSS train 0.16514014021959156 valid 0.5929530262947083\n",
      "EPOCH 575:\n",
      "  batch 16 loss: 0.17005462670931593\n",
      "  batch 32 loss: 0.11075070724473335\n",
      "  batch 48 loss: 0.21462159318616614\n",
      "LOSS train 0.21462159318616614 valid 0.5928646326065063\n",
      "EPOCH 576:\n",
      "  batch 16 loss: 0.16392523801187053\n",
      "  batch 32 loss: 0.09838319558184594\n",
      "  batch 48 loss: 0.19362847253796645\n",
      "LOSS train 0.19362847253796645 valid 0.5931179523468018\n",
      "EPOCH 577:\n",
      "  batch 16 loss: 0.09648980380734429\n",
      "  batch 32 loss: 0.18486944503092673\n",
      "  batch 48 loss: 0.15854315069736913\n",
      "LOSS train 0.15854315069736913 valid 0.5938993692398071\n",
      "EPOCH 578:\n",
      "  batch 16 loss: 0.2307546357915271\n",
      "  batch 32 loss: 0.11967306805308908\n",
      "  batch 48 loss: 0.11047415470238775\n",
      "LOSS train 0.11047415470238775 valid 0.5940465331077576\n",
      "EPOCH 579:\n",
      "  batch 16 loss: 0.1431859344302211\n",
      "  batch 32 loss: 0.20639449555892497\n",
      "  batch 48 loss: 0.12699293409241363\n",
      "LOSS train 0.12699293409241363 valid 0.5942565202713013\n",
      "EPOCH 580:\n",
      "  batch 16 loss: 0.15317300357855856\n",
      "  batch 32 loss: 0.14234027336351573\n",
      "  batch 48 loss: 0.17644488904625177\n",
      "LOSS train 0.17644488904625177 valid 0.5949929356575012\n",
      "EPOCH 581:\n",
      "  batch 16 loss: 0.15247180417645723\n",
      "  batch 32 loss: 0.10839928907807916\n",
      "  batch 48 loss: 0.19274653145112097\n",
      "LOSS train 0.19274653145112097 valid 0.595343828201294\n",
      "EPOCH 582:\n",
      "  batch 16 loss: 0.14577745585120283\n",
      "  batch 32 loss: 0.14636164203693625\n",
      "  batch 48 loss: 0.1381662201602012\n",
      "LOSS train 0.1381662201602012 valid 0.5950082540512085\n",
      "EPOCH 583:\n",
      "  batch 16 loss: 0.11668767262017354\n",
      "  batch 32 loss: 0.12503092747647315\n",
      "  batch 48 loss: 0.19074244785588235\n",
      "LOSS train 0.19074244785588235 valid 0.5959627628326416\n",
      "EPOCH 584:\n",
      "  batch 16 loss: 0.16634344714111648\n",
      "  batch 32 loss: 0.07182867696974427\n",
      "  batch 48 loss: 0.1907088435254991\n",
      "LOSS train 0.1907088435254991 valid 0.5963772535324097\n",
      "EPOCH 585:\n",
      "  batch 16 loss: 0.20616930758114904\n",
      "  batch 32 loss: 0.0997328264056705\n",
      "  batch 48 loss: 0.16375093939132057\n",
      "LOSS train 0.16375093939132057 valid 0.5968123078346252\n",
      "EPOCH 586:\n",
      "  batch 16 loss: 0.1883041630499065\n",
      "  batch 32 loss: 0.09150068432791159\n",
      "  batch 48 loss: 0.11260955038596876\n",
      "LOSS train 0.11260955038596876 valid 0.5971691608428955\n",
      "EPOCH 587:\n",
      "  batch 16 loss: 0.07962951745139435\n",
      "  batch 32 loss: 0.21087096142582595\n",
      "  batch 48 loss: 0.1474880000751\n",
      "LOSS train 0.1474880000751 valid 0.5975656509399414\n",
      "EPOCH 588:\n",
      "  batch 16 loss: 0.1630220381775871\n",
      "  batch 32 loss: 0.18890228083182592\n",
      "  batch 48 loss: 0.1208121458766982\n",
      "LOSS train 0.1208121458766982 valid 0.5978419780731201\n",
      "EPOCH 589:\n",
      "  batch 16 loss: 0.1158537435112521\n",
      "  batch 32 loss: 0.14987432642374188\n",
      "  batch 48 loss: 0.15901692712213844\n",
      "LOSS train 0.15901692712213844 valid 0.5976239442825317\n",
      "EPOCH 590:\n",
      "  batch 16 loss: 0.13422495443955995\n",
      "  batch 32 loss: 0.13391724869143218\n",
      "  batch 48 loss: 0.1542638498940505\n",
      "LOSS train 0.1542638498940505 valid 0.5996543169021606\n",
      "EPOCH 591:\n",
      "  batch 16 loss: 0.1982012535445392\n",
      "  batch 32 loss: 0.1987136845709756\n",
      "  batch 48 loss: 0.08848813267832156\n",
      "LOSS train 0.08848813267832156 valid 0.5988193154335022\n",
      "EPOCH 592:\n",
      "  batch 16 loss: 0.1331127785379067\n",
      "  batch 32 loss: 0.17863388388650492\n",
      "  batch 48 loss: 0.16368822345975786\n",
      "LOSS train 0.16368822345975786 valid 0.5992584824562073\n",
      "EPOCH 593:\n",
      "  batch 16 loss: 0.1364722479484044\n",
      "  batch 32 loss: 0.15250529814511538\n",
      "  batch 48 loss: 0.14352892496390268\n",
      "LOSS train 0.14352892496390268 valid 0.5998777151107788\n",
      "EPOCH 594:\n",
      "  batch 16 loss: 0.2023858156753704\n",
      "  batch 32 loss: 0.10421224229503423\n",
      "  batch 48 loss: 0.11867855442687869\n",
      "LOSS train 0.11867855442687869 valid 0.6003740429878235\n",
      "EPOCH 595:\n",
      "  batch 16 loss: 0.06982106622308493\n",
      "  batch 32 loss: 0.13880984450224787\n",
      "  batch 48 loss: 0.13320082752034068\n",
      "LOSS train 0.13320082752034068 valid 0.6004078388214111\n",
      "EPOCH 596:\n",
      "  batch 16 loss: 0.13915589067619294\n",
      "  batch 32 loss: 0.2493017230881378\n",
      "  batch 48 loss: 0.09278759796870872\n",
      "LOSS train 0.09278759796870872 valid 0.6009876132011414\n",
      "EPOCH 597:\n",
      "  batch 16 loss: 0.1497391138691455\n",
      "  batch 32 loss: 0.09070648439228535\n",
      "  batch 48 loss: 0.15598164458060637\n",
      "LOSS train 0.15598164458060637 valid 0.6020364165306091\n",
      "EPOCH 598:\n",
      "  batch 16 loss: 0.16807653359137475\n",
      "  batch 32 loss: 0.12456406856654212\n",
      "  batch 48 loss: 0.14846178388688713\n",
      "LOSS train 0.14846178388688713 valid 0.6020643711090088\n",
      "EPOCH 599:\n",
      "  batch 16 loss: 0.17246304656146094\n",
      "  batch 32 loss: 0.051425427198410034\n",
      "  batch 48 loss: 0.1506560132256709\n",
      "LOSS train 0.1506560132256709 valid 0.6019707918167114\n",
      "EPOCH 600:\n",
      "  batch 16 loss: 0.11902595436549746\n",
      "  batch 32 loss: 0.13874069765006425\n",
      "  batch 48 loss: 0.22009956144029275\n",
      "LOSS train 0.22009956144029275 valid 0.6019880175590515\n",
      "EPOCH 601:\n",
      "  batch 16 loss: 0.14876163087319583\n",
      "  batch 32 loss: 0.11592511332128197\n",
      "  batch 48 loss: 0.10039298789342865\n",
      "LOSS train 0.10039298789342865 valid 0.6026325225830078\n",
      "EPOCH 602:\n",
      "  batch 16 loss: 0.1262735357740894\n",
      "  batch 32 loss: 0.16370618174551055\n",
      "  batch 48 loss: 0.14369568915572017\n",
      "LOSS train 0.14369568915572017 valid 0.6033161878585815\n",
      "EPOCH 603:\n",
      "  batch 16 loss: 0.13493015617132187\n",
      "  batch 32 loss: 0.13081014782073908\n",
      "  batch 48 loss: 0.11857110308483243\n",
      "LOSS train 0.11857110308483243 valid 0.6034027934074402\n",
      "EPOCH 604:\n",
      "  batch 16 loss: 0.17450952786020935\n",
      "  batch 32 loss: 0.12733245705021545\n",
      "  batch 48 loss: 0.1031515053473413\n",
      "LOSS train 0.1031515053473413 valid 0.6037074327468872\n",
      "EPOCH 605:\n",
      "  batch 16 loss: 0.13235152687411755\n",
      "  batch 32 loss: 0.18965113133890554\n",
      "  batch 48 loss: 0.13111844391096383\n",
      "LOSS train 0.13111844391096383 valid 0.603873074054718\n",
      "EPOCH 606:\n",
      "  batch 16 loss: 0.09882714730338193\n",
      "  batch 32 loss: 0.1189599156496115\n",
      "  batch 48 loss: 0.15617953467881307\n",
      "LOSS train 0.15617953467881307 valid 0.6047605276107788\n",
      "EPOCH 607:\n",
      "  batch 16 loss: 0.08792532602092251\n",
      "  batch 32 loss: 0.10393502478837036\n",
      "  batch 48 loss: 0.1725114194850903\n",
      "LOSS train 0.1725114194850903 valid 0.6048845052719116\n",
      "EPOCH 608:\n",
      "  batch 16 loss: 0.1619054423936177\n",
      "  batch 32 loss: 0.14360691065667197\n",
      "  batch 48 loss: 0.09692898899083957\n",
      "LOSS train 0.09692898899083957 valid 0.6052184104919434\n",
      "EPOCH 609:\n",
      "  batch 16 loss: 0.15871416492154822\n",
      "  batch 32 loss: 0.12261455180123448\n",
      "  batch 48 loss: 0.14853764092549682\n",
      "LOSS train 0.14853764092549682 valid 0.6060836315155029\n",
      "EPOCH 610:\n",
      "  batch 16 loss: 0.1148717412725091\n",
      "  batch 32 loss: 0.1160921901173424\n",
      "  batch 48 loss: 0.14791135635459796\n",
      "LOSS train 0.14791135635459796 valid 0.606390118598938\n",
      "EPOCH 611:\n",
      "  batch 16 loss: 0.10401284183899406\n",
      "  batch 32 loss: 0.15529412822797894\n",
      "  batch 48 loss: 0.13062283984618261\n",
      "LOSS train 0.13062283984618261 valid 0.6066597700119019\n",
      "EPOCH 612:\n",
      "  batch 16 loss: 0.13459147047251463\n",
      "  batch 32 loss: 0.14552884851582348\n",
      "  batch 48 loss: 0.10675764878396876\n",
      "LOSS train 0.10675764878396876 valid 0.6071667671203613\n",
      "EPOCH 613:\n",
      "  batch 16 loss: 0.13208017087890767\n",
      "  batch 32 loss: 0.13444122043438256\n",
      "  batch 48 loss: 0.1316911515314132\n",
      "LOSS train 0.1316911515314132 valid 0.607762336730957\n",
      "EPOCH 614:\n",
      "  batch 16 loss: 0.11348364711739123\n",
      "  batch 32 loss: 0.09295961877796799\n",
      "  batch 48 loss: 0.17243569600395858\n",
      "LOSS train 0.17243569600395858 valid 0.6080876588821411\n",
      "EPOCH 615:\n",
      "  batch 16 loss: 0.1079152332386002\n",
      "  batch 32 loss: 0.16076677769888192\n",
      "  batch 48 loss: 0.15159120596945286\n",
      "LOSS train 0.15159120596945286 valid 0.6082135438919067\n",
      "EPOCH 616:\n",
      "  batch 16 loss: 0.15682323859073222\n",
      "  batch 32 loss: 0.11431232819450088\n",
      "  batch 48 loss: 0.15199000491702463\n",
      "LOSS train 0.15199000491702463 valid 0.6084811091423035\n",
      "EPOCH 617:\n",
      "  batch 16 loss: 0.1876363535411656\n",
      "  batch 32 loss: 0.12629114952869713\n",
      "  batch 48 loss: 0.10782422075862996\n",
      "LOSS train 0.10782422075862996 valid 0.609284520149231\n",
      "EPOCH 618:\n",
      "  batch 16 loss: 0.12648265360621735\n",
      "  batch 32 loss: 0.1545815550525731\n",
      "  batch 48 loss: 0.12536915618693456\n",
      "LOSS train 0.12536915618693456 valid 0.6096054911613464\n",
      "EPOCH 619:\n",
      "  batch 16 loss: 0.15345384797547013\n",
      "  batch 32 loss: 0.09192335285479203\n",
      "  batch 48 loss: 0.17516505173989572\n",
      "LOSS train 0.17516505173989572 valid 0.6097855567932129\n",
      "EPOCH 620:\n",
      "  batch 16 loss: 0.2064087379258126\n",
      "  batch 32 loss: 0.12326090363785625\n",
      "  batch 48 loss: 0.13618441484868526\n",
      "LOSS train 0.13618441484868526 valid 0.610078752040863\n",
      "EPOCH 621:\n",
      "  batch 16 loss: 0.1119439312024042\n",
      "  batch 32 loss: 0.18642418454692233\n",
      "  batch 48 loss: 0.11091065092477947\n",
      "LOSS train 0.11091065092477947 valid 0.6104060411453247\n",
      "EPOCH 622:\n",
      "  batch 16 loss: 0.16456492274301127\n",
      "  batch 32 loss: 0.14265390776563436\n",
      "  batch 48 loss: 0.11693080941040535\n",
      "LOSS train 0.11693080941040535 valid 0.6111894845962524\n",
      "EPOCH 623:\n",
      "  batch 16 loss: 0.1282472599123139\n",
      "  batch 32 loss: 0.15865006507374346\n",
      "  batch 48 loss: 0.10154435801086947\n",
      "LOSS train 0.10154435801086947 valid 0.6114636659622192\n",
      "EPOCH 624:\n",
      "  batch 16 loss: 0.17787444149143994\n",
      "  batch 32 loss: 0.14182885526679456\n",
      "  batch 48 loss: 0.14282879477832466\n",
      "LOSS train 0.14282879477832466 valid 0.6109567880630493\n",
      "EPOCH 625:\n",
      "  batch 16 loss: 0.1118246141995769\n",
      "  batch 32 loss: 0.1713594482280314\n",
      "  batch 48 loss: 0.05547510151518509\n",
      "LOSS train 0.05547510151518509 valid 0.6119714379310608\n",
      "EPOCH 626:\n",
      "  batch 16 loss: 0.12798850995022804\n",
      "  batch 32 loss: 0.1149251694441773\n",
      "  batch 48 loss: 0.17650428926572204\n",
      "LOSS train 0.17650428926572204 valid 0.6127350926399231\n",
      "EPOCH 627:\n",
      "  batch 16 loss: 0.09649690927471966\n",
      "  batch 32 loss: 0.1957876537926495\n",
      "  batch 48 loss: 0.12700561003293842\n",
      "LOSS train 0.12700561003293842 valid 0.6132833957672119\n",
      "EPOCH 628:\n",
      "  batch 16 loss: 0.09198880143230781\n",
      "  batch 32 loss: 0.13086247676983476\n",
      "  batch 48 loss: 0.18960604176390916\n",
      "LOSS train 0.18960604176390916 valid 0.6135145425796509\n",
      "EPOCH 629:\n",
      "  batch 16 loss: 0.15722958068363369\n",
      "  batch 32 loss: 0.08242739911656827\n",
      "  batch 48 loss: 0.17367700891918503\n",
      "LOSS train 0.17367700891918503 valid 0.6137512922286987\n",
      "EPOCH 630:\n",
      "  batch 16 loss: 0.14525982888881117\n",
      "  batch 32 loss: 0.1179762533865869\n",
      "  batch 48 loss: 0.11931383662158623\n",
      "LOSS train 0.11931383662158623 valid 0.6142704486846924\n",
      "EPOCH 631:\n",
      "  batch 16 loss: 0.16455752233741805\n",
      "  batch 32 loss: 0.14066971628926694\n",
      "  batch 48 loss: 0.14163890533382073\n",
      "LOSS train 0.14163890533382073 valid 0.614155113697052\n",
      "EPOCH 632:\n",
      "  batch 16 loss: 0.1401701598369982\n",
      "  batch 32 loss: 0.13472340985026676\n",
      "  batch 48 loss: 0.14644554432015866\n",
      "LOSS train 0.14644554432015866 valid 0.6153340339660645\n",
      "EPOCH 633:\n",
      "  batch 16 loss: 0.1475565538275987\n",
      "  batch 32 loss: 0.13533945643575862\n",
      "  batch 48 loss: 0.149912690569181\n",
      "LOSS train 0.149912690569181 valid 0.6151732802391052\n",
      "EPOCH 634:\n",
      "  batch 16 loss: 0.1512976820813492\n",
      "  batch 32 loss: 0.11749300453811884\n",
      "  batch 48 loss: 0.13635432766750455\n",
      "LOSS train 0.13635432766750455 valid 0.6161056756973267\n",
      "EPOCH 635:\n",
      "  batch 16 loss: 0.1199277077394072\n",
      "  batch 32 loss: 0.11884339163952973\n",
      "  batch 48 loss: 0.14332412893418223\n",
      "LOSS train 0.14332412893418223 valid 0.6164782643318176\n",
      "EPOCH 636:\n",
      "  batch 16 loss: 0.16271425189916044\n",
      "  batch 32 loss: 0.1267078557284549\n",
      "  batch 48 loss: 0.1194176624994725\n",
      "LOSS train 0.1194176624994725 valid 0.6170206069946289\n",
      "EPOCH 637:\n",
      "  batch 16 loss: 0.1403775709331967\n",
      "  batch 32 loss: 0.15384007085231133\n",
      "  batch 48 loss: 0.13181659870315343\n",
      "LOSS train 0.13181659870315343 valid 0.6177343726158142\n",
      "EPOCH 638:\n",
      "  batch 16 loss: 0.11566450312966481\n",
      "  batch 32 loss: 0.16631980007514358\n",
      "  batch 48 loss: 0.08780166500946507\n",
      "LOSS train 0.08780166500946507 valid 0.6179044842720032\n",
      "EPOCH 639:\n",
      "  batch 16 loss: 0.1427484906744212\n",
      "  batch 32 loss: 0.12222793936962262\n",
      "  batch 48 loss: 0.12142145351390354\n",
      "LOSS train 0.12142145351390354 valid 0.6183434724807739\n",
      "EPOCH 640:\n",
      "  batch 16 loss: 0.12308880957425572\n",
      "  batch 32 loss: 0.18078181199962273\n",
      "  batch 48 loss: 0.08173263381468132\n",
      "LOSS train 0.08173263381468132 valid 0.6182976961135864\n",
      "EPOCH 641:\n",
      "  batch 16 loss: 0.13608573550300207\n",
      "  batch 32 loss: 0.13802268111612648\n",
      "  batch 48 loss: 0.15868237253744155\n",
      "LOSS train 0.15868237253744155 valid 0.6187820434570312\n",
      "EPOCH 642:\n",
      "  batch 16 loss: 0.16848906473751413\n",
      "  batch 32 loss: 0.07014346614596434\n",
      "  batch 48 loss: 0.15407951135421172\n",
      "LOSS train 0.15407951135421172 valid 0.6194379925727844\n",
      "EPOCH 643:\n",
      "  batch 16 loss: 0.0881217286514584\n",
      "  batch 32 loss: 0.22355878714006394\n",
      "  batch 48 loss: 0.07634789773146622\n",
      "LOSS train 0.07634789773146622 valid 0.6204453110694885\n",
      "EPOCH 644:\n",
      "  batch 16 loss: 0.08746787370182574\n",
      "  batch 32 loss: 0.14769884367706254\n",
      "  batch 48 loss: 0.16365176305407658\n",
      "LOSS train 0.16365176305407658 valid 0.6202200651168823\n",
      "EPOCH 645:\n",
      "  batch 16 loss: 0.06540464570571203\n",
      "  batch 32 loss: 0.17331969970837235\n",
      "  batch 48 loss: 0.10630954196676612\n",
      "LOSS train 0.10630954196676612 valid 0.6205925941467285\n",
      "EPOCH 646:\n",
      "  batch 16 loss: 0.1170886825711932\n",
      "  batch 32 loss: 0.13044890464516357\n",
      "  batch 48 loss: 0.1432497011264786\n",
      "LOSS train 0.1432497011264786 valid 0.6215113401412964\n",
      "EPOCH 647:\n",
      "  batch 16 loss: 0.08556964440504089\n",
      "  batch 32 loss: 0.13882145535899326\n",
      "  batch 48 loss: 0.15391103888396174\n",
      "LOSS train 0.15391103888396174 valid 0.621605396270752\n",
      "EPOCH 648:\n",
      "  batch 16 loss: 0.08923326813965105\n",
      "  batch 32 loss: 0.18079065915662795\n",
      "  batch 48 loss: 0.10661442362470552\n",
      "LOSS train 0.10661442362470552 valid 0.6217702031135559\n",
      "EPOCH 649:\n",
      "  batch 16 loss: 0.1201136473682709\n",
      "  batch 32 loss: 0.11215341766364872\n",
      "  batch 48 loss: 0.17986561203724705\n",
      "LOSS train 0.17986561203724705 valid 0.6227099299430847\n",
      "EPOCH 650:\n",
      "  batch 16 loss: 0.15288943063933402\n",
      "  batch 32 loss: 0.1501679711509496\n",
      "  batch 48 loss: 0.08883107907604426\n",
      "LOSS train 0.08883107907604426 valid 0.6231311559677124\n",
      "EPOCH 651:\n",
      "  batch 16 loss: 0.11797695088898763\n",
      "  batch 32 loss: 0.1370249454921577\n",
      "  batch 48 loss: 0.14942384557798505\n",
      "LOSS train 0.14942384557798505 valid 0.6243919134140015\n",
      "EPOCH 652:\n",
      "  batch 16 loss: 0.1390675886068493\n",
      "  batch 32 loss: 0.16994374312344007\n",
      "  batch 48 loss: 0.11441812336852308\n",
      "LOSS train 0.11441812336852308 valid 0.6240329742431641\n",
      "EPOCH 653:\n",
      "  batch 16 loss: 0.08875311187148327\n",
      "  batch 32 loss: 0.16873688343912363\n",
      "  batch 48 loss: 0.14586336913635023\n",
      "LOSS train 0.14586336913635023 valid 0.6243792772293091\n",
      "EPOCH 654:\n",
      "  batch 16 loss: 0.09198647574521601\n",
      "  batch 32 loss: 0.13357855065260082\n",
      "  batch 48 loss: 0.1011667828715872\n",
      "LOSS train 0.1011667828715872 valid 0.6249420046806335\n",
      "EPOCH 655:\n",
      "  batch 16 loss: 0.13498618701123632\n",
      "  batch 32 loss: 0.19656448904424906\n",
      "  batch 48 loss: 0.1030548948328942\n",
      "LOSS train 0.1030548948328942 valid 0.6251378655433655\n",
      "EPOCH 656:\n",
      "  batch 16 loss: 0.1494570339564234\n",
      "  batch 32 loss: 0.1053166550482274\n",
      "  batch 48 loss: 0.12396065559005365\n",
      "LOSS train 0.12396065559005365 valid 0.6256875395774841\n",
      "EPOCH 657:\n",
      "  batch 16 loss: 0.1016736114397645\n",
      "  batch 32 loss: 0.09391816525021568\n",
      "  batch 48 loss: 0.16103975055739284\n",
      "LOSS train 0.16103975055739284 valid 0.6261927485466003\n",
      "EPOCH 658:\n",
      "  batch 16 loss: 0.15227443569892785\n",
      "  batch 32 loss: 0.0883181740064174\n",
      "  batch 48 loss: 0.10582209145650268\n",
      "LOSS train 0.10582209145650268 valid 0.6273837089538574\n",
      "EPOCH 659:\n",
      "  batch 16 loss: 0.08087442521355115\n",
      "  batch 32 loss: 0.07967142500274349\n",
      "  batch 48 loss: 0.23847688059322536\n",
      "LOSS train 0.23847688059322536 valid 0.6271721124649048\n",
      "EPOCH 660:\n",
      "  batch 16 loss: 0.10250749171245843\n",
      "  batch 32 loss: 0.150747821375262\n",
      "  batch 48 loss: 0.143764900858514\n",
      "LOSS train 0.143764900858514 valid 0.6271145343780518\n",
      "EPOCH 661:\n",
      "  batch 16 loss: 0.15764840101473965\n",
      "  batch 32 loss: 0.11919587216107175\n",
      "  batch 48 loss: 0.11660431046038866\n",
      "LOSS train 0.11660431046038866 valid 0.627974808216095\n",
      "EPOCH 662:\n",
      "  batch 16 loss: 0.1209739770856686\n",
      "  batch 32 loss: 0.17393112636636943\n",
      "  batch 48 loss: 0.12211541522992775\n",
      "LOSS train 0.12211541522992775 valid 0.6287336349487305\n",
      "EPOCH 663:\n",
      "  batch 16 loss: 0.16959870391292498\n",
      "  batch 32 loss: 0.14724199642660096\n",
      "  batch 48 loss: 0.09730078908614814\n",
      "LOSS train 0.09730078908614814 valid 0.62892746925354\n",
      "EPOCH 664:\n",
      "  batch 16 loss: 0.12928440247196704\n",
      "  batch 32 loss: 0.10435457911808044\n",
      "  batch 48 loss: 0.16183517395984381\n",
      "LOSS train 0.16183517395984381 valid 0.6294451951980591\n",
      "EPOCH 665:\n",
      "  batch 16 loss: 0.13024952226987807\n",
      "  batch 32 loss: 0.10799790051532909\n",
      "  batch 48 loss: 0.14460277208127081\n",
      "LOSS train 0.14460277208127081 valid 0.629553496837616\n",
      "EPOCH 666:\n",
      "  batch 16 loss: 0.20013263332657516\n",
      "  batch 32 loss: 0.06399248808156699\n",
      "  batch 48 loss: 0.07965272388537414\n",
      "LOSS train 0.07965272388537414 valid 0.6309614181518555\n",
      "EPOCH 667:\n",
      "  batch 16 loss: 0.06777020745357731\n",
      "  batch 32 loss: 0.17124096472980455\n",
      "  batch 48 loss: 0.15918150611105375\n",
      "LOSS train 0.15918150611105375 valid 0.6288405656814575\n",
      "EPOCH 668:\n",
      "  batch 16 loss: 0.12184734549373388\n",
      "  batch 32 loss: 0.19154199387412518\n",
      "  batch 48 loss: 0.10460223550762748\n",
      "LOSS train 0.10460223550762748 valid 0.6298022270202637\n",
      "EPOCH 669:\n",
      "  batch 16 loss: 0.09926472121151164\n",
      "  batch 32 loss: 0.11432725260965526\n",
      "  batch 48 loss: 0.17796423437539488\n",
      "LOSS train 0.17796423437539488 valid 0.6308577656745911\n",
      "EPOCH 670:\n",
      "  batch 16 loss: 0.18662226997548714\n",
      "  batch 32 loss: 0.08334946553804912\n",
      "  batch 48 loss: 0.128483774169581\n",
      "LOSS train 0.128483774169581 valid 0.631698489189148\n",
      "EPOCH 671:\n",
      "  batch 16 loss: 0.15523012878838927\n",
      "  batch 32 loss: 0.1528752013691701\n",
      "  batch 48 loss: 0.08629292316618375\n",
      "LOSS train 0.08629292316618375 valid 0.6322262287139893\n",
      "EPOCH 672:\n",
      "  batch 16 loss: 0.091552753772703\n",
      "  batch 32 loss: 0.11974243522854522\n",
      "  batch 48 loss: 0.1071930225007236\n",
      "LOSS train 0.1071930225007236 valid 0.6323697566986084\n",
      "EPOCH 673:\n",
      "  batch 16 loss: 0.16613423361559398\n",
      "  batch 32 loss: 0.1471943010110408\n",
      "  batch 48 loss: 0.09986541987746023\n",
      "LOSS train 0.09986541987746023 valid 0.6318945288658142\n",
      "EPOCH 674:\n",
      "  batch 16 loss: 0.16075449861818925\n",
      "  batch 32 loss: 0.11434683226980269\n",
      "  batch 48 loss: 0.08380091795697808\n",
      "LOSS train 0.08380091795697808 valid 0.6329425573348999\n",
      "EPOCH 675:\n",
      "  batch 16 loss: 0.105290207429789\n",
      "  batch 32 loss: 0.08592313117696904\n",
      "  batch 48 loss: 0.20074966311221942\n",
      "LOSS train 0.20074966311221942 valid 0.6338539123535156\n",
      "EPOCH 676:\n",
      "  batch 16 loss: 0.1265861680440139\n",
      "  batch 32 loss: 0.15534354443661869\n",
      "  batch 48 loss: 0.12467953287705313\n",
      "LOSS train 0.12467953287705313 valid 0.6339874267578125\n",
      "EPOCH 677:\n",
      "  batch 16 loss: 0.1178923643892631\n",
      "  batch 32 loss: 0.10614851053105667\n",
      "  batch 48 loss: 0.142014226061292\n",
      "LOSS train 0.142014226061292 valid 0.6346055269241333\n",
      "EPOCH 678:\n",
      "  batch 16 loss: 0.09473710987367667\n",
      "  batch 32 loss: 0.08880004653474316\n",
      "  batch 48 loss: 0.20923937714542262\n",
      "LOSS train 0.20923937714542262 valid 0.6350845098495483\n",
      "EPOCH 679:\n",
      "  batch 16 loss: 0.09917930062511005\n",
      "  batch 32 loss: 0.15584005365963094\n",
      "  batch 48 loss: 0.10681428509997204\n",
      "LOSS train 0.10681428509997204 valid 0.6356510519981384\n",
      "EPOCH 680:\n",
      "  batch 16 loss: 0.13077408919343725\n",
      "  batch 32 loss: 0.08687410276615992\n",
      "  batch 48 loss: 0.10503551104920916\n",
      "LOSS train 0.10503551104920916 valid 0.6361139416694641\n",
      "EPOCH 681:\n",
      "  batch 16 loss: 0.10080522333737463\n",
      "  batch 32 loss: 0.12854032239556545\n",
      "  batch 48 loss: 0.1614439976401627\n",
      "LOSS train 0.1614439976401627 valid 0.6365898847579956\n",
      "EPOCH 682:\n",
      "  batch 16 loss: 0.15867682396492455\n",
      "  batch 32 loss: 0.14972247619880363\n",
      "  batch 48 loss: 0.08901605452410877\n",
      "LOSS train 0.08901605452410877 valid 0.6369207501411438\n",
      "EPOCH 683:\n",
      "  batch 16 loss: 0.13581695225730073\n",
      "  batch 32 loss: 0.10268860068754293\n",
      "  batch 48 loss: 0.13747206749394536\n",
      "LOSS train 0.13747206749394536 valid 0.6375841498374939\n",
      "EPOCH 684:\n",
      "  batch 16 loss: 0.11933292489266023\n",
      "  batch 32 loss: 0.1105715062294621\n",
      "  batch 48 loss: 0.09906800219323486\n",
      "LOSS train 0.09906800219323486 valid 0.637764036655426\n",
      "EPOCH 685:\n",
      "  batch 16 loss: 0.1094313312205486\n",
      "  batch 32 loss: 0.1600870926922653\n",
      "  batch 48 loss: 0.0975958660710603\n",
      "LOSS train 0.0975958660710603 valid 0.6383575797080994\n",
      "EPOCH 686:\n",
      "  batch 16 loss: 0.12788816052488983\n",
      "  batch 32 loss: 0.1665338345337659\n",
      "  batch 48 loss: 0.054187487869057804\n",
      "LOSS train 0.054187487869057804 valid 0.6391642689704895\n",
      "EPOCH 687:\n",
      "  batch 16 loss: 0.09718318344675936\n",
      "  batch 32 loss: 0.16677237895783037\n",
      "  batch 48 loss: 0.08581577931181528\n",
      "LOSS train 0.08581577931181528 valid 0.6392773389816284\n",
      "EPOCH 688:\n",
      "  batch 16 loss: 0.16367831360548735\n",
      "  batch 32 loss: 0.15275549818761647\n",
      "  batch 48 loss: 0.07523013884201646\n",
      "LOSS train 0.07523013884201646 valid 0.6400498151779175\n",
      "EPOCH 689:\n",
      "  batch 16 loss: 0.11504627965041436\n",
      "  batch 32 loss: 0.10551934866816737\n",
      "  batch 48 loss: 0.12580559053458273\n",
      "LOSS train 0.12580559053458273 valid 0.6401576995849609\n",
      "EPOCH 690:\n",
      "  batch 16 loss: 0.10370846433215775\n",
      "  batch 32 loss: 0.1051381261786446\n",
      "  batch 48 loss: 0.1902808983286377\n",
      "LOSS train 0.1902808983286377 valid 0.6404774188995361\n",
      "EPOCH 691:\n",
      "  batch 16 loss: 0.08050283799821045\n",
      "  batch 32 loss: 0.15925989381503314\n",
      "  batch 48 loss: 0.1188441132690059\n",
      "LOSS train 0.1188441132690059 valid 0.6413271427154541\n",
      "EPOCH 692:\n",
      "  batch 16 loss: 0.0846756431274116\n",
      "  batch 32 loss: 0.14215357397915795\n",
      "  batch 48 loss: 0.1520913032873068\n",
      "LOSS train 0.1520913032873068 valid 0.6412861347198486\n",
      "EPOCH 693:\n",
      "  batch 16 loss: 0.10578738297044765\n",
      "  batch 32 loss: 0.10488520778017119\n",
      "  batch 48 loss: 0.16173840686678886\n",
      "LOSS train 0.16173840686678886 valid 0.6423367261886597\n",
      "EPOCH 694:\n",
      "  batch 16 loss: 0.10019286916212877\n",
      "  batch 32 loss: 0.15700026485137641\n",
      "  batch 48 loss: 0.0666212325450033\n",
      "LOSS train 0.0666212325450033 valid 0.6416990160942078\n",
      "EPOCH 695:\n",
      "  batch 16 loss: 0.09888298623263836\n",
      "  batch 32 loss: 0.10985797247849405\n",
      "  batch 48 loss: 0.09488317398063373\n",
      "LOSS train 0.09488317398063373 valid 0.6423653364181519\n",
      "EPOCH 696:\n",
      "  batch 16 loss: 0.08131261676317081\n",
      "  batch 32 loss: 0.1155016717966646\n",
      "  batch 48 loss: 0.13129394654970383\n",
      "LOSS train 0.13129394654970383 valid 0.6427651643753052\n",
      "EPOCH 697:\n",
      "  batch 16 loss: 0.12536050518974662\n",
      "  batch 32 loss: 0.11247793276561424\n",
      "  batch 48 loss: 0.12410077714594081\n",
      "LOSS train 0.12410077714594081 valid 0.6424340605735779\n",
      "EPOCH 698:\n",
      "  batch 16 loss: 0.16311143831990194\n",
      "  batch 32 loss: 0.11067366585484706\n",
      "  batch 48 loss: 0.10956513322889805\n",
      "LOSS train 0.10956513322889805 valid 0.6437370777130127\n",
      "EPOCH 699:\n",
      "  batch 16 loss: 0.09383282061753562\n",
      "  batch 32 loss: 0.10113642265787348\n",
      "  batch 48 loss: 0.1558805321692489\n",
      "LOSS train 0.1558805321692489 valid 0.6436026096343994\n",
      "EPOCH 700:\n",
      "  batch 16 loss: 0.07912723536719568\n",
      "  batch 32 loss: 0.20782294214586727\n",
      "  batch 48 loss: 0.07345008902484551\n",
      "LOSS train 0.07345008902484551 valid 0.6443030834197998\n",
      "EPOCH 701:\n",
      "  batch 16 loss: 0.15317119692917913\n",
      "  batch 32 loss: 0.08946882179589011\n",
      "  batch 48 loss: 0.11932300194166601\n",
      "LOSS train 0.11932300194166601 valid 0.6450495719909668\n",
      "EPOCH 702:\n",
      "  batch 16 loss: 0.09591228867066093\n",
      "  batch 32 loss: 0.07367394538596272\n",
      "  batch 48 loss: 0.1413401846657507\n",
      "LOSS train 0.1413401846657507 valid 0.6455117464065552\n",
      "EPOCH 703:\n",
      "  batch 16 loss: 0.1726445637177676\n",
      "  batch 32 loss: 0.1313257035217248\n",
      "  batch 48 loss: 0.07852026756154373\n",
      "LOSS train 0.07852026756154373 valid 0.6455829739570618\n",
      "EPOCH 704:\n",
      "  batch 16 loss: 0.10606738399656024\n",
      "  batch 32 loss: 0.1399361576186493\n",
      "  batch 48 loss: 0.1504637161269784\n",
      "LOSS train 0.1504637161269784 valid 0.6462656259536743\n",
      "EPOCH 705:\n",
      "  batch 16 loss: 0.1352888239780441\n",
      "  batch 32 loss: 0.06796596862841398\n",
      "  batch 48 loss: 0.09806402631511446\n",
      "LOSS train 0.09806402631511446 valid 0.6472291350364685\n",
      "EPOCH 706:\n",
      "  batch 16 loss: 0.13181706175237196\n",
      "  batch 32 loss: 0.08660627971403301\n",
      "  batch 48 loss: 0.13049210695317015\n",
      "LOSS train 0.13049210695317015 valid 0.6470287442207336\n",
      "EPOCH 707:\n",
      "  batch 16 loss: 0.10908580583054572\n",
      "  batch 32 loss: 0.13108363805804402\n",
      "  batch 48 loss: 0.11999130071490072\n",
      "LOSS train 0.11999130071490072 valid 0.6486152410507202\n",
      "EPOCH 708:\n",
      "  batch 16 loss: 0.09180638819816522\n",
      "  batch 32 loss: 0.15528751793317497\n",
      "  batch 48 loss: 0.09017659121309407\n",
      "LOSS train 0.09017659121309407 valid 0.6482403874397278\n",
      "EPOCH 709:\n",
      "  batch 16 loss: 0.13342214014846832\n",
      "  batch 32 loss: 0.10830486880149692\n",
      "  batch 48 loss: 0.10184946800291073\n",
      "LOSS train 0.10184946800291073 valid 0.6490853428840637\n",
      "EPOCH 710:\n",
      "  batch 16 loss: 0.11113746516639367\n",
      "  batch 32 loss: 0.10460160064394586\n",
      "  batch 48 loss: 0.1502993680187501\n",
      "LOSS train 0.1502993680187501 valid 0.6494614481925964\n",
      "EPOCH 711:\n",
      "  batch 16 loss: 0.15392182916912134\n",
      "  batch 32 loss: 0.05630511260824278\n",
      "  batch 48 loss: 0.15051437390502542\n",
      "LOSS train 0.15051437390502542 valid 0.6500582695007324\n",
      "EPOCH 712:\n",
      "  batch 16 loss: 0.09722529322607443\n",
      "  batch 32 loss: 0.08000952255679294\n",
      "  batch 48 loss: 0.15424336228170432\n",
      "LOSS train 0.15424336228170432 valid 0.6498973369598389\n",
      "EPOCH 713:\n",
      "  batch 16 loss: 0.10398440398421371\n",
      "  batch 32 loss: 0.0781320272362791\n",
      "  batch 48 loss: 0.13460825988295255\n",
      "LOSS train 0.13460825988295255 valid 0.6505621075630188\n",
      "EPOCH 714:\n",
      "  batch 16 loss: 0.13084267875819933\n",
      "  batch 32 loss: 0.09861562677542679\n",
      "  batch 48 loss: 0.11103041673777625\n",
      "LOSS train 0.11103041673777625 valid 0.6522917747497559\n",
      "EPOCH 715:\n",
      "  batch 16 loss: 0.10524266766151413\n",
      "  batch 32 loss: 0.08228923645219766\n",
      "  batch 48 loss: 0.1098442884394899\n",
      "LOSS train 0.1098442884394899 valid 0.6520826816558838\n",
      "EPOCH 716:\n",
      "  batch 16 loss: 0.11060861448640935\n",
      "  batch 32 loss: 0.11924886654378497\n",
      "  batch 48 loss: 0.07306166527268942\n",
      "LOSS train 0.07306166527268942 valid 0.652374267578125\n",
      "EPOCH 717:\n",
      "  batch 16 loss: 0.07873477984685451\n",
      "  batch 32 loss: 0.10737678484292701\n",
      "  batch 48 loss: 0.1571310321232886\n",
      "LOSS train 0.1571310321232886 valid 0.6513463258743286\n",
      "EPOCH 718:\n",
      "  batch 16 loss: 0.10654584399890155\n",
      "  batch 32 loss: 0.11533642379799858\n",
      "  batch 48 loss: 0.1130160959000932\n",
      "LOSS train 0.1130160959000932 valid 0.6528322696685791\n",
      "EPOCH 719:\n",
      "  batch 16 loss: 0.09994221682427451\n",
      "  batch 32 loss: 0.1458229781128466\n",
      "  batch 48 loss: 0.10968571990088094\n",
      "LOSS train 0.10968571990088094 valid 0.6529790163040161\n",
      "EPOCH 720:\n",
      "  batch 16 loss: 0.12588270351989195\n",
      "  batch 32 loss: 0.12731190463819075\n",
      "  batch 48 loss: 0.12229273142293096\n",
      "LOSS train 0.12229273142293096 valid 0.6532872319221497\n",
      "EPOCH 721:\n",
      "  batch 16 loss: 0.1162934391759336\n",
      "  batch 32 loss: 0.1265797018277226\n",
      "  batch 48 loss: 0.11201969138346612\n",
      "LOSS train 0.11201969138346612 valid 0.6540108919143677\n",
      "EPOCH 722:\n",
      "  batch 16 loss: 0.13415267679374665\n",
      "  batch 32 loss: 0.11749687453266233\n",
      "  batch 48 loss: 0.09824880119413137\n",
      "LOSS train 0.09824880119413137 valid 0.6544209718704224\n",
      "EPOCH 723:\n",
      "  batch 16 loss: 0.09277816006215289\n",
      "  batch 32 loss: 0.09066038974560797\n",
      "  batch 48 loss: 0.09925998916151002\n",
      "LOSS train 0.09925998916151002 valid 0.6550602912902832\n",
      "EPOCH 724:\n",
      "  batch 16 loss: 0.10059921654465143\n",
      "  batch 32 loss: 0.10239341558190063\n",
      "  batch 48 loss: 0.12349666736554354\n",
      "LOSS train 0.12349666736554354 valid 0.655853271484375\n",
      "EPOCH 725:\n",
      "  batch 16 loss: 0.0779299063142389\n",
      "  batch 32 loss: 0.13326311961282045\n",
      "  batch 48 loss: 0.138811908691423\n",
      "LOSS train 0.138811908691423 valid 0.6555554866790771\n",
      "EPOCH 726:\n",
      "  batch 16 loss: 0.07146686455234885\n",
      "  batch 32 loss: 0.14713025622768328\n",
      "  batch 48 loss: 0.13432569391443394\n",
      "LOSS train 0.13432569391443394 valid 0.6563106775283813\n",
      "EPOCH 727:\n",
      "  batch 16 loss: 0.12019533524289727\n",
      "  batch 32 loss: 0.1048578760237433\n",
      "  batch 48 loss: 0.09938356463680975\n",
      "LOSS train 0.09938356463680975 valid 0.6572017669677734\n",
      "EPOCH 728:\n",
      "  batch 16 loss: 0.12500315648503602\n",
      "  batch 32 loss: 0.10976489435415715\n",
      "  batch 48 loss: 0.0737812985025812\n",
      "LOSS train 0.0737812985025812 valid 0.6571511030197144\n",
      "EPOCH 729:\n",
      "  batch 16 loss: 0.14991006604395807\n",
      "  batch 32 loss: 0.0866597801214084\n",
      "  batch 48 loss: 0.09694858915463556\n",
      "LOSS train 0.09694858915463556 valid 0.6574469804763794\n",
      "EPOCH 730:\n",
      "  batch 16 loss: 0.07782515094731934\n",
      "  batch 32 loss: 0.07627431181026623\n",
      "  batch 48 loss: 0.1974526655394584\n",
      "LOSS train 0.1974526655394584 valid 0.6585525274276733\n",
      "EPOCH 731:\n",
      "  batch 16 loss: 0.09411250788252801\n",
      "  batch 32 loss: 0.09389543739962392\n",
      "  batch 48 loss: 0.07165315886959434\n",
      "LOSS train 0.07165315886959434 valid 0.6596238017082214\n",
      "EPOCH 732:\n",
      "  batch 16 loss: 0.12237026283401065\n",
      "  batch 32 loss: 0.12385438036289997\n",
      "  batch 48 loss: 0.06364892254350707\n",
      "LOSS train 0.06364892254350707 valid 0.6591454744338989\n",
      "EPOCH 733:\n",
      "  batch 16 loss: 0.058336605463409796\n",
      "  batch 32 loss: 0.1292192132677883\n",
      "  batch 48 loss: 0.08736415841849521\n",
      "LOSS train 0.08736415841849521 valid 0.6604617238044739\n",
      "EPOCH 734:\n",
      "  batch 16 loss: 0.13996726390905678\n",
      "  batch 32 loss: 0.12771767273079604\n",
      "  batch 48 loss: 0.0742926117090974\n",
      "LOSS train 0.0742926117090974 valid 0.6606614589691162\n",
      "EPOCH 735:\n",
      "  batch 16 loss: 0.07345993592753075\n",
      "  batch 32 loss: 0.09055572756915353\n",
      "  batch 48 loss: 0.11976404552115127\n",
      "LOSS train 0.11976404552115127 valid 0.6606017351150513\n",
      "EPOCH 736:\n",
      "  batch 16 loss: 0.07818173395935446\n",
      "  batch 32 loss: 0.11941265429777559\n",
      "  batch 48 loss: 0.08946935809217393\n",
      "LOSS train 0.08946935809217393 valid 0.6614542603492737\n",
      "EPOCH 737:\n",
      "  batch 16 loss: 0.08982872424530797\n",
      "  batch 32 loss: 0.11580969971691957\n",
      "  batch 48 loss: 0.12708982871845365\n",
      "LOSS train 0.12708982871845365 valid 0.6617873907089233\n",
      "EPOCH 738:\n",
      "  batch 16 loss: 0.13061753381043673\n",
      "  batch 32 loss: 0.13096751753619174\n",
      "  batch 48 loss: 0.08331578897195868\n",
      "LOSS train 0.08331578897195868 valid 0.6630136966705322\n",
      "EPOCH 739:\n",
      "  batch 16 loss: 0.08009226142894477\n",
      "  batch 32 loss: 0.10345704481005669\n",
      "  batch 48 loss: 0.12790916190715507\n",
      "LOSS train 0.12790916190715507 valid 0.6638609766960144\n",
      "EPOCH 740:\n",
      "  batch 16 loss: 0.1353623870672891\n",
      "  batch 32 loss: 0.05475804046727717\n",
      "  batch 48 loss: 0.12411705072736368\n",
      "LOSS train 0.12411705072736368 valid 0.6634707450866699\n",
      "EPOCH 741:\n",
      "  batch 16 loss: 0.09430100140161812\n",
      "  batch 32 loss: 0.09978226578095928\n",
      "  batch 48 loss: 0.10177176212891936\n",
      "LOSS train 0.10177176212891936 valid 0.6645945906639099\n",
      "EPOCH 742:\n",
      "  batch 16 loss: 0.11320920317666605\n",
      "  batch 32 loss: 0.11703923657478299\n",
      "  batch 48 loss: 0.10461900057271123\n",
      "LOSS train 0.10461900057271123 valid 0.6643656492233276\n",
      "EPOCH 743:\n",
      "  batch 16 loss: 0.08222851506434381\n",
      "  batch 32 loss: 0.10765571825322695\n",
      "  batch 48 loss: 0.12522332987282425\n",
      "LOSS train 0.12522332987282425 valid 0.6648157238960266\n",
      "EPOCH 744:\n",
      "  batch 16 loss: 0.0832341288914904\n",
      "  batch 32 loss: 0.09587930515408516\n",
      "  batch 48 loss: 0.10056318971328437\n",
      "LOSS train 0.10056318971328437 valid 0.6651425361633301\n",
      "EPOCH 745:\n",
      "  batch 16 loss: 0.07922661630436778\n",
      "  batch 32 loss: 0.08684752146655228\n",
      "  batch 48 loss: 0.1581904684426263\n",
      "LOSS train 0.1581904684426263 valid 0.6657199859619141\n",
      "EPOCH 746:\n",
      "  batch 16 loss: 0.08238147483643843\n",
      "  batch 32 loss: 0.0751079131005099\n",
      "  batch 48 loss: 0.11438132272451185\n",
      "LOSS train 0.11438132272451185 valid 0.6666479110717773\n",
      "EPOCH 747:\n",
      "  batch 16 loss: 0.1477753283106722\n",
      "  batch 32 loss: 0.10490071900130715\n",
      "  batch 48 loss: 0.0647111134021543\n",
      "LOSS train 0.0647111134021543 valid 0.6669386625289917\n",
      "EPOCH 748:\n",
      "  batch 16 loss: 0.11813564811018296\n",
      "  batch 32 loss: 0.13949324103305116\n",
      "  batch 48 loss: 0.09721228224225342\n",
      "LOSS train 0.09721228224225342 valid 0.6670998334884644\n",
      "EPOCH 749:\n",
      "  batch 16 loss: 0.07587700599106029\n",
      "  batch 32 loss: 0.1364580003428273\n",
      "  batch 48 loss: 0.09117841470288113\n",
      "LOSS train 0.09117841470288113 valid 0.6678301095962524\n",
      "EPOCH 750:\n",
      "  batch 16 loss: 0.10183615807909518\n",
      "  batch 32 loss: 0.09651281812693924\n",
      "  batch 48 loss: 0.11530760169262066\n",
      "LOSS train 0.11530760169262066 valid 0.6684054732322693\n",
      "EPOCH 751:\n",
      "  batch 16 loss: 0.10602902657410596\n",
      "  batch 32 loss: 0.1525012330384925\n",
      "  batch 48 loss: 0.08354537436389364\n",
      "LOSS train 0.08354537436389364 valid 0.6688368916511536\n",
      "EPOCH 752:\n",
      "  batch 16 loss: 0.13114684663014486\n",
      "  batch 32 loss: 0.09111443816800602\n",
      "  batch 48 loss: 0.12257130717625841\n",
      "LOSS train 0.12257130717625841 valid 0.6689324378967285\n",
      "EPOCH 753:\n",
      "  batch 16 loss: 0.16007074696244672\n",
      "  batch 32 loss: 0.07098114254404209\n",
      "  batch 48 loss: 0.07632249628659338\n",
      "LOSS train 0.07632249628659338 valid 0.6696677207946777\n",
      "EPOCH 754:\n",
      "  batch 16 loss: 0.1167197551112622\n",
      "  batch 32 loss: 0.13038348930422217\n",
      "  batch 48 loss: 0.06259266895358451\n",
      "LOSS train 0.06259266895358451 valid 0.6706453561782837\n",
      "EPOCH 755:\n",
      "  batch 16 loss: 0.10479474224848673\n",
      "  batch 32 loss: 0.0725693860440515\n",
      "  batch 48 loss: 0.14165538595989347\n",
      "LOSS train 0.14165538595989347 valid 0.6708389520645142\n",
      "EPOCH 756:\n",
      "  batch 16 loss: 0.07143721374450251\n",
      "  batch 32 loss: 0.1268777289078571\n",
      "  batch 48 loss: 0.13049072108697146\n",
      "LOSS train 0.13049072108697146 valid 0.671011209487915\n",
      "EPOCH 757:\n",
      "  batch 16 loss: 0.0880599739466561\n",
      "  batch 32 loss: 0.09744035499170423\n",
      "  batch 48 loss: 0.11005460110027343\n",
      "LOSS train 0.11005460110027343 valid 0.6716820001602173\n",
      "EPOCH 758:\n",
      "  batch 16 loss: 0.09000392438611016\n",
      "  batch 32 loss: 0.08457974017073866\n",
      "  batch 48 loss: 0.12989820754592074\n",
      "LOSS train 0.12989820754592074 valid 0.6715523600578308\n",
      "EPOCH 759:\n",
      "  batch 16 loss: 0.10251280781812966\n",
      "  batch 32 loss: 0.07917242392431945\n",
      "  batch 48 loss: 0.10823135673126671\n",
      "LOSS train 0.10823135673126671 valid 0.6725826263427734\n",
      "EPOCH 760:\n",
      "  batch 16 loss: 0.0811707698594546\n",
      "  batch 32 loss: 0.11701409629313275\n",
      "  batch 48 loss: 0.09030878663179465\n",
      "LOSS train 0.09030878663179465 valid 0.6728116869926453\n",
      "EPOCH 761:\n",
      "  batch 16 loss: 0.08327775972429663\n",
      "  batch 32 loss: 0.06766817055176944\n",
      "  batch 48 loss: 0.13980473642004654\n",
      "LOSS train 0.13980473642004654 valid 0.6736118793487549\n",
      "EPOCH 762:\n",
      "  batch 16 loss: 0.09009089431492612\n",
      "  batch 32 loss: 0.1119887379463762\n",
      "  batch 48 loss: 0.10794058366445825\n",
      "LOSS train 0.10794058366445825 valid 0.674662709236145\n",
      "EPOCH 763:\n",
      "  batch 16 loss: 0.05546026754018385\n",
      "  batch 32 loss: 0.10383029453805648\n",
      "  batch 48 loss: 0.10822223633294925\n",
      "LOSS train 0.10822223633294925 valid 0.6750457286834717\n",
      "EPOCH 764:\n",
      "  batch 16 loss: 0.14046411609160714\n",
      "  batch 32 loss: 0.08652803394943476\n",
      "  batch 48 loss: 0.0999077603337355\n",
      "LOSS train 0.0999077603337355 valid 0.6748865246772766\n",
      "EPOCH 765:\n",
      "  batch 16 loss: 0.12878981261746958\n",
      "  batch 32 loss: 0.09173007975914516\n",
      "  batch 48 loss: 0.09948242426617071\n",
      "LOSS train 0.09948242426617071 valid 0.6751731038093567\n",
      "EPOCH 766:\n",
      "  batch 16 loss: 0.15231654022500152\n",
      "  batch 32 loss: 0.13276517676422372\n",
      "  batch 48 loss: 0.04050659170025028\n",
      "LOSS train 0.04050659170025028 valid 0.6764705777168274\n",
      "EPOCH 767:\n",
      "  batch 16 loss: 0.09387826779857278\n",
      "  batch 32 loss: 0.06485229622921906\n",
      "  batch 48 loss: 0.1431950178812258\n",
      "LOSS train 0.1431950178812258 valid 0.6762245893478394\n",
      "EPOCH 768:\n",
      "  batch 16 loss: 0.11388625990366563\n",
      "  batch 32 loss: 0.09985468714148737\n",
      "  batch 48 loss: 0.11434448687941767\n",
      "LOSS train 0.11434448687941767 valid 0.676240086555481\n",
      "EPOCH 769:\n",
      "  batch 16 loss: 0.07594633107510163\n",
      "  batch 32 loss: 0.09638611882110126\n",
      "  batch 48 loss: 0.1581052299006842\n",
      "LOSS train 0.1581052299006842 valid 0.6772447824478149\n",
      "EPOCH 770:\n",
      "  batch 16 loss: 0.10119711305014789\n",
      "  batch 32 loss: 0.11416560993529856\n",
      "  batch 48 loss: 0.06717086519347504\n",
      "LOSS train 0.06717086519347504 valid 0.6779416799545288\n",
      "EPOCH 771:\n",
      "  batch 16 loss: 0.05266360193490982\n",
      "  batch 32 loss: 0.05079122938332148\n",
      "  batch 48 loss: 0.13994194151018746\n",
      "LOSS train 0.13994194151018746 valid 0.6784707903862\n",
      "EPOCH 772:\n",
      "  batch 16 loss: 0.09330492990557104\n",
      "  batch 32 loss: 0.13976607742370106\n",
      "  batch 48 loss: 0.07325366031000158\n",
      "LOSS train 0.07325366031000158 valid 0.6785451769828796\n",
      "EPOCH 773:\n",
      "  batch 16 loss: 0.0661291375872679\n",
      "  batch 32 loss: 0.08297195637715049\n",
      "  batch 48 loss: 0.1291555126372259\n",
      "LOSS train 0.1291555126372259 valid 0.6794750094413757\n",
      "EPOCH 774:\n",
      "  batch 16 loss: 0.10970725849256269\n",
      "  batch 32 loss: 0.10009706830896903\n",
      "  batch 48 loss: 0.1036190812883433\n",
      "LOSS train 0.1036190812883433 valid 0.6792527437210083\n",
      "EPOCH 775:\n",
      "  batch 16 loss: 0.0959743463608902\n",
      "  batch 32 loss: 0.0970885381102562\n",
      "  batch 48 loss: 0.1389500204095384\n",
      "LOSS train 0.1389500204095384 valid 0.679839015007019\n",
      "EPOCH 776:\n",
      "  batch 16 loss: 0.09595570576493628\n",
      "  batch 32 loss: 0.09744707844220102\n",
      "  batch 48 loss: 0.1307494395005051\n",
      "LOSS train 0.1307494395005051 valid 0.6798685193061829\n",
      "EPOCH 777:\n",
      "  batch 16 loss: 0.05904436856508255\n",
      "  batch 32 loss: 0.1181652910890989\n",
      "  batch 48 loss: 0.10755340810283087\n",
      "LOSS train 0.10755340810283087 valid 0.6808809041976929\n",
      "EPOCH 778:\n",
      "  batch 16 loss: 0.08471268997527659\n",
      "  batch 32 loss: 0.13631855533458292\n",
      "  batch 48 loss: 0.06845619226805866\n",
      "LOSS train 0.06845619226805866 valid 0.6816575527191162\n",
      "EPOCH 779:\n",
      "  batch 16 loss: 0.0963104308466427\n",
      "  batch 32 loss: 0.13205599528737366\n",
      "  batch 48 loss: 0.0982008806313388\n",
      "LOSS train 0.0982008806313388 valid 0.6815808415412903\n",
      "EPOCH 780:\n",
      "  batch 16 loss: 0.09184057806851342\n",
      "  batch 32 loss: 0.08661279297666624\n",
      "  batch 48 loss: 0.10534655849914998\n",
      "LOSS train 0.10534655849914998 valid 0.6822611093521118\n",
      "EPOCH 781:\n",
      "  batch 16 loss: 0.10545371446642093\n",
      "  batch 32 loss: 0.11689840297913179\n",
      "  batch 48 loss: 0.08648564736358821\n",
      "LOSS train 0.08648564736358821 valid 0.6823040843009949\n",
      "EPOCH 782:\n",
      "  batch 16 loss: 0.07345660054124892\n",
      "  batch 32 loss: 0.06181722772453213\n",
      "  batch 48 loss: 0.09531495696865022\n",
      "LOSS train 0.09531495696865022 valid 0.6832024455070496\n",
      "EPOCH 783:\n",
      "  batch 16 loss: 0.06935697584412992\n",
      "  batch 32 loss: 0.06518356926972046\n",
      "  batch 48 loss: 0.15649903602025006\n",
      "LOSS train 0.15649903602025006 valid 0.6832734942436218\n",
      "EPOCH 784:\n",
      "  batch 16 loss: 0.10509417555294931\n",
      "  batch 32 loss: 0.08165024372283369\n",
      "  batch 48 loss: 0.09930323161825072\n",
      "LOSS train 0.09930323161825072 valid 0.6835939884185791\n",
      "EPOCH 785:\n",
      "  batch 16 loss: 0.10297904961043969\n",
      "  batch 32 loss: 0.0779793473193422\n",
      "  batch 48 loss: 0.10465307347476482\n",
      "LOSS train 0.10465307347476482 valid 0.6851155757904053\n",
      "EPOCH 786:\n",
      "  batch 16 loss: 0.1310351665597409\n",
      "  batch 32 loss: 0.10691387402039254\n",
      "  batch 48 loss: 0.08220666093984619\n",
      "LOSS train 0.08220666093984619 valid 0.6851239204406738\n",
      "EPOCH 787:\n",
      "  batch 16 loss: 0.09328763448866084\n",
      "  batch 32 loss: 0.11781956782215275\n",
      "  batch 48 loss: 0.08543495633057319\n",
      "LOSS train 0.08543495633057319 valid 0.6861662268638611\n",
      "EPOCH 788:\n",
      "  batch 16 loss: 0.08645332313972176\n",
      "  batch 32 loss: 0.09751805616542697\n",
      "  batch 48 loss: 0.12369542369560804\n",
      "LOSS train 0.12369542369560804 valid 0.6861035823822021\n",
      "EPOCH 789:\n",
      "  batch 16 loss: 0.10725366187398322\n",
      "  batch 32 loss: 0.11000700949807651\n",
      "  batch 48 loss: 0.08401182485977188\n",
      "LOSS train 0.08401182485977188 valid 0.686855673789978\n",
      "EPOCH 790:\n",
      "  batch 16 loss: 0.09031405572750373\n",
      "  batch 32 loss: 0.10348022932885215\n",
      "  batch 48 loss: 0.09468418877804652\n",
      "LOSS train 0.09468418877804652 valid 0.6873908042907715\n",
      "EPOCH 791:\n",
      "  batch 16 loss: 0.12880941876210272\n",
      "  batch 32 loss: 0.0556531115580583\n",
      "  batch 48 loss: 0.08076399611309171\n",
      "LOSS train 0.08076399611309171 valid 0.6877635717391968\n",
      "EPOCH 792:\n",
      "  batch 16 loss: 0.08210924657760188\n",
      "  batch 32 loss: 0.07784616213757545\n",
      "  batch 48 loss: 0.1282499474764336\n",
      "LOSS train 0.1282499474764336 valid 0.6880666017532349\n",
      "EPOCH 793:\n",
      "  batch 16 loss: 0.14265353421797045\n",
      "  batch 32 loss: 0.08790622738888487\n",
      "  batch 48 loss: 0.07400760319433175\n",
      "LOSS train 0.07400760319433175 valid 0.6891695261001587\n",
      "EPOCH 794:\n",
      "  batch 16 loss: 0.10566588534857146\n",
      "  batch 32 loss: 0.09198297647526488\n",
      "  batch 48 loss: 0.12123274721670896\n",
      "LOSS train 0.12123274721670896 valid 0.6896344423294067\n",
      "EPOCH 795:\n",
      "  batch 16 loss: 0.12262273806845769\n",
      "  batch 32 loss: 0.09871922811726108\n",
      "  batch 48 loss: 0.10631956456927583\n",
      "LOSS train 0.10631956456927583 valid 0.6893408298492432\n",
      "EPOCH 796:\n",
      "  batch 16 loss: 0.11097513331333175\n",
      "  batch 32 loss: 0.10780942882411182\n",
      "  batch 48 loss: 0.09046199475415051\n",
      "LOSS train 0.09046199475415051 valid 0.689591109752655\n",
      "EPOCH 797:\n",
      "  batch 16 loss: 0.09581714251544327\n",
      "  batch 32 loss: 0.10073226445820183\n",
      "  batch 48 loss: 0.1109680462977849\n",
      "LOSS train 0.1109680462977849 valid 0.6905905604362488\n",
      "EPOCH 798:\n",
      "  batch 16 loss: 0.11918240003433311\n",
      "  batch 32 loss: 0.09161033749114722\n",
      "  batch 48 loss: 0.07806994473503437\n",
      "LOSS train 0.07806994473503437 valid 0.6907607316970825\n",
      "EPOCH 799:\n",
      "  batch 16 loss: 0.08511831448413432\n",
      "  batch 32 loss: 0.10968666075496003\n",
      "  batch 48 loss: 0.09012736397562549\n",
      "LOSS train 0.09012736397562549 valid 0.6913282871246338\n",
      "EPOCH 800:\n",
      "  batch 16 loss: 0.11108719350886531\n",
      "  batch 32 loss: 0.08302101433218922\n",
      "  batch 48 loss: 0.10519509555888362\n",
      "LOSS train 0.10519509555888362 valid 0.6916964650154114\n",
      "EPOCH 801:\n",
      "  batch 16 loss: 0.06245844764634967\n",
      "  batch 32 loss: 0.0709331703837961\n",
      "  batch 48 loss: 0.09793704468756914\n",
      "LOSS train 0.09793704468756914 valid 0.6927640438079834\n",
      "EPOCH 802:\n",
      "  batch 16 loss: 0.11656311448314227\n",
      "  batch 32 loss: 0.09679297494585626\n",
      "  batch 48 loss: 0.06670352860237472\n",
      "LOSS train 0.06670352860237472 valid 0.6923216581344604\n",
      "EPOCH 803:\n",
      "  batch 16 loss: 0.10310311173088849\n",
      "  batch 32 loss: 0.08771400558180176\n",
      "  batch 48 loss: 0.07785744951252127\n",
      "LOSS train 0.07785744951252127 valid 0.6935646533966064\n",
      "EPOCH 804:\n",
      "  batch 16 loss: 0.040595481637865305\n",
      "  batch 32 loss: 0.09067628544289619\n",
      "  batch 48 loss: 0.10099227580940351\n",
      "LOSS train 0.10099227580940351 valid 0.6931060552597046\n",
      "EPOCH 805:\n",
      "  batch 16 loss: 0.10269122011959553\n",
      "  batch 32 loss: 0.07845058175735176\n",
      "  batch 48 loss: 0.08874048273719382\n",
      "LOSS train 0.08874048273719382 valid 0.6938586831092834\n",
      "EPOCH 806:\n",
      "  batch 16 loss: 0.10973714978899807\n",
      "  batch 32 loss: 0.07951812297687866\n",
      "  batch 48 loss: 0.0967876196373254\n",
      "LOSS train 0.0967876196373254 valid 0.6942297220230103\n",
      "EPOCH 807:\n",
      "  batch 16 loss: 0.06214970356086269\n",
      "  batch 32 loss: 0.06787291081855074\n",
      "  batch 48 loss: 0.07576670474372804\n",
      "LOSS train 0.07576670474372804 valid 0.6956629753112793\n",
      "EPOCH 808:\n",
      "  batch 16 loss: 0.0692216728348285\n",
      "  batch 32 loss: 0.11375314340693876\n",
      "  batch 48 loss: 0.10443659301381558\n",
      "LOSS train 0.10443659301381558 valid 0.69570392370224\n",
      "EPOCH 809:\n",
      "  batch 16 loss: 0.06769493693718687\n",
      "  batch 32 loss: 0.09432158584240824\n",
      "  batch 48 loss: 0.12209242879180238\n",
      "LOSS train 0.12209242879180238 valid 0.6955891847610474\n",
      "EPOCH 810:\n",
      "  batch 16 loss: 0.0778685993864201\n",
      "  batch 32 loss: 0.13616276299580932\n",
      "  batch 48 loss: 0.0687344089965336\n",
      "LOSS train 0.0687344089965336 valid 0.6965934038162231\n",
      "EPOCH 811:\n",
      "  batch 16 loss: 0.069265685393475\n",
      "  batch 32 loss: 0.10166337402188219\n",
      "  batch 48 loss: 0.08176034738426097\n",
      "LOSS train 0.08176034738426097 valid 0.697180986404419\n",
      "EPOCH 812:\n",
      "  batch 16 loss: 0.07126049585349392\n",
      "  batch 32 loss: 0.11195847584167495\n",
      "  batch 48 loss: 0.07123818517720792\n",
      "LOSS train 0.07123818517720792 valid 0.6972299814224243\n",
      "EPOCH 813:\n",
      "  batch 16 loss: 0.07204740970337298\n",
      "  batch 32 loss: 0.07300489846238634\n",
      "  batch 48 loss: 0.08164851082256064\n",
      "LOSS train 0.08164851082256064 valid 0.6987140774726868\n",
      "EPOCH 814:\n",
      "  batch 16 loss: 0.06349207502353238\n",
      "  batch 32 loss: 0.11687126295873895\n",
      "  batch 48 loss: 0.08688227646052837\n",
      "LOSS train 0.08688227646052837 valid 0.6989836096763611\n",
      "EPOCH 815:\n",
      "  batch 16 loss: 0.1026204666995909\n",
      "  batch 32 loss: 0.06944621179718524\n",
      "  batch 48 loss: 0.117688632934005\n",
      "LOSS train 0.117688632934005 valid 0.6994036436080933\n",
      "EPOCH 816:\n",
      "  batch 16 loss: 0.09050826402381063\n",
      "  batch 32 loss: 0.056328163904254325\n",
      "  batch 48 loss: 0.10519970749737695\n",
      "LOSS train 0.10519970749737695 valid 0.6998195052146912\n",
      "EPOCH 817:\n",
      "  batch 16 loss: 0.11082882157643326\n",
      "  batch 32 loss: 0.09021123469574377\n",
      "  batch 48 loss: 0.05564390402287245\n",
      "LOSS train 0.05564390402287245 valid 0.7000123262405396\n",
      "EPOCH 818:\n",
      "  batch 16 loss: 0.0987629244918935\n",
      "  batch 32 loss: 0.08288069360423833\n",
      "  batch 48 loss: 0.08802238285716157\n",
      "LOSS train 0.08802238285716157 valid 0.7014855146408081\n",
      "EPOCH 819:\n",
      "  batch 16 loss: 0.07545423356350511\n",
      "  batch 32 loss: 0.1550529477535747\n",
      "  batch 48 loss: 0.060332272230880335\n",
      "LOSS train 0.060332272230880335 valid 0.7017010450363159\n",
      "EPOCH 820:\n",
      "  batch 16 loss: 0.09066086407983676\n",
      "  batch 32 loss: 0.12601127673406154\n",
      "  batch 48 loss: 0.0648741399490973\n",
      "LOSS train 0.0648741399490973 valid 0.7019381523132324\n",
      "EPOCH 821:\n",
      "  batch 16 loss: 0.05572852800833061\n",
      "  batch 32 loss: 0.11746926492196508\n",
      "  batch 48 loss: 0.11357766148285009\n",
      "LOSS train 0.11357766148285009 valid 0.7019256353378296\n",
      "EPOCH 822:\n",
      "  batch 16 loss: 0.07788132928544655\n",
      "  batch 32 loss: 0.12683054694207385\n",
      "  batch 48 loss: 0.07698057095694821\n",
      "LOSS train 0.07698057095694821 valid 0.7024657726287842\n",
      "EPOCH 823:\n",
      "  batch 16 loss: 0.1215201485902071\n",
      "  batch 32 loss: 0.06129144594888203\n",
      "  batch 48 loss: 0.09897009545238689\n",
      "LOSS train 0.09897009545238689 valid 0.7028636932373047\n",
      "EPOCH 824:\n",
      "  batch 16 loss: 0.1336156629549805\n",
      "  batch 32 loss: 0.07613059063442051\n",
      "  batch 48 loss: 0.08793709066230804\n",
      "LOSS train 0.08793709066230804 valid 0.7028112411499023\n",
      "EPOCH 825:\n",
      "  batch 16 loss: 0.07045534515054896\n",
      "  batch 32 loss: 0.10738182510249317\n",
      "  batch 48 loss: 0.09406402418971993\n",
      "LOSS train 0.09406402418971993 valid 0.7034117579460144\n",
      "EPOCH 826:\n",
      "  batch 16 loss: 0.09393126294889953\n",
      "  batch 32 loss: 0.06976435607066378\n",
      "  batch 48 loss: 0.11603269068291411\n",
      "LOSS train 0.11603269068291411 valid 0.7038043737411499\n",
      "EPOCH 827:\n",
      "  batch 16 loss: 0.12351051584118977\n",
      "  batch 32 loss: 0.06597312743542716\n",
      "  batch 48 loss: 0.06901774031575769\n",
      "LOSS train 0.06901774031575769 valid 0.7042540907859802\n",
      "EPOCH 828:\n",
      "  batch 16 loss: 0.061029629985569045\n",
      "  batch 32 loss: 0.13130620140509563\n",
      "  batch 48 loss: 0.09541111733415164\n",
      "LOSS train 0.09541111733415164 valid 0.7052146196365356\n",
      "EPOCH 829:\n",
      "  batch 16 loss: 0.14243004610762\n",
      "  batch 32 loss: 0.060868767788633704\n",
      "  batch 48 loss: 0.06942778622033074\n",
      "LOSS train 0.06942778622033074 valid 0.7051509618759155\n",
      "EPOCH 830:\n",
      "  batch 16 loss: 0.09974084072746336\n",
      "  batch 32 loss: 0.10423766830353998\n",
      "  batch 48 loss: 0.052625050564529374\n",
      "LOSS train 0.052625050564529374 valid 0.7066137790679932\n",
      "EPOCH 831:\n",
      "  batch 16 loss: 0.07907990374951623\n",
      "  batch 32 loss: 0.09534531278768554\n",
      "  batch 48 loss: 0.0851869362522848\n",
      "LOSS train 0.0851869362522848 valid 0.7067625522613525\n",
      "EPOCH 832:\n",
      "  batch 16 loss: 0.09034303849330172\n",
      "  batch 32 loss: 0.07916984034818597\n",
      "  batch 48 loss: 0.08713266387348995\n",
      "LOSS train 0.08713266387348995 valid 0.7070570588111877\n",
      "EPOCH 833:\n",
      "  batch 16 loss: 0.07702613226138055\n",
      "  batch 32 loss: 0.09845378313912079\n",
      "  batch 48 loss: 0.09489696966193151\n",
      "LOSS train 0.09489696966193151 valid 0.707031786441803\n",
      "EPOCH 834:\n",
      "  batch 16 loss: 0.08167087502079085\n",
      "  batch 32 loss: 0.0714765166194411\n",
      "  batch 48 loss: 0.10288681322708726\n",
      "LOSS train 0.10288681322708726 valid 0.707730770111084\n",
      "EPOCH 835:\n",
      "  batch 16 loss: 0.07889803522266448\n",
      "  batch 32 loss: 0.06792009133096144\n",
      "  batch 48 loss: 0.10549545765388757\n",
      "LOSS train 0.10549545765388757 valid 0.708469033241272\n",
      "EPOCH 836:\n",
      "  batch 16 loss: 0.09124259719101246\n",
      "  batch 32 loss: 0.07550179222016595\n",
      "  batch 48 loss: 0.07868624283582903\n",
      "LOSS train 0.07868624283582903 valid 0.7080867290496826\n",
      "EPOCH 837:\n",
      "  batch 16 loss: 0.07713959889952093\n",
      "  batch 32 loss: 0.07919796131318435\n",
      "  batch 48 loss: 0.11319691622338723\n",
      "LOSS train 0.11319691622338723 valid 0.7094172239303589\n",
      "EPOCH 838:\n",
      "  batch 16 loss: 0.08957348600961268\n",
      "  batch 32 loss: 0.0994303947663866\n",
      "  batch 48 loss: 0.06887897744309157\n",
      "LOSS train 0.06887897744309157 valid 0.7094792723655701\n",
      "EPOCH 839:\n",
      "  batch 16 loss: 0.08855655969819054\n",
      "  batch 32 loss: 0.059081109531689435\n",
      "  batch 48 loss: 0.07140096236253157\n",
      "LOSS train 0.07140096236253157 valid 0.710274338722229\n",
      "EPOCH 840:\n",
      "  batch 16 loss: 0.0890464159892872\n",
      "  batch 32 loss: 0.08176409616135061\n",
      "  batch 48 loss: 0.08456186033617996\n",
      "LOSS train 0.08456186033617996 valid 0.7102420330047607\n",
      "EPOCH 841:\n",
      "  batch 16 loss: 0.11509689374361187\n",
      "  batch 32 loss: 0.09668442852853332\n",
      "  batch 48 loss: 0.06459000441464013\n",
      "LOSS train 0.06459000441464013 valid 0.7114058136940002\n",
      "EPOCH 842:\n",
      "  batch 16 loss: 0.09526392725820187\n",
      "  batch 32 loss: 0.08295273134717718\n",
      "  batch 48 loss: 0.06669377494836226\n",
      "LOSS train 0.06669377494836226 valid 0.7114503383636475\n",
      "EPOCH 843:\n",
      "  batch 16 loss: 0.12532301581813954\n",
      "  batch 32 loss: 0.08419253176543862\n",
      "  batch 48 loss: 0.07880454929545522\n",
      "LOSS train 0.07880454929545522 valid 0.7116976976394653\n",
      "EPOCH 844:\n",
      "  batch 16 loss: 0.08531800046330318\n",
      "  batch 32 loss: 0.0884201493463479\n",
      "  batch 48 loss: 0.09986287547508255\n",
      "LOSS train 0.09986287547508255 valid 0.7122399210929871\n",
      "EPOCH 845:\n",
      "  batch 16 loss: 0.07870234260917641\n",
      "  batch 32 loss: 0.07113296857278328\n",
      "  batch 48 loss: 0.09131732457899489\n",
      "LOSS train 0.09131732457899489 valid 0.7129936218261719\n",
      "EPOCH 846:\n",
      "  batch 16 loss: 0.11268049629870802\n",
      "  batch 32 loss: 0.05782470724079758\n",
      "  batch 48 loss: 0.09439767195726745\n",
      "LOSS train 0.09439767195726745 valid 0.7136887311935425\n",
      "EPOCH 847:\n",
      "  batch 16 loss: 0.06425751489587128\n",
      "  batch 32 loss: 0.10066372092114761\n",
      "  batch 48 loss: 0.0603556583810132\n",
      "LOSS train 0.0603556583810132 valid 0.7146226167678833\n",
      "EPOCH 848:\n",
      "  batch 16 loss: 0.0952236174052814\n",
      "  batch 32 loss: 0.06768757481768262\n",
      "  batch 48 loss: 0.0962188171397429\n",
      "LOSS train 0.0962188171397429 valid 0.7145142555236816\n",
      "EPOCH 849:\n",
      "  batch 16 loss: 0.09172884327563224\n",
      "  batch 32 loss: 0.052325208111142274\n",
      "  batch 48 loss: 0.08337957128242124\n",
      "LOSS train 0.08337957128242124 valid 0.7148267030715942\n",
      "EPOCH 850:\n",
      "  batch 16 loss: 0.0869480844703503\n",
      "  batch 32 loss: 0.05772439886641223\n",
      "  batch 48 loss: 0.08071725749687175\n",
      "LOSS train 0.08071725749687175 valid 0.7153133153915405\n",
      "EPOCH 851:\n",
      "  batch 16 loss: 0.11296404899621848\n",
      "  batch 32 loss: 0.04109004388010362\n",
      "  batch 48 loss: 0.12019663257524371\n",
      "LOSS train 0.12019663257524371 valid 0.7154349684715271\n",
      "EPOCH 852:\n",
      "  batch 16 loss: 0.10359220272584935\n",
      "  batch 32 loss: 0.06714706748607568\n",
      "  batch 48 loss: 0.07224509515799582\n",
      "LOSS train 0.07224509515799582 valid 0.7159386873245239\n",
      "EPOCH 853:\n",
      "  batch 16 loss: 0.10358743197866715\n",
      "  batch 32 loss: 0.07134753372520208\n",
      "  batch 48 loss: 0.07882843393599615\n",
      "LOSS train 0.07882843393599615 valid 0.7175807356834412\n",
      "EPOCH 854:\n",
      "  batch 16 loss: 0.0754280387336621\n",
      "  batch 32 loss: 0.07611864630598575\n",
      "  batch 48 loss: 0.06657486344920471\n",
      "LOSS train 0.06657486344920471 valid 0.7178781032562256\n",
      "EPOCH 855:\n",
      "  batch 16 loss: 0.06887554271816043\n",
      "  batch 32 loss: 0.0861463897745125\n",
      "  batch 48 loss: 0.09937692138191778\n",
      "LOSS train 0.09937692138191778 valid 0.7170110940933228\n",
      "EPOCH 856:\n",
      "  batch 16 loss: 0.06910981077817269\n",
      "  batch 32 loss: 0.06034767054734402\n",
      "  batch 48 loss: 0.09698800172191113\n",
      "LOSS train 0.09698800172191113 valid 0.7172155380249023\n",
      "EPOCH 857:\n",
      "  batch 16 loss: 0.07326616971477051\n",
      "  batch 32 loss: 0.07940430915914476\n",
      "  batch 48 loss: 0.09841657200558984\n",
      "LOSS train 0.09841657200558984 valid 0.7180779576301575\n",
      "EPOCH 858:\n",
      "  batch 16 loss: 0.07335715713270474\n",
      "  batch 32 loss: 0.09510016860440373\n",
      "  batch 48 loss: 0.09638670773711056\n",
      "LOSS train 0.09638670773711056 valid 0.7186034321784973\n",
      "EPOCH 859:\n",
      "  batch 16 loss: 0.09586167526140343\n",
      "  batch 32 loss: 0.09264314634492621\n",
      "  batch 48 loss: 0.06895717974111903\n",
      "LOSS train 0.06895717974111903 valid 0.7188308238983154\n",
      "EPOCH 860:\n",
      "  batch 16 loss: 0.06104234809754416\n",
      "  batch 32 loss: 0.0672205928422045\n",
      "  batch 48 loss: 0.09493230556836352\n",
      "LOSS train 0.09493230556836352 valid 0.7191720604896545\n",
      "EPOCH 861:\n",
      "  batch 16 loss: 0.09546632927958854\n",
      "  batch 32 loss: 0.09939725176081993\n",
      "  batch 48 loss: 0.040418871329165995\n",
      "LOSS train 0.040418871329165995 valid 0.7201361656188965\n",
      "EPOCH 862:\n",
      "  batch 16 loss: 0.09647889396001119\n",
      "  batch 32 loss: 0.08205374598037452\n",
      "  batch 48 loss: 0.08507688681129366\n",
      "LOSS train 0.08507688681129366 valid 0.7202633619308472\n",
      "EPOCH 863:\n",
      "  batch 16 loss: 0.07463703525718302\n",
      "  batch 32 loss: 0.08392303154687397\n",
      "  batch 48 loss: 0.10573482326435624\n",
      "LOSS train 0.10573482326435624 valid 0.7206050157546997\n",
      "EPOCH 864:\n",
      "  batch 16 loss: 0.05549288405745756\n",
      "  batch 32 loss: 0.08346289847395383\n",
      "  batch 48 loss: 0.07787326135439798\n",
      "LOSS train 0.07787326135439798 valid 0.7217468023300171\n",
      "EPOCH 865:\n",
      "  batch 16 loss: 0.10164018569048494\n",
      "  batch 32 loss: 0.09104590884089703\n",
      "  batch 48 loss: 0.061110102222301066\n",
      "LOSS train 0.061110102222301066 valid 0.7217561602592468\n",
      "EPOCH 866:\n",
      "  batch 16 loss: 0.08008476693066768\n",
      "  batch 32 loss: 0.05574499469003058\n",
      "  batch 48 loss: 0.10747759381774813\n",
      "LOSS train 0.10747759381774813 valid 0.7219387292861938\n",
      "EPOCH 867:\n",
      "  batch 16 loss: 0.087557938270038\n",
      "  batch 32 loss: 0.07537808702909388\n",
      "  batch 48 loss: 0.07004809417412616\n",
      "LOSS train 0.07004809417412616 valid 0.7227544784545898\n",
      "EPOCH 868:\n",
      "  batch 16 loss: 0.05473425869422499\n",
      "  batch 32 loss: 0.11354846034373622\n",
      "  batch 48 loss: 0.0487344405555632\n",
      "LOSS train 0.0487344405555632 valid 0.7231706380844116\n",
      "EPOCH 869:\n",
      "  batch 16 loss: 0.07577736186794937\n",
      "  batch 32 loss: 0.09193283434433397\n",
      "  batch 48 loss: 0.06595217259018682\n",
      "LOSS train 0.06595217259018682 valid 0.7227641344070435\n",
      "EPOCH 870:\n",
      "  batch 16 loss: 0.06281646090792492\n",
      "  batch 32 loss: 0.09891283573233522\n",
      "  batch 48 loss: 0.0916271631285781\n",
      "LOSS train 0.0916271631285781 valid 0.7243906259536743\n",
      "EPOCH 871:\n",
      "  batch 16 loss: 0.04951144330698298\n",
      "  batch 32 loss: 0.05607787275221199\n",
      "  batch 48 loss: 0.09244069312990177\n",
      "LOSS train 0.09244069312990177 valid 0.724582850933075\n",
      "EPOCH 872:\n",
      "  batch 16 loss: 0.08758423586550634\n",
      "  batch 32 loss: 0.06534911043854663\n",
      "  batch 48 loss: 0.10716877336381003\n",
      "LOSS train 0.10716877336381003 valid 0.7246092557907104\n",
      "EPOCH 873:\n",
      "  batch 16 loss: 0.08426312921801582\n",
      "  batch 32 loss: 0.0757825941691408\n",
      "  batch 48 loss: 0.07050934247672558\n",
      "LOSS train 0.07050934247672558 valid 0.7253183126449585\n",
      "EPOCH 874:\n",
      "  batch 16 loss: 0.09209999197628349\n",
      "  batch 32 loss: 0.07942189535242505\n",
      "  batch 48 loss: 0.04946997357910732\n",
      "LOSS train 0.04946997357910732 valid 0.7260360717773438\n",
      "EPOCH 875:\n",
      "  batch 16 loss: 0.052548332561855204\n",
      "  batch 32 loss: 0.10534540849039331\n",
      "  batch 48 loss: 0.08010299383386155\n",
      "LOSS train 0.08010299383386155 valid 0.7265223860740662\n",
      "EPOCH 876:\n",
      "  batch 16 loss: 0.09068082294834312\n",
      "  batch 32 loss: 0.05520993504978833\n",
      "  batch 48 loss: 0.06958988403494004\n",
      "LOSS train 0.06958988403494004 valid 0.7272918224334717\n",
      "EPOCH 877:\n",
      "  batch 16 loss: 0.059739085381806944\n",
      "  batch 32 loss: 0.09179978020256385\n",
      "  batch 48 loss: 0.09540397103410214\n",
      "LOSS train 0.09540397103410214 valid 0.727363646030426\n",
      "EPOCH 878:\n",
      "  batch 16 loss: 0.05110479682480218\n",
      "  batch 32 loss: 0.08098396557761589\n",
      "  batch 48 loss: 0.1086022126255557\n",
      "LOSS train 0.1086022126255557 valid 0.7270058393478394\n",
      "EPOCH 879:\n",
      "  batch 16 loss: 0.1285760996688623\n",
      "  batch 32 loss: 0.07193586777430028\n",
      "  batch 48 loss: 0.05145131316385232\n",
      "LOSS train 0.05145131316385232 valid 0.7285451889038086\n",
      "EPOCH 880:\n",
      "  batch 16 loss: 0.0569049462210387\n",
      "  batch 32 loss: 0.09398451313609257\n",
      "  batch 48 loss: 0.10211217128380667\n",
      "LOSS train 0.10211217128380667 valid 0.7287354469299316\n",
      "EPOCH 881:\n",
      "  batch 16 loss: 0.09436305635608733\n",
      "  batch 32 loss: 0.0942494818882551\n",
      "  batch 48 loss: 0.06561314617283642\n",
      "LOSS train 0.06561314617283642 valid 0.7289400100708008\n",
      "EPOCH 882:\n",
      "  batch 16 loss: 0.06119424075586721\n",
      "  batch 32 loss: 0.06745506168226711\n",
      "  batch 48 loss: 0.10447440941425157\n",
      "LOSS train 0.10447440941425157 valid 0.7300052046775818\n",
      "EPOCH 883:\n",
      "  batch 16 loss: 0.0813595320796594\n",
      "  batch 32 loss: 0.07525302772410214\n",
      "  batch 48 loss: 0.07794578184984857\n",
      "LOSS train 0.07794578184984857 valid 0.7299124002456665\n",
      "EPOCH 884:\n",
      "  batch 16 loss: 0.041835675365291536\n",
      "  batch 32 loss: 0.10538639922015136\n",
      "  batch 48 loss: 0.10551864115404896\n",
      "LOSS train 0.10551864115404896 valid 0.7305396795272827\n",
      "EPOCH 885:\n",
      "  batch 16 loss: 0.11497587124904385\n",
      "  batch 32 loss: 0.042688956222264096\n",
      "  batch 48 loss: 0.05231263206223957\n",
      "LOSS train 0.05231263206223957 valid 0.731185793876648\n",
      "EPOCH 886:\n",
      "  batch 16 loss: 0.11437399988790276\n",
      "  batch 32 loss: 0.0379576982522849\n",
      "  batch 48 loss: 0.0536226510303095\n",
      "LOSS train 0.0536226510303095 valid 0.7317304015159607\n",
      "EPOCH 887:\n",
      "  batch 16 loss: 0.08451321616303176\n",
      "  batch 32 loss: 0.057938687241403386\n",
      "  batch 48 loss: 0.06478003655502107\n",
      "LOSS train 0.06478003655502107 valid 0.7322206497192383\n",
      "EPOCH 888:\n",
      "  batch 16 loss: 0.09075273497001035\n",
      "  batch 32 loss: 0.09109752561198547\n",
      "  batch 48 loss: 0.06080323996138759\n",
      "LOSS train 0.06080323996138759 valid 0.7316946983337402\n",
      "EPOCH 889:\n",
      "  batch 16 loss: 0.057401328784180805\n",
      "  batch 32 loss: 0.07743953498720657\n",
      "  batch 48 loss: 0.11177152988966554\n",
      "LOSS train 0.11177152988966554 valid 0.7327226400375366\n",
      "EPOCH 890:\n",
      "  batch 16 loss: 0.05879188553080894\n",
      "  batch 32 loss: 0.08624998928280547\n",
      "  batch 48 loss: 0.055843982845544815\n",
      "LOSS train 0.055843982845544815 valid 0.7338588833808899\n",
      "EPOCH 891:\n",
      "  batch 16 loss: 0.10362776546389796\n",
      "  batch 32 loss: 0.07737036203616299\n",
      "  batch 48 loss: 0.07478319214715157\n",
      "LOSS train 0.07478319214715157 valid 0.7335634827613831\n",
      "EPOCH 892:\n",
      "  batch 16 loss: 0.07351033430313691\n",
      "  batch 32 loss: 0.07867289599380456\n",
      "  batch 48 loss: 0.06385534195578657\n",
      "LOSS train 0.06385534195578657 valid 0.7342914938926697\n",
      "EPOCH 893:\n",
      "  batch 16 loss: 0.08350925493868999\n",
      "  batch 32 loss: 0.07073296121961903\n",
      "  batch 48 loss: 0.07794473008834757\n",
      "LOSS train 0.07794473008834757 valid 0.7338102459907532\n",
      "EPOCH 894:\n",
      "  batch 16 loss: 0.097288954857504\n",
      "  batch 32 loss: 0.06791214132681489\n",
      "  batch 48 loss: 0.07095824580756016\n",
      "LOSS train 0.07095824580756016 valid 0.7354387044906616\n",
      "EPOCH 895:\n",
      "  batch 16 loss: 0.055490352475317195\n",
      "  batch 32 loss: 0.10852422290190589\n",
      "  batch 48 loss: 0.07888527958857594\n",
      "LOSS train 0.07888527958857594 valid 0.7353134751319885\n",
      "EPOCH 896:\n",
      "  batch 16 loss: 0.06384488711773884\n",
      "  batch 32 loss: 0.052862023207126185\n",
      "  batch 48 loss: 0.11231094656250207\n",
      "LOSS train 0.11231094656250207 valid 0.7360659837722778\n",
      "EPOCH 897:\n",
      "  batch 16 loss: 0.07904026577307377\n",
      "  batch 32 loss: 0.07712587696732953\n",
      "  batch 48 loss: 0.05698087238124572\n",
      "LOSS train 0.05698087238124572 valid 0.7362217903137207\n",
      "EPOCH 898:\n",
      "  batch 16 loss: 0.05031391579541378\n",
      "  batch 32 loss: 0.1298380403095507\n",
      "  batch 48 loss: 0.05875361699145287\n",
      "LOSS train 0.05875361699145287 valid 0.736808717250824\n",
      "EPOCH 899:\n",
      "  batch 16 loss: 0.051468367124471115\n",
      "  batch 32 loss: 0.07474498084047809\n",
      "  batch 48 loss: 0.07394352267147042\n",
      "LOSS train 0.07394352267147042 valid 0.7373417615890503\n",
      "EPOCH 900:\n",
      "  batch 16 loss: 0.07041583685713704\n",
      "  batch 32 loss: 0.07143952639307827\n",
      "  batch 48 loss: 0.10998218326130882\n",
      "LOSS train 0.10998218326130882 valid 0.7371665835380554\n",
      "EPOCH 901:\n",
      "  batch 16 loss: 0.04963475371187087\n",
      "  batch 32 loss: 0.08901227383830701\n",
      "  batch 48 loss: 0.09328165580518544\n",
      "LOSS train 0.09328165580518544 valid 0.7381899952888489\n",
      "EPOCH 902:\n",
      "  batch 16 loss: 0.0814267894893419\n",
      "  batch 32 loss: 0.09709042543545365\n",
      "  batch 48 loss: 0.06402499508112669\n",
      "LOSS train 0.06402499508112669 valid 0.7380724549293518\n",
      "EPOCH 903:\n",
      "  batch 16 loss: 0.06760029385623056\n",
      "  batch 32 loss: 0.04181717673782259\n",
      "  batch 48 loss: 0.12043522944441065\n",
      "LOSS train 0.12043522944441065 valid 0.7382672429084778\n",
      "EPOCH 904:\n",
      "  batch 16 loss: 0.05346314734197222\n",
      "  batch 32 loss: 0.08636988073703833\n",
      "  batch 48 loss: 0.06696167873451486\n",
      "LOSS train 0.06696167873451486 valid 0.7397217750549316\n",
      "EPOCH 905:\n",
      "  batch 16 loss: 0.09958241597632878\n",
      "  batch 32 loss: 0.05124659421198885\n",
      "  batch 48 loss: 0.08096237111021765\n",
      "LOSS train 0.08096237111021765 valid 0.7404952049255371\n",
      "EPOCH 906:\n",
      "  batch 16 loss: 0.11353233712725341\n",
      "  batch 32 loss: 0.04104491890029749\n",
      "  batch 48 loss: 0.048629671218805015\n",
      "LOSS train 0.048629671218805015 valid 0.74183189868927\n",
      "EPOCH 907:\n",
      "  batch 16 loss: 0.06170414773259836\n",
      "  batch 32 loss: 0.08548967603564961\n",
      "  batch 48 loss: 0.0797628681466449\n",
      "LOSS train 0.0797628681466449 valid 0.7407089471817017\n",
      "EPOCH 908:\n",
      "  batch 16 loss: 0.1147743929468561\n",
      "  batch 32 loss: 0.07244140732655069\n",
      "  batch 48 loss: 0.051146622034139\n",
      "LOSS train 0.051146622034139 valid 0.7406336069107056\n",
      "EPOCH 909:\n",
      "  batch 16 loss: 0.041124270763248205\n",
      "  batch 32 loss: 0.05539161661363323\n",
      "  batch 48 loss: 0.10120904975337908\n",
      "LOSS train 0.10120904975337908 valid 0.7413432002067566\n",
      "EPOCH 910:\n",
      "  batch 16 loss: 0.06484183388965903\n",
      "  batch 32 loss: 0.07101801257522311\n",
      "  batch 48 loss: 0.0602348406391684\n",
      "LOSS train 0.0602348406391684 valid 0.7430218458175659\n",
      "EPOCH 911:\n",
      "  batch 16 loss: 0.06215276787406765\n",
      "  batch 32 loss: 0.07678790861973539\n",
      "  batch 48 loss: 0.07203114277217537\n",
      "LOSS train 0.07203114277217537 valid 0.7429506182670593\n",
      "EPOCH 912:\n",
      "  batch 16 loss: 0.11078974601696245\n",
      "  batch 32 loss: 0.04880448689800687\n",
      "  batch 48 loss: 0.08943989419640275\n",
      "LOSS train 0.08943989419640275 valid 0.7424258589744568\n",
      "EPOCH 913:\n",
      "  batch 16 loss: 0.06271073542302474\n",
      "  batch 32 loss: 0.06278148829733254\n",
      "  batch 48 loss: 0.07914367289049551\n",
      "LOSS train 0.07914367289049551 valid 0.7432448863983154\n",
      "EPOCH 914:\n",
      "  batch 16 loss: 0.09576346419635229\n",
      "  batch 32 loss: 0.06559317704704881\n",
      "  batch 48 loss: 0.052495981130050495\n",
      "LOSS train 0.052495981130050495 valid 0.7442454695701599\n",
      "EPOCH 915:\n",
      "  batch 16 loss: 0.0708072300767526\n",
      "  batch 32 loss: 0.05738358087546658\n",
      "  batch 48 loss: 0.048896399410296\n",
      "LOSS train 0.048896399410296 valid 0.7457719445228577\n",
      "EPOCH 916:\n",
      "  batch 16 loss: 0.052283013181295246\n",
      "  batch 32 loss: 0.0865160295215901\n",
      "  batch 48 loss: 0.08636892047070432\n",
      "LOSS train 0.08636892047070432 valid 0.7453927993774414\n",
      "EPOCH 917:\n",
      "  batch 16 loss: 0.09151264221873134\n",
      "  batch 32 loss: 0.058645753510063514\n",
      "  batch 48 loss: 0.04176556757010985\n",
      "LOSS train 0.04176556757010985 valid 0.7457855939865112\n",
      "EPOCH 918:\n",
      "  batch 16 loss: 0.058926535508362576\n",
      "  batch 32 loss: 0.08015998287010007\n",
      "  batch 48 loss: 0.05463147861883044\n",
      "LOSS train 0.05463147861883044 valid 0.7465370297431946\n",
      "EPOCH 919:\n",
      "  batch 16 loss: 0.057186988822650164\n",
      "  batch 32 loss: 0.044423772509617265\n",
      "  batch 48 loss: 0.07588291866704822\n",
      "LOSS train 0.07588291866704822 valid 0.7475939989089966\n",
      "EPOCH 920:\n",
      "  batch 16 loss: 0.09090946579817683\n",
      "  batch 32 loss: 0.04289705658447929\n",
      "  batch 48 loss: 0.0956907028448768\n",
      "LOSS train 0.0956907028448768 valid 0.7472267150878906\n",
      "EPOCH 921:\n",
      "  batch 16 loss: 0.05670998433197383\n",
      "  batch 32 loss: 0.04889444820582867\n",
      "  batch 48 loss: 0.06925999777740799\n",
      "LOSS train 0.06925999777740799 valid 0.7480055689811707\n",
      "EPOCH 922:\n",
      "  batch 16 loss: 0.08782692166278139\n",
      "  batch 32 loss: 0.044138696353911655\n",
      "  batch 48 loss: 0.08839566196547821\n",
      "LOSS train 0.08839566196547821 valid 0.7481305599212646\n",
      "EPOCH 923:\n",
      "  batch 16 loss: 0.10239682923565852\n",
      "  batch 32 loss: 0.036453267428441904\n",
      "  batch 48 loss: 0.06871326739201322\n",
      "LOSS train 0.06871326739201322 valid 0.7483319044113159\n",
      "EPOCH 924:\n",
      "  batch 16 loss: 0.06439188157673925\n",
      "  batch 32 loss: 0.06414807136752643\n",
      "  batch 48 loss: 0.08239694440271705\n",
      "LOSS train 0.08239694440271705 valid 0.7484639883041382\n",
      "EPOCH 925:\n",
      "  batch 16 loss: 0.07491976395249367\n",
      "  batch 32 loss: 0.08302628851379268\n",
      "  batch 48 loss: 0.06959405197994784\n",
      "LOSS train 0.06959405197994784 valid 0.7496377825737\n",
      "EPOCH 926:\n",
      "  batch 16 loss: 0.035204527481255354\n",
      "  batch 32 loss: 0.08746787972631864\n",
      "  batch 48 loss: 0.06811751995974191\n",
      "LOSS train 0.06811751995974191 valid 0.750149667263031\n",
      "EPOCH 927:\n",
      "  batch 16 loss: 0.07965960635920055\n",
      "  batch 32 loss: 0.0688561798306182\n",
      "  batch 48 loss: 0.06387615844141692\n",
      "LOSS train 0.06387615844141692 valid 0.7506128549575806\n",
      "EPOCH 928:\n",
      "  batch 16 loss: 0.06037928277510218\n",
      "  batch 32 loss: 0.06411447541904636\n",
      "  batch 48 loss: 0.09149037295719609\n",
      "LOSS train 0.09149037295719609 valid 0.7512339353561401\n",
      "EPOCH 929:\n",
      "  batch 16 loss: 0.0327566618216224\n",
      "  batch 32 loss: 0.06291941242670873\n",
      "  batch 48 loss: 0.1118450054564164\n",
      "LOSS train 0.1118450054564164 valid 0.7515755891799927\n",
      "EPOCH 930:\n",
      "  batch 16 loss: 0.10077296732924879\n",
      "  batch 32 loss: 0.04433875301037915\n",
      "  batch 48 loss: 0.06804323182586813\n",
      "LOSS train 0.06804323182586813 valid 0.7524219751358032\n",
      "EPOCH 931:\n",
      "  batch 16 loss: 0.07921998472011182\n",
      "  batch 32 loss: 0.08608887909213081\n",
      "  batch 48 loss: 0.03544601251633139\n",
      "LOSS train 0.03544601251633139 valid 0.7528465390205383\n",
      "EPOCH 932:\n",
      "  batch 16 loss: 0.06427444270229898\n",
      "  batch 32 loss: 0.11296764583676122\n",
      "  batch 48 loss: 0.049630101217189804\n",
      "LOSS train 0.049630101217189804 valid 0.7532292008399963\n",
      "EPOCH 933:\n",
      "  batch 16 loss: 0.08607806658255868\n",
      "  batch 32 loss: 0.06997068494092673\n",
      "  batch 48 loss: 0.06620334440594888\n",
      "LOSS train 0.06620334440594888 valid 0.753717303276062\n",
      "EPOCH 934:\n",
      "  batch 16 loss: 0.059234759319224395\n",
      "  batch 32 loss: 0.07856184594857041\n",
      "  batch 48 loss: 0.06376992522564251\n",
      "LOSS train 0.06376992522564251 valid 0.7541974186897278\n",
      "EPOCH 935:\n",
      "  batch 16 loss: 0.10017312868512818\n",
      "  batch 32 loss: 0.046162043465301394\n",
      "  batch 48 loss: 0.06040160162410757\n",
      "LOSS train 0.06040160162410757 valid 0.7537095546722412\n",
      "EPOCH 936:\n",
      "  batch 16 loss: 0.06053790979785845\n",
      "  batch 32 loss: 0.06815371477568988\n",
      "  batch 48 loss: 0.07489707801141776\n",
      "LOSS train 0.07489707801141776 valid 0.7552571296691895\n",
      "EPOCH 937:\n",
      "  batch 16 loss: 0.06469027436105534\n",
      "  batch 32 loss: 0.0759038320829859\n",
      "  batch 48 loss: 0.036569210278685205\n",
      "LOSS train 0.036569210278685205 valid 0.7556282877922058\n",
      "EPOCH 938:\n",
      "  batch 16 loss: 0.061782578071870375\n",
      "  batch 32 loss: 0.07435603950580116\n",
      "  batch 48 loss: 0.057023200555704534\n",
      "LOSS train 0.057023200555704534 valid 0.7559624910354614\n",
      "EPOCH 939:\n",
      "  batch 16 loss: 0.06290204653487308\n",
      "  batch 32 loss: 0.07604131543484982\n",
      "  batch 48 loss: 0.08973435493680881\n",
      "LOSS train 0.08973435493680881 valid 0.7562942504882812\n",
      "EPOCH 940:\n",
      "  batch 16 loss: 0.06287419979344122\n",
      "  batch 32 loss: 0.09285213155089878\n",
      "  batch 48 loss: 0.07525819752481766\n",
      "LOSS train 0.07525819752481766 valid 0.7571470737457275\n",
      "EPOCH 941:\n",
      "  batch 16 loss: 0.06754831348553125\n",
      "  batch 32 loss: 0.08021524267678615\n",
      "  batch 48 loss: 0.07014193519717082\n",
      "LOSS train 0.07014193519717082 valid 0.7572109699249268\n",
      "EPOCH 942:\n",
      "  batch 16 loss: 0.07019565679365769\n",
      "  batch 32 loss: 0.09016008119215257\n",
      "  batch 48 loss: 0.054792296134110074\n",
      "LOSS train 0.054792296134110074 valid 0.7576166391372681\n",
      "EPOCH 943:\n",
      "  batch 16 loss: 0.05728432990144938\n",
      "  batch 32 loss: 0.0790058255661279\n",
      "  batch 48 loss: 0.06964668773071025\n",
      "LOSS train 0.06964668773071025 valid 0.7588770985603333\n",
      "EPOCH 944:\n",
      "  batch 16 loss: 0.06826347856258508\n",
      "  batch 32 loss: 0.08225363507881411\n",
      "  batch 48 loss: 0.07350741171103436\n",
      "LOSS train 0.07350741171103436 valid 0.7590104937553406\n",
      "EPOCH 945:\n",
      "  batch 16 loss: 0.1026758190637338\n",
      "  batch 32 loss: 0.05956967978272587\n",
      "  batch 48 loss: 0.04767902407911606\n",
      "LOSS train 0.04767902407911606 valid 0.7597326636314392\n",
      "EPOCH 946:\n",
      "  batch 16 loss: 0.07602916215546429\n",
      "  batch 32 loss: 0.05444627278484404\n",
      "  batch 48 loss: 0.06840565102174878\n",
      "LOSS train 0.06840565102174878 valid 0.7599884867668152\n",
      "EPOCH 947:\n",
      "  batch 16 loss: 0.05892934733128641\n",
      "  batch 32 loss: 0.05201356837642379\n",
      "  batch 48 loss: 0.07287370800622739\n",
      "LOSS train 0.07287370800622739 valid 0.7612895965576172\n",
      "EPOCH 948:\n",
      "  batch 16 loss: 0.04268426788621582\n",
      "  batch 32 loss: 0.09851020624410012\n",
      "  batch 48 loss: 0.0725668455415871\n",
      "LOSS train 0.0725668455415871 valid 0.7615746855735779\n",
      "EPOCH 949:\n",
      "  batch 16 loss: 0.05427676939871162\n",
      "  batch 32 loss: 0.057010000258742366\n",
      "  batch 48 loss: 0.0924231626704568\n",
      "LOSS train 0.0924231626704568 valid 0.7621982097625732\n",
      "EPOCH 950:\n",
      "  batch 16 loss: 0.06078386966510152\n",
      "  batch 32 loss: 0.05760569655831205\n",
      "  batch 48 loss: 0.05642561134300195\n",
      "LOSS train 0.05642561134300195 valid 0.7627626657485962\n",
      "EPOCH 951:\n",
      "  batch 16 loss: 0.057347025722265244\n",
      "  batch 32 loss: 0.10390772002574522\n",
      "  batch 48 loss: 0.05390930490466417\n",
      "LOSS train 0.05390930490466417 valid 0.7626975774765015\n",
      "EPOCH 952:\n",
      "  batch 16 loss: 0.05763012221723329\n",
      "  batch 32 loss: 0.08064078546885867\n",
      "  batch 48 loss: 0.06451885681599379\n",
      "LOSS train 0.06451885681599379 valid 0.7638378739356995\n",
      "EPOCH 953:\n",
      "  batch 16 loss: 0.08064078429015353\n",
      "  batch 32 loss: 0.06965817097807303\n",
      "  batch 48 loss: 0.06822616331373865\n",
      "LOSS train 0.06822616331373865 valid 0.7636930346488953\n",
      "EPOCH 954:\n",
      "  batch 16 loss: 0.06493924002825224\n",
      "  batch 32 loss: 0.04961679784901207\n",
      "  batch 48 loss: 0.09160572434484493\n",
      "LOSS train 0.09160572434484493 valid 0.7646021246910095\n",
      "EPOCH 955:\n",
      "  batch 16 loss: 0.07579506769252475\n",
      "  batch 32 loss: 0.05526607495266944\n",
      "  batch 48 loss: 0.07287720733438618\n",
      "LOSS train 0.07287720733438618 valid 0.7650749087333679\n",
      "EPOCH 956:\n",
      "  batch 16 loss: 0.04413926498091314\n",
      "  batch 32 loss: 0.09397079766495153\n",
      "  batch 48 loss: 0.07357177697122097\n",
      "LOSS train 0.07357177697122097 valid 0.7654677033424377\n",
      "EPOCH 957:\n",
      "  batch 16 loss: 0.06650453532347456\n",
      "  batch 32 loss: 0.0412306726720999\n",
      "  batch 48 loss: 0.10662341077113524\n",
      "LOSS train 0.10662341077113524 valid 0.7663466930389404\n",
      "EPOCH 958:\n",
      "  batch 16 loss: 0.08097527583595365\n",
      "  batch 32 loss: 0.06765793988597579\n",
      "  batch 48 loss: 0.05603767704451457\n",
      "LOSS train 0.05603767704451457 valid 0.7663692831993103\n",
      "EPOCH 959:\n",
      "  batch 16 loss: 0.04571643650706392\n",
      "  batch 32 loss: 0.05869554661330767\n",
      "  batch 48 loss: 0.07598101797339041\n",
      "LOSS train 0.07598101797339041 valid 0.7671819925308228\n",
      "EPOCH 960:\n",
      "  batch 16 loss: 0.05241928338364232\n",
      "  batch 32 loss: 0.06783316438668407\n",
      "  batch 48 loss: 0.0532164581672987\n",
      "LOSS train 0.0532164581672987 valid 0.768744945526123\n",
      "EPOCH 961:\n",
      "  batch 16 loss: 0.07217556174146011\n",
      "  batch 32 loss: 0.06621728703430563\n",
      "  batch 48 loss: 0.06432024234527489\n",
      "LOSS train 0.06432024234527489 valid 0.7684919834136963\n",
      "EPOCH 962:\n",
      "  batch 16 loss: 0.07329912609748135\n",
      "  batch 32 loss: 0.046222838456742465\n",
      "  batch 48 loss: 0.07757820427650586\n",
      "LOSS train 0.07757820427650586 valid 0.7698025703430176\n",
      "EPOCH 963:\n",
      "  batch 16 loss: 0.05750908798654564\n",
      "  batch 32 loss: 0.05271082586841658\n",
      "  batch 48 loss: 0.07330329999967944\n",
      "LOSS train 0.07330329999967944 valid 0.769559919834137\n",
      "EPOCH 964:\n",
      "  batch 16 loss: 0.062014445633394644\n",
      "  batch 32 loss: 0.07090431684628129\n",
      "  batch 48 loss: 0.06738862476777285\n",
      "LOSS train 0.06738862476777285 valid 0.7697160243988037\n",
      "EPOCH 965:\n",
      "  batch 16 loss: 0.04127765592420474\n",
      "  batch 32 loss: 0.035315644709044136\n",
      "  batch 48 loss: 0.09209981472668005\n",
      "LOSS train 0.09209981472668005 valid 0.7709863185882568\n",
      "EPOCH 966:\n",
      "  batch 16 loss: 0.0737769636180019\n",
      "  batch 32 loss: 0.06976694345939904\n",
      "  batch 48 loss: 0.06311546375582111\n",
      "LOSS train 0.06311546375582111 valid 0.7707562446594238\n",
      "EPOCH 967:\n",
      "  batch 16 loss: 0.061943395805428736\n",
      "  batch 32 loss: 0.05783043214614736\n",
      "  batch 48 loss: 0.058214668548316695\n",
      "LOSS train 0.058214668548316695 valid 0.77248615026474\n",
      "EPOCH 968:\n",
      "  batch 16 loss: 0.04749532126879785\n",
      "  batch 32 loss: 0.04972548730438575\n",
      "  batch 48 loss: 0.06263403266348178\n",
      "LOSS train 0.06263403266348178 valid 0.7723523378372192\n",
      "EPOCH 969:\n",
      "  batch 16 loss: 0.05952765238180291\n",
      "  batch 32 loss: 0.0431705073278863\n",
      "  batch 48 loss: 0.08159232744947076\n",
      "LOSS train 0.08159232744947076 valid 0.7717834115028381\n",
      "EPOCH 970:\n",
      "  batch 16 loss: 0.05580663331784308\n",
      "  batch 32 loss: 0.06640565295674605\n",
      "  batch 48 loss: 0.06509799182094866\n",
      "LOSS train 0.06509799182094866 valid 0.7726966142654419\n",
      "EPOCH 971:\n",
      "  batch 16 loss: 0.06078672544390429\n",
      "  batch 32 loss: 0.07284558625542559\n",
      "  batch 48 loss: 0.054184401320526376\n",
      "LOSS train 0.054184401320526376 valid 0.7729770541191101\n",
      "EPOCH 972:\n",
      "  batch 16 loss: 0.06401947670383379\n",
      "  batch 32 loss: 0.039768727103364654\n",
      "  batch 48 loss: 0.07767468187375925\n",
      "LOSS train 0.07767468187375925 valid 0.7739138603210449\n",
      "EPOCH 973:\n",
      "  batch 16 loss: 0.0696119872154668\n",
      "  batch 32 loss: 0.04179307087906636\n",
      "  batch 48 loss: 0.08634333469672129\n",
      "LOSS train 0.08634333469672129 valid 0.7740590572357178\n",
      "EPOCH 974:\n",
      "  batch 16 loss: 0.051599832833744586\n",
      "  batch 32 loss: 0.08240820036735386\n",
      "  batch 48 loss: 0.0675719067803584\n",
      "LOSS train 0.0675719067803584 valid 0.7751531004905701\n",
      "EPOCH 975:\n",
      "  batch 16 loss: 0.06423546613814324\n",
      "  batch 32 loss: 0.09845269678044133\n",
      "  batch 48 loss: 0.03519474869972328\n",
      "LOSS train 0.03519474869972328 valid 0.7756068110466003\n",
      "EPOCH 976:\n",
      "  batch 16 loss: 0.059650576673448086\n",
      "  batch 32 loss: 0.07212463556788862\n",
      "  batch 48 loss: 0.07725098266382702\n",
      "LOSS train 0.07725098266382702 valid 0.7759325504302979\n",
      "EPOCH 977:\n",
      "  batch 16 loss: 0.057268394477432594\n",
      "  batch 32 loss: 0.056545747560448945\n",
      "  batch 48 loss: 0.060805164030171\n",
      "LOSS train 0.060805164030171 valid 0.776443362236023\n",
      "EPOCH 978:\n",
      "  batch 16 loss: 0.05035936739295721\n",
      "  batch 32 loss: 0.06405799434287474\n",
      "  batch 48 loss: 0.06462814373662695\n",
      "LOSS train 0.06462814373662695 valid 0.7770429849624634\n",
      "EPOCH 979:\n",
      "  batch 16 loss: 0.07529649545904249\n",
      "  batch 32 loss: 0.05835325064253993\n",
      "  batch 48 loss: 0.039277541989577\n",
      "LOSS train 0.039277541989577 valid 0.7779434323310852\n",
      "EPOCH 980:\n",
      "  batch 16 loss: 0.04887704114662483\n",
      "  batch 32 loss: 0.04696135643462185\n",
      "  batch 48 loss: 0.07269481754337903\n",
      "LOSS train 0.07269481754337903 valid 0.7783851623535156\n",
      "EPOCH 981:\n",
      "  batch 16 loss: 0.04874943141476251\n",
      "  batch 32 loss: 0.059504404838662595\n",
      "  batch 48 loss: 0.08676704511162825\n",
      "LOSS train 0.08676704511162825 valid 0.779083788394928\n",
      "EPOCH 982:\n",
      "  batch 16 loss: 0.06477523464127444\n",
      "  batch 32 loss: 0.0695179992071644\n",
      "  batch 48 loss: 0.052177253375703\n",
      "LOSS train 0.052177253375703 valid 0.7787736654281616\n",
      "EPOCH 983:\n",
      "  batch 16 loss: 0.07122438630904071\n",
      "  batch 32 loss: 0.041468386429187376\n",
      "  batch 48 loss: 0.07960084765363717\n",
      "LOSS train 0.07960084765363717 valid 0.7802608609199524\n",
      "EPOCH 984:\n",
      "  batch 16 loss: 0.0611451355216559\n",
      "  batch 32 loss: 0.08703962073195726\n",
      "  batch 48 loss: 0.0551414295041468\n",
      "LOSS train 0.0551414295041468 valid 0.7796576619148254\n",
      "EPOCH 985:\n",
      "  batch 16 loss: 0.04593373663374223\n",
      "  batch 32 loss: 0.05774549981288146\n",
      "  batch 48 loss: 0.06826137819734868\n",
      "LOSS train 0.06826137819734868 valid 0.7812304496765137\n",
      "EPOCH 986:\n",
      "  batch 16 loss: 0.08054819831158966\n",
      "  batch 32 loss: 0.04041021392913535\n",
      "  batch 48 loss: 0.06109002561424859\n",
      "LOSS train 0.06109002561424859 valid 0.7812985777854919\n",
      "EPOCH 987:\n",
      "  batch 16 loss: 0.06123131398635451\n",
      "  batch 32 loss: 0.07667246132768923\n",
      "  batch 48 loss: 0.0473589138200623\n",
      "LOSS train 0.0473589138200623 valid 0.7816438674926758\n",
      "EPOCH 988:\n",
      "  batch 16 loss: 0.04661457776091993\n",
      "  batch 32 loss: 0.07939241149597365\n",
      "  batch 48 loss: 0.08688868911121972\n",
      "LOSS train 0.08688868911121972 valid 0.7828431725502014\n",
      "EPOCH 989:\n",
      "  batch 16 loss: 0.07161758623988135\n",
      "  batch 32 loss: 0.04271457494905917\n",
      "  batch 48 loss: 0.07201350259128958\n",
      "LOSS train 0.07201350259128958 valid 0.7826551795005798\n",
      "EPOCH 990:\n",
      "  batch 16 loss: 0.0670729045450571\n",
      "  batch 32 loss: 0.08779023366514593\n",
      "  batch 48 loss: 0.03877142307464965\n",
      "LOSS train 0.03877142307464965 valid 0.7833870053291321\n",
      "EPOCH 991:\n",
      "  batch 16 loss: 0.05590927261800971\n",
      "  batch 32 loss: 0.09300182599690743\n",
      "  batch 48 loss: 0.05114318494088366\n",
      "LOSS train 0.05114318494088366 valid 0.7836542725563049\n",
      "EPOCH 992:\n",
      "  batch 16 loss: 0.05407724062388297\n",
      "  batch 32 loss: 0.0633584979805164\n",
      "  batch 48 loss: 0.08028489245043602\n",
      "LOSS train 0.08028489245043602 valid 0.7845937609672546\n",
      "EPOCH 993:\n",
      "  batch 16 loss: 0.04950471713527804\n",
      "  batch 32 loss: 0.04738024188554846\n",
      "  batch 48 loss: 0.08777884309529327\n",
      "LOSS train 0.08777884309529327 valid 0.7841416001319885\n",
      "EPOCH 994:\n",
      "  batch 16 loss: 0.05212757224944653\n",
      "  batch 32 loss: 0.03292890630837064\n",
      "  batch 48 loss: 0.06962227448821068\n",
      "LOSS train 0.06962227448821068 valid 0.7853604555130005\n",
      "EPOCH 995:\n",
      "  batch 16 loss: 0.05157518124906346\n",
      "  batch 32 loss: 0.05599181533034425\n",
      "  batch 48 loss: 0.067163972184062\n",
      "LOSS train 0.067163972184062 valid 0.7854787707328796\n",
      "EPOCH 996:\n",
      "  batch 16 loss: 0.051296453100803774\n",
      "  batch 32 loss: 0.06424688280640112\n",
      "  batch 48 loss: 0.03676886257017031\n",
      "LOSS train 0.03676886257017031 valid 0.7864600419998169\n",
      "EPOCH 997:\n",
      "  batch 16 loss: 0.06399736402090639\n",
      "  batch 32 loss: 0.078000885387155\n",
      "  batch 48 loss: 0.06673760538978968\n",
      "LOSS train 0.06673760538978968 valid 0.7869951725006104\n",
      "EPOCH 998:\n",
      "  batch 16 loss: 0.049617837321420666\n",
      "  batch 32 loss: 0.05681547074345872\n",
      "  batch 48 loss: 0.07212515908759087\n",
      "LOSS train 0.07212515908759087 valid 0.7872850298881531\n",
      "EPOCH 999:\n",
      "  batch 16 loss: 0.06628918679780327\n",
      "  batch 32 loss: 0.034745232581371965\n",
      "  batch 48 loss: 0.07186625868780538\n",
      "LOSS train 0.07186625868780538 valid 0.7882830500602722\n",
      "EPOCH 1000:\n",
      "  batch 16 loss: 0.055391067115124315\n",
      "  batch 32 loss: 0.05251627201869269\n",
      "  batch 48 loss: 0.08508076564612566\n",
      "LOSS train 0.08508076564612566 valid 0.7877464890480042\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/heart_disease_classifier{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "    \n",
    "\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)  # Move data to GPU\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     37\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/studia/s7/sieci-neuronowe/cw4/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/projects/studia/s7/sieci-neuronowe/cw4/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/studia/s7/sieci-neuronowe/cw4/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/studia/s7/sieci-neuronowe/cw4/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:211\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define new code for training and testing with parameter sweeps.\n",
    "from itertools import product\n",
    "import torch.optim as optim\n",
    "\n",
    "# Parameters to sweep\n",
    "batch_sizes = [4, 16, 32]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "optimizers = ['SGD', 'Adam', 'RMSprop']\n",
    "\n",
    "# Placeholder for results\n",
    "results = []\n",
    "\n",
    "# Updated training loop with parameter sweeps\n",
    "for batch_size, lr, opt_name in product(batch_sizes, learning_rates, optimizers):\n",
    "    print(f\"testing with batch size {batch_size}, lr {lr}, optimiser {opt_name}\")\n",
    "    # Reinitialize dataloaders with current batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Reinitialize the model\n",
    "    model = HeartDiseaseClassifier(input_dim=X_train.shape[1], output_dim=1).to(device)\n",
    "    \n",
    "    # Configure optimizer\n",
    "    if opt_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif opt_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif opt_name == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training process\n",
    "    num_epochs = 750\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            test_accuracy += calculate_accuracy(outputs, y_batch).item()\n",
    "    test_accuracy /= len(test_loader)\n",
    "    \n",
    "    # Save results\n",
    "    results.append({\n",
    "        'Batch Size': batch_size,\n",
    "        'Learning Rate': lr,\n",
    "        'Optimizer': opt_name,\n",
    "        'Test Accuracy': test_accuracy\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for better readability\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(by='Test Accuracy', ascending=False, inplace=True)\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
